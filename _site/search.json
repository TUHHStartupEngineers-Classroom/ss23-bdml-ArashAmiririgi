[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\nThis is a .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#header-2",
    "href": "content/01_journal/01_tidyverse.html#header-2",
    "title": "Tidyverse",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "content/03_other/06_links.html#r-and-r-studio",
    "href": "content/03_other/06_links.html#r-and-r-studio",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual ."
  },
  {
    "objectID": "content/03_other/06_links.html#additional-r-resources",
    "href": "content/03_other/06_links.html#additional-r-resources",
    "title": "Links",
    "section": "",
    "text": "Google is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "sdvdyxv"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/01_journal/01_Fundamentals.html",
    "href": "content/01_journal/01_Fundamentals.html",
    "title": "Machine Learning Fundamentals",
    "section": "",
    "text": "Challenge:\nYour organization wants to know which companies are similar to each other to help in identifying potential customers of a SAAS software solution (e.g. Sales force CRM or equivalent) in various segments of the market. The Sales Department is very interested in this analysis, which will help them more easily penetrate various market segments.\nCode:\n\nknitr::opts_chunk$set(\n    echo = TRUE,\n    message = FALSE,\n    warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(broom)\nlibrary(umap)\nsp_500_prices_tbl &lt;- read_rds(\"sp_500_prices_tbl.rds\")\nsp_500_prices_tbl\n\n\n\n  \n\n\nsp_500_index_tbl &lt;- read_rds(\"sp_500_index_tbl.rds\")\nsp_500_index_tbl\n\n\n\n  \n\n\n## Step 1 - Convert stock prices to a standardized format (daily returns)\nsp_500_daily_returns_tbl &lt;- sp_500_prices_tbl %&gt;% select(symbol,date, adjusted) %&gt;% filter(year(date) &gt;= '2018') %&gt;% group_by(symbol) %&gt;%\n  mutate(lag = lag(adjusted, n = 1)) %&gt;% filter(!(is.na(lag))) %&gt;% mutate(pct_return = (adjusted - lag)/lag) %&gt;%\n  select(symbol, date, pct_return)\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\n## Step 2 - Convert to User-Item Format\nsp_500_daily_returns_tbl &lt;- read_rds(\"sp_500_daily_returns_tbl.rds\")\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\nstock_date_matrix_tbl &lt;- sp_500_daily_returns_tbl %&gt;% pivot_wider(names_from = date, values_from = pct_return, values_fill = 0)\nstock_date_matrix_tbl\n\n\n\n  \n\n\n## Step 3 - Perform K-Means Clustering\nstock_date_matrix_tbl &lt;- read_rds(\"stock_date_matrix_tbl.rds\")\nkmeans_obj &lt;- stock_date_matrix_tbl %&gt;% select(-symbol) %&gt;% kmeans(centers = 4, nstart = 20)\n\n## Step 4 - Find the optimal value of K\nkmeans_mapper &lt;- function(center = 3) {\n    stock_date_matrix_tbl %&gt;%\n        select(-symbol) %&gt;%\n        kmeans(centers = center, nstart = 20)}\nk_means_mapped_tbl &lt;- tibble(centers = 1:30) %&gt;%\n    mutate(k_means = centers %&gt;% map(kmeans_mapper)) %&gt;%\n    mutate(glance  = k_means %&gt;% map(glance))\nk_means_mapped_tbl\n\n\n\n  \n\n\nk_means_mapped_tbl %&gt;% unnest(glance) %&gt;% select(centers, tot.withinss) %&gt;%  ggplot(aes(centers, tot.withinss)) +\n    geom_point(color = \"blue\", size = 4) +\n    geom_line(color = \"blue\", size = 1) +\n    ggrepel::geom_label_repel(aes(label = centers), color = \"blue\")\n\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n## Step 5 - Apply UMAP\nk_means_mapped_tbl &lt;- read_rds(\"k_means_mapped_tbl.rds\")\numap_results &lt;- stock_date_matrix_tbl %&gt;% select(-symbol) %&gt;% umap()\numap_results\n\n#&gt; umap embedding of 502 items in 2 dimensions\n#&gt; object components: layout, data, knn, config\n\numap_results_tbl &lt;- umap_results$layou %&gt;% as_tibble() %&gt;% set_names(c(\"x\", \"y\")) %&gt;% bind_cols(stock_date_matrix_tbl %&gt;% select(symbol))\n\n#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#&gt; `.name_repair` is omitted as of tibble 2.0.0.\n#&gt; ℹ Using compatibility `.name_repair`.\n\numap_results_tbl\n\n\n\n  \n\n\numap_results_tbl %&gt;% ggplot(aes(x, y)) + geom_point(alpha = 0.5) + theme_tq() + labs(title = \"UMAP Projection\")\n\n\n\n\n\n\n## Step 6 - Combine K-Means and UMAP\nk_means_mapped_tbl &lt;- read_rds(\"k_means_mapped_tbl.rds\")\numap_results_tbl   &lt;- read_rds(\"umap_results_tbl.rds\")\nk_means_obj &lt;- k_means_mapped_tbl %&gt;% pull(k_means) %&gt;% pluck(10)\numap_kmeans_results_tbl &lt;- k_means_obj %&gt;% augment(stock_date_matrix_tbl) %&gt;% select(symbol, .cluster) %&gt;% left_join(umap_results_tbl) %&gt;% left_join(sp_500_index_tbl %&gt;% select(symbol, company, sector))\n\n#&gt; Joining with `by = join_by(symbol)`\n\n\n#&gt; Joining with `by = join_by(symbol)`\n\numap_kmeans_results_tbl\n\n\n\n  \n\n\numap_kmeans_results_tbl %&gt;% ggplot(aes(V1, V2, color = .cluster)) + geom_point(alpha = 0.5)"
  },
  {
    "objectID": "content/01_journal/06_LIME.html",
    "href": "content/01_journal/06_LIME.html",
    "title": "Explaining Black-Box Models With LIME",
    "section": "",
    "text": "Challenge: IBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. In this regard, we are going to Visualize feature importance for a single and multiple explanation.\nCode:\nLoading Libraries\n\nlibrary(h2o)\nlibrary(recipes)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tidyquant)\nlibrary(lime)\n\nLoading data\n\nemployee_attrition_tbl &lt;- read_csv(\"Employee_Attrition.csv\")\ndefinitions_raw_tbl &lt;- read_excel(\"data_definitions.xlsx\",\n                                  sheet = 1, col_names = FALSE)\n\nProcessing Pipeline\n\nsource(\"00_Scripts/data_processing_pipeline.R\")\nemployee_attrition_readable_tbl &lt;- process_hr_data_readable(employee_attrition_tbl, definitions_raw_tbl)\n\nAssign training and test data\n\n# set.seed(seed = 1113)\nsplit_obj &lt;- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\nrecipe_obj &lt;- recipe(Attrition ~ ., data = train_readable_tbl) %&gt;%\n                step_zv(all_predictors()) %&gt;%\n                step_mutate_at(c(\"JobLevel\", \"StockOptionLevel\"), fn = as.factor) %&gt;% \n                prep()\ntrain_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  &lt;- bake(recipe_obj, new_data = test_readable_tbl)\n\nModels\n\nh2o.init()\n\n#&gt;  Connection successful!\n#&gt; \n#&gt; R is connected to the H2O cluster: \n#&gt;     H2O cluster uptime:         2 hours 21 minutes \n#&gt;     H2O cluster timezone:       Europe/Berlin \n#&gt;     H2O data parsing timezone:  UTC \n#&gt;     H2O cluster version:        3.40.0.4 \n#&gt;     H2O cluster version age:    1 month and 17 days \n#&gt;     H2O cluster name:           H2O_started_from_R_arash_zrw498 \n#&gt;     H2O cluster total nodes:    1 \n#&gt;     H2O cluster total memory:   2.72 GB \n#&gt;     H2O cluster total cores:    16 \n#&gt;     H2O cluster allowed cores:  16 \n#&gt;     H2O cluster healthy:        TRUE \n#&gt;     H2O Connection ip:          localhost \n#&gt;     H2O Connection port:        54321 \n#&gt;     H2O Connection proxy:       NA \n#&gt;     H2O Internal Security:      FALSE \n#&gt;     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\nsplit_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o &lt;- split_h2o[[1]]\nvalid_h2o &lt;- split_h2o[[2]]\ntest_h2o  &lt;- as.h2o(test_tbl)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ny &lt;- \"Attrition\"\nx &lt;- setdiff(names(train_h2o), y)\n\nautoml_models_h2o &lt;- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 30,\n  nfolds            = 5)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n#&gt; 09:23:38.967: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#&gt; 09:23:38.974: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |======================================================================| 100%\n\nmodels &lt;- automl_models_h2o@leaderboard %&gt;% as_tibble()\nmodels\n\n\n\n  \n\n\nextract_h2o_model_name_by_position &lt;- function(h2o_leaderboard,\n                                                n= 1, verbose = T){\n   model_name &lt;- h2o_leaderboard %&gt;%\n     as_tibble() %&gt;%\n     slice(n) %&gt;%\n     pull(model_id)\n   if (verbose) message(model_name)\n   return(model_name)}\n\n# StackedEnsemble_BestOfFamily_1_AutoML_5_20230615_90739\nautoml_leader &lt;- automl_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(4) %&gt;% \n  h2o.getModel()\n\nPredictoin\n\npredictions_tbl &lt;- automl_leader %&gt;% \n    h2o.predict(newdata = as.h2o(test_tbl)) %&gt;%\n    as.tibble() %&gt;%\n    bind_cols(test_tbl %&gt;%\n                select(Attrition, EmployeeNumber))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl\n\n\n\n  \n\n\n\nLime for single explanation\n\nexplainer &lt;- train_tbl %&gt;%\n    select(-Attrition) %&gt;%\n    lime(\n        model           = automl_leader,\n        bin_continuous  = TRUE,\n        n_bins          = 4,\n        quantile_bins   = TRUE)\n\nexplanation &lt;- test_tbl %&gt;%\n    slice(1) %&gt;%\n    select(-Attrition) %&gt;%\n    lime::explain(\n        explainer = explainer,\n        n_labels   = 1,\n        n_features = 20,\n        n_permutations = 5000,\n        kernel_width   = 1)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nexplanation %&gt;% as.tibble()\n\n\n\n  \n\n\ng1 &lt;- plot_features(explanation = explanation, ncol = 1)\ng1\n\n\n\n\n\n\nexplanation_df &lt;- as.data.frame(explanation)\nexp_tibble &lt;- explanation_df[, c(\"feature\", \"feature_weight\")]\nexp_tibble &lt;- exp_tibble[order(exp_tibble$feature_weight), ]\nexp_tibble$Support &lt;- ifelse(exp_tibble$feature_weight &gt; 0, \"Supports\", \"Contradicts\")\nexplained_employee &lt;- test_tbl$EmployeeNumber[1]\nfeatures &lt;- test_tbl[1, ] %&gt;% as_tibble()\nfeatures_df &lt;- as.data.frame(features)\n\nfor(i in 1:nrow(explanation_df)){\n  feature_name &lt;- explanation_df$feature[i]\n  feature_value &lt;- features_df[[feature_name]]\n  explanation_df$feature[i] &lt;- paste(feature_name, \"=\", feature_value)}\n\nggplot(data = exp_tibble, aes(x = reorder(feature, feature_weight), y = feature_weight, fill = Support)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"Supports\" = \"darkturquoise\", \"Contradicts\" = \"gray\")) +\n  labs(x = \"Features\", y = \"Weight\", title = \"Feature Importance\") +\n  theme_minimal()\n\n\n\n\n\n\n\nVisualizing Feature Importance For Multiple Explanations\n\nexplanation &lt;- test_tbl %&gt;%\n    slice(1:20) %&gt;%\n    select(-Attrition) %&gt;%\n    lime::explain(\n        explainer = explainer,\n        n_labels   = 1,\n        n_features = 8,\n        n_permutations = 5000,\n        kernel_width   = 0.5)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nexplanation %&gt;% as.tibble()\n\n\n\n  \n\n\ng2 &lt;- plot_explanations(explanation)\ng2\n\n\n\n\n\n\nexplanation %&gt;% ggplot(aes_(~(case), ~feature_desc)) +\n  geom_tile(aes_(fill = ~feature_weight)) +\n  scale_x_discrete(\"Case\", expand = c(0, 0)) +\n  scale_y_discrete(\"Feature\", expand = c(0, 0)) +\n  scale_fill_gradient2(\"Feature\\nweight\", low = \"firebrick\", mid = \"white\", high = \"steelblue\") +\n  theme(panel.border = element_rect(fill = NA,\n                                    size = 1),\n        panel.grid = element_blank(),\n        legend.position = \"right\",\n        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +\n  facet_wrap(~label) + theme_light() + theme(panel.grid.major = element_blank())"
  },
  {
    "objectID": "content/01_journal/05_Performance_Measures.html",
    "href": "content/01_journal/05_Performance_Measures.html",
    "title": "Performance Measures",
    "section": "",
    "text": "Challenge: The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales. It’s a classic Binary Classification problem.\nCode:\nPreparing the model based previous chapter\n\n# Loading libraries\nlibrary(tidymodels)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(rstanarm)\nlibrary(stringr)\nlibrary(broom.mixed)\nlibrary(h2o)\nlibrary(cowplot)\nlibrary(glue)\nlibrary(forcats)\n\n# Loading the training & test dataset\nproduct_backorders_tbl &lt;- read.csv(\"product_backorders.csv\")\nsplit_obj &lt;- initial_split(product_backorders_tbl, prop = 0.8)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\n\n# Specifying the response and predictor variables\nfactor_name &lt;- \"went_on_backorder\"\nrecipe_obj &lt;- \n  recipe(went_on_backorder ~ ., data = train_readable_tbl) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_mutate_at(factor_name, fn = as.factor) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  prep()\ntrain_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  &lt;- bake(recipe_obj, new_data = test_readable_tbl)\n\nh2o.init()\n\n#&gt;  Connection successful!\n#&gt; \n#&gt; R is connected to the H2O cluster: \n#&gt;     H2O cluster uptime:         1 hours 47 minutes \n#&gt;     H2O cluster timezone:       Europe/Berlin \n#&gt;     H2O data parsing timezone:  UTC \n#&gt;     H2O cluster version:        3.40.0.4 \n#&gt;     H2O cluster version age:    1 month and 16 days \n#&gt;     H2O cluster name:           H2O_started_from_R_arash_mtu845 \n#&gt;     H2O cluster total nodes:    1 \n#&gt;     H2O cluster total memory:   3.38 GB \n#&gt;     H2O cluster total cores:    16 \n#&gt;     H2O cluster allowed cores:  16 \n#&gt;     H2O cluster healthy:        TRUE \n#&gt;     H2O Connection ip:          localhost \n#&gt;     H2O Connection port:        54321 \n#&gt;     H2O Connection proxy:       NA \n#&gt;     H2O Internal Security:      FALSE \n#&gt;     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\nsplit_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.8), seed = 1234)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o &lt;- split_h2o[[1]]\nvalid_h2o &lt;- split_h2o[[2]]\ntest_h2o  &lt;- as.h2o(test_tbl)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ny &lt;- \"went_on_backorder\"\nx &lt;- setdiff(names(train_h2o), y)\n\n# running AutoML specifying the stopping criterion\nautoml_models_h2o &lt;- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 60,\n  nfolds            = 5)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n#&gt; 11:31:34.434: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#&gt; 11:31:34.442: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\n# View the leaderboard\ntypeof(automl_models_h2o)\n\n#&gt; [1] \"S4\"\n\nslotNames(automl_models_h2o)\n\n#&gt; [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#&gt; [5] \"modeling_steps\" \"training_info\"\n\nautoml_models_h2o@leaderboard \n\n#&gt;                                                  model_id       auc   logloss\n#&gt; 1 StackedEnsemble_BestOfFamily_3_AutoML_7_20230614_113134 0.9558991 0.1633229\n#&gt; 2 StackedEnsemble_BestOfFamily_2_AutoML_7_20230614_113134 0.9551468 0.1636052\n#&gt; 3 StackedEnsemble_BestOfFamily_4_AutoML_7_20230614_113134 0.9551417 0.1642400\n#&gt; 4    StackedEnsemble_AllModels_3_AutoML_7_20230614_113134 0.9550815 0.1656647\n#&gt; 5    StackedEnsemble_AllModels_2_AutoML_7_20230614_113134 0.9548151 0.1648209\n#&gt; 6    StackedEnsemble_AllModels_1_AutoML_7_20230614_113134 0.9543871 0.1650046\n#&gt;       aucpr mean_per_class_error      rmse        mse\n#&gt; 1 0.7562312            0.1452976 0.2208875 0.04879128\n#&gt; 2 0.7547404            0.1438671 0.2208614 0.04877976\n#&gt; 3 0.7533004            0.1687993 0.2219230 0.04924982\n#&gt; 4 0.7553987            0.1534357 0.2222604 0.04939968\n#&gt; 5 0.7529800            0.1400849 0.2223844 0.04945480\n#&gt; 6 0.7517535            0.1379657 0.2224020 0.04946265\n#&gt; \n#&gt; [23 rows x 7 columns]\n\nautoml_models_h2o@leader\n\n#&gt; Model Details:\n#&gt; ==============\n#&gt; \n#&gt; H2OBinomialModel: stackedensemble\n#&gt; Model ID:  StackedEnsemble_BestOfFamily_3_AutoML_7_20230614_113134 \n#&gt; Model Summary for Stacked Ensemble: \n#&gt;                                          key            value\n#&gt; 1                          Stacking strategy cross_validation\n#&gt; 2       Number of base models (used / total)              3/5\n#&gt; 3           # GBM base models (used / total)              1/1\n#&gt; 4           # DRF base models (used / total)              2/2\n#&gt; 5  # DeepLearning base models (used / total)              0/1\n#&gt; 6           # GLM base models (used / total)              0/1\n#&gt; 7                      Metalearner algorithm              GLM\n#&gt; 8         Metalearner fold assignment scheme           Random\n#&gt; 9                         Metalearner nfolds                5\n#&gt; 10                   Metalearner fold_column               NA\n#&gt; 11        Custom metalearner hyperparameters             None\n#&gt; \n#&gt; \n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on training data. **\n#&gt; \n#&gt; MSE:  0.01745464\n#&gt; RMSE:  0.132116\n#&gt; LogLoss:  0.07403128\n#&gt; Mean Per-Class Error:  0.04530698\n#&gt; AUC:  0.9965165\n#&gt; AUCPR:  0.9801323\n#&gt; Gini:  0.993033\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No  Yes    Error       Rate\n#&gt; No     8697   71 0.008098   =71/8768\n#&gt; Yes     101 1123 0.082516  =101/1224\n#&gt; Totals 8798 1194 0.017214  =172/9992\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.451056    0.928867 170\n#&gt; 2                       max f2  0.276389    0.937599 218\n#&gt; 3                 max f0point5  0.590170    0.949108 136\n#&gt; 4                 max accuracy  0.451056    0.982786 170\n#&gt; 5                max precision  0.991648    1.000000   0\n#&gt; 6                   max recall  0.048981    1.000000 319\n#&gt; 7              max specificity  0.991648    1.000000   0\n#&gt; 8             max absolute_mcc  0.451056    0.919168 170\n#&gt; 9   max min_per_class_accuracy  0.259345    0.969771 224\n#&gt; 10 max mean_per_class_accuracy  0.276389    0.970316 218\n#&gt; 11                     max tns  0.991648 8768.000000   0\n#&gt; 12                     max fns  0.991648 1218.000000   0\n#&gt; 13                     max fps  0.000131 8768.000000 399\n#&gt; 14                     max tps  0.048981 1224.000000 319\n#&gt; 15                     max tnr  0.991648    1.000000   0\n#&gt; 16                     max fnr  0.991648    0.995098   0\n#&gt; 17                     max fpr  0.000131    1.000000 399\n#&gt; 18                     max tpr  0.048981    1.000000 319\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on validation data. **\n#&gt; \n#&gt; MSE:  0.04287206\n#&gt; RMSE:  0.2070557\n#&gt; LogLoss:  0.1477067\n#&gt; Mean Per-Class Error:  0.1103119\n#&gt; AUC:  0.9623304\n#&gt; AUCPR:  0.7865083\n#&gt; Gini:  0.9246607\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No Yes    Error       Rate\n#&gt; No     2524 137 0.051484  =137/2661\n#&gt; Yes      57 280 0.169139    =57/337\n#&gt; Totals 2581 417 0.064710  =194/2998\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.289812    0.742706 203\n#&gt; 2                       max f2  0.150911    0.806025 255\n#&gt; 3                 max f0point5  0.632171    0.771784 100\n#&gt; 4                 max accuracy  0.563618    0.941628 117\n#&gt; 5                max precision  0.981602    1.000000   0\n#&gt; 6                   max recall  0.000124    1.000000 399\n#&gt; 7              max specificity  0.981602    1.000000   0\n#&gt; 8             max absolute_mcc  0.289812    0.711414 203\n#&gt; 9   max min_per_class_accuracy  0.146481    0.907929 257\n#&gt; 10 max mean_per_class_accuracy  0.146481    0.907971 257\n#&gt; 11                     max tns  0.981602 2661.000000   0\n#&gt; 12                     max fns  0.981602  336.000000   0\n#&gt; 13                     max fps  0.000124 2661.000000 399\n#&gt; 14                     max tps  0.000124  337.000000 399\n#&gt; 15                     max tnr  0.981602    1.000000   0\n#&gt; 16                     max fnr  0.981602    0.997033   0\n#&gt; 17                     max fpr  0.000124    1.000000 399\n#&gt; 18                     max tpr  0.000124    1.000000 399\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on cross-validation data. **\n#&gt; ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#&gt; \n#&gt; MSE:  0.05186746\n#&gt; RMSE:  0.2277443\n#&gt; LogLoss:  0.1736725\n#&gt; Mean Per-Class Error:  0.1552887\n#&gt; AUC:  0.9508387\n#&gt; AUCPR:  0.7482311\n#&gt; Gini:  0.9016773\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;           No  Yes    Error        Rate\n#&gt; No     10249  507 0.047136  =507/10756\n#&gt; Yes      392 1096 0.263441   =392/1488\n#&gt; Totals 10641 1603 0.073424  =899/12244\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold        value idx\n#&gt; 1                       max f1  0.356142     0.709156 195\n#&gt; 2                       max f2  0.130567     0.773612 282\n#&gt; 3                 max f0point5  0.510779     0.726817 146\n#&gt; 4                 max accuracy  0.498024     0.930333 150\n#&gt; 5                max precision  0.984746     1.000000   0\n#&gt; 6                   max recall  0.002859     1.000000 392\n#&gt; 7              max specificity  0.984746     1.000000   0\n#&gt; 8             max absolute_mcc  0.356142     0.667812 195\n#&gt; 9   max min_per_class_accuracy  0.118599     0.883693 287\n#&gt; 10 max mean_per_class_accuracy  0.115107     0.884326 289\n#&gt; 11                     max tns  0.984746 10756.000000   0\n#&gt; 12                     max fns  0.984746  1482.000000   0\n#&gt; 13                     max fps  0.000099 10756.000000 399\n#&gt; 14                     max tps  0.002859  1488.000000 392\n#&gt; 15                     max tnr  0.984746     1.000000   0\n#&gt; 16                     max fnr  0.984746     0.995968   0\n#&gt; 17                     max fpr  0.000099     1.000000 399\n#&gt; 18                     max tpr  0.002859     1.000000 392\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; Cross-Validation Metrics Summary: \n#&gt;                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#&gt; accuracy    0.928681  0.008464   0.932979   0.934500   0.936752   0.920947\n#&gt; auc         0.950893  0.003629   0.950354   0.956759   0.951497   0.948040\n#&gt; err         0.071319  0.008464   0.067021   0.065500   0.063248   0.079053\n#&gt; err_count 175.000000 24.647514 164.000000 161.000000 148.000000 197.000000\n#&gt; f0point5    0.704618  0.047251   0.722683   0.726496   0.762423   0.653893\n#&gt;           cv_5_valid\n#&gt; accuracy    0.918229\n#&gt; auc         0.947815\n#&gt; err         0.081771\n#&gt; err_count 205.000000\n#&gt; f0point5    0.657596\n#&gt; \n#&gt; ---\n#&gt;                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#&gt; precision           0.697502  0.060128   0.725694   0.722222   0.769760\n#&gt; r2                  0.514445  0.033783   0.521898   0.542368   0.549641\n#&gt; recall              0.739821  0.019199   0.710884   0.744108   0.734426\n#&gt; residual_deviance 850.046940 43.134464 840.404540 803.531300 819.674260\n#&gt; rmse                0.227511  0.005835   0.224813   0.220486   0.225941\n#&gt; specificity         0.954883  0.012349   0.963307   0.960666   0.967076\n#&gt;                   cv_4_valid cv_5_valid\n#&gt; precision           0.634218   0.635616\n#&gt; r2                  0.479855   0.478464\n#&gt; recall              0.746528   0.763158\n#&gt; residual_deviance 877.197940 909.426800\n#&gt; rmse                0.230577   0.235739\n#&gt; specificity         0.943739   0.939628\n\nextract_h2o_model_name_by_position &lt;- function(h2o_leaderboard,\n                                                n= 1, verbose = T){\n   model_name &lt;- h2o_leaderboard %&gt;%\n     as_tibble() %&gt;%\n     slice(n) %&gt;%\n     pull(model_id)\n   if (verbose) message(model_name)\n   return(model_name)}\n\nautoml_models_h2o@leaderboard %&gt;%\n  extract_h2o_model_name_by_position(11) %&gt;%\n  h2o.getModel() %&gt;%\n  h2o.saveModel(path = \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\")\n\n#&gt; [1] \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\\\\StackedEnsemble_BestOfFamily_1_AutoML_7_20230614_113134\"\n\n# StackedEnsemble_BestOfFamily_1_AutoML_6_20230614_111147\nstacked_ensemble_h2o &lt;- automl_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(13) %&gt;% \n  h2o.getModel()\n\npredictions &lt;- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#&gt; [1] \"environment\"\n\npredictions_tbl &lt;- predictions %&gt;% as_tibble()\npredictions_tbl\n\n\n\n  \n\n\n\nLeaderboard vizualization\n\nautoml_models_h2o@leaderboard %&gt;% \n              as_tibble() %&gt;% \n              select(-c(mean_per_class_error, rmse, mse))\n\n\n\n  \n\n\n\n\nplot_h2o_leaderboard &lt;- function(h2o_leaderboard, order_by = c(\"auc\", \"logloss\"), \n                                 n_max = 20, size = 4, include_lbl = TRUE) {\n    order_by &lt;- tolower(order_by[[1]])\n    leaderboard_tbl &lt;- h2o_leaderboard %&gt;%\n        as.tibble() %&gt;%\n        select(-c(aucpr, mean_per_class_error, rmse, mse)) %&gt;% \n        mutate(model_type = str_extract(model_id, \"[^_]+\")) %&gt;%\n        rownames_to_column(var = \"rowname\") %&gt;%\n        mutate(model_id = paste0(rowname, \". \", model_id) %&gt;% as.factor())\n    if (order_by == \"auc\") {\n        data_transformed_tbl &lt;- leaderboard_tbl %&gt;%\n            slice(1:n_max) %&gt;%\n            mutate(\n                model_id   = as_factor(model_id) %&gt;% reorder(auc),\n                model_type = as.factor(model_type)) %&gt;%\n                pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder))\n    } else if (order_by == \"logloss\") {\n        data_transformed_tbl &lt;- leaderboard_tbl %&gt;%\n            slice(1:n_max) %&gt;%\n            mutate(\n                model_id   = as_factor(model_id) %&gt;% reorder(logloss) %&gt;% fct_rev(),\n                model_type = as.factor(model_type)\n            ) %&gt;%\n            pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder))\n    } else {\n        stop(paste0(\"order_by = '\", order_by, \"' is not a permitted option.\"))\n    }\n    g &lt;- data_transformed_tbl %&gt;%\n        ggplot(aes(value, model_id, color = model_type)) +\n        geom_point(size = size) +\n        facet_wrap(~ key, scales = \"free_x\") +\n        labs(title = \"Leaderboard Metrics\",\n             subtitle = paste0(\"Ordered by: \", toupper(order_by)),\n             y = \"Model Postion, Model ID\", x = \"\")\n\n    if (include_lbl) g &lt;- g + geom_label(aes(label = round(value, 2), \n                                             hjust = \"inward\"))\n\n    return(g)\n}\n\nplot_h2o_leaderboard(automl_models_h2o@leaderboard)\n\n\n\n\n\n\n\nTune a model with grid search\n\ndeeplearning_h2o &lt;- h2o.loadModel(\"04_Modeling/DeepLearning_grid_1_AutoML_6_20230614_111147_model_1\")\nh2o.performance(deeplearning_h2o, newdata = as.h2o(test_tbl))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n#&gt; H2OBinomialMetrics: deeplearning\n#&gt; \n#&gt; MSE:  0.09042893\n#&gt; RMSE:  0.300714\n#&gt; LogLoss:  0.3166993\n#&gt; Mean Per-Class Error:  0.3312235\n#&gt; AUC:  0.7808898\n#&gt; AUCPR:  0.3486153\n#&gt; Gini:  0.5617796\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No Yes    Error       Rate\n#&gt; No     2941 429 0.127300  =429/3370\n#&gt; Yes     236 205 0.535147   =236/441\n#&gt; Totals 3177 634 0.174495  =665/3811\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.193420    0.381395 139\n#&gt; 2                       max f2  0.136079    0.510475 221\n#&gt; 3                 max f0point5  0.264469    0.406091  72\n#&gt; 4                 max accuracy  0.276958    0.892941  65\n#&gt; 5                max precision  0.810485    0.722222  10\n#&gt; 6                   max recall  0.000044    1.000000 399\n#&gt; 7              max specificity  0.999759    0.999110   0\n#&gt; 8             max absolute_mcc  0.193420    0.289950 139\n#&gt; 9   max min_per_class_accuracy  0.154093    0.700890 195\n#&gt; 10 max mean_per_class_accuracy  0.136079    0.702528 221\n#&gt; 11                     max tns  0.999759 3367.000000   0\n#&gt; 12                     max fns  0.999759  437.000000   0\n#&gt; 13                     max fps  0.000044 3370.000000 399\n#&gt; 14                     max tps  0.000044  441.000000 399\n#&gt; 15                     max tnr  0.999759    0.999110   0\n#&gt; 16                     max fnr  0.999759    0.990930   0\n#&gt; 17                     max fpr  0.000044    1.000000 399\n#&gt; 18                     max tpr  0.000044    1.000000 399\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n\ndeeplearning_grid_01 &lt;- h2o.grid(\n    algorithm = \"deeplearning\",\n    grid_id = \"deeplearning_grid_01\",\n    x = x, y = y,\n    training_frame   = train_h2o,\n    validation_frame = valid_h2o,\n    nfolds = 5,\n    hyper_params = list(\n        hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),\n        epochs = c(10, 50, 100)))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ndeeplearning_grid_01\n\n#&gt; H2O Grid Details\n#&gt; ================\n#&gt; \n#&gt; Grid ID: deeplearning_grid_01 \n#&gt; Used hyper parameters: \n#&gt;   -  epochs \n#&gt;   -  hidden \n#&gt; Number of models: 27 \n#&gt; Number of failed models: 0 \n#&gt; \n#&gt; Hyper-Parameter Search Summary: ordered by increasing logloss\n#&gt;      epochs       hidden                     model_ids logloss\n#&gt; 1  51.99628 [20, 20, 20] deeplearning_grid_01_model_26 0.27077\n#&gt; 2 104.00449 [50, 20, 10] deeplearning_grid_01_model_24 0.27415\n#&gt; 3 103.99751 [10, 10, 10] deeplearning_grid_01_model_21 0.27608\n#&gt; 4 104.01403 [10, 10, 10]  deeplearning_grid_01_model_3 0.27775\n#&gt; 5 103.99767 [20, 20, 20] deeplearning_grid_01_model_27 0.27874\n#&gt; \n#&gt; ---\n#&gt;      epochs       hidden                     model_ids logloss\n#&gt; 22 10.39905 [10, 10, 10] deeplearning_grid_01_model_10 0.32161\n#&gt; 23 10.40711 [50, 20, 10] deeplearning_grid_01_model_22 0.32260\n#&gt; 24 10.40346 [10, 10, 10]  deeplearning_grid_01_model_1 0.32496\n#&gt; 25 10.38659 [10, 10, 10] deeplearning_grid_01_model_19 0.32514\n#&gt; 26 10.40053 [50, 20, 10]  deeplearning_grid_01_model_4 0.32592\n#&gt; 27 10.39831 [20, 20, 20]  deeplearning_grid_01_model_7 0.32676\n\nh2o.getGrid(grid_id = \"deeplearning_grid_01\", sort_by = \"auc\", decreasing = TRUE)\n\n#&gt; H2O Grid Details\n#&gt; ================\n#&gt; \n#&gt; Grid ID: deeplearning_grid_01 \n#&gt; Used hyper parameters: \n#&gt;   -  epochs \n#&gt;   -  hidden \n#&gt; Number of models: 27 \n#&gt; Number of failed models: 0 \n#&gt; \n#&gt; Hyper-Parameter Search Summary: ordered by decreasing auc\n#&gt;      epochs       hidden                     model_ids     auc\n#&gt; 1 103.99767 [20, 20, 20] deeplearning_grid_01_model_27 0.87510\n#&gt; 2 103.99751 [10, 10, 10] deeplearning_grid_01_model_21 0.87291\n#&gt; 3  51.99628 [20, 20, 20] deeplearning_grid_01_model_26 0.86995\n#&gt; 4 104.00449 [50, 20, 10] deeplearning_grid_01_model_24 0.86799\n#&gt; 5  51.97890 [10, 10, 10] deeplearning_grid_01_model_20 0.86482\n#&gt; \n#&gt; ---\n#&gt;      epochs       hidden                     model_ids     auc\n#&gt; 22 10.40711 [50, 20, 10] deeplearning_grid_01_model_22 0.77136\n#&gt; 23 10.39193 [50, 20, 10] deeplearning_grid_01_model_13 0.76877\n#&gt; 24 10.40346 [10, 10, 10]  deeplearning_grid_01_model_1 0.75313\n#&gt; 25 10.39905 [10, 10, 10] deeplearning_grid_01_model_10 0.75285\n#&gt; 26 10.40053 [50, 20, 10]  deeplearning_grid_01_model_4 0.75234\n#&gt; 27 10.39831 [20, 20, 20]  deeplearning_grid_01_model_7 0.74788\n\ndeeplearning_grid_01_model_1 &lt;- h2o.getModel(\"deeplearning_grid_01_model_1\")\n\ndeeplearning_grid_01_model_1 %&gt;% h2o.auc(train = T, valid = T, xval = T)\n\n#&gt;     train     valid      xval \n#&gt; 0.7855436 0.7690894 0.7531344\n\ndeeplearning_grid_01_model_1 %&gt;%\n    h2o.performance(newdata = as.h2o(test_tbl))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n#&gt; H2OBinomialMetrics: deeplearning\n#&gt; \n#&gt; MSE:  0.09404392\n#&gt; RMSE:  0.3066658\n#&gt; LogLoss:  0.3195257\n#&gt; Mean Per-Class Error:  0.3200586\n#&gt; AUC:  0.7832694\n#&gt; AUCPR:  0.3527772\n#&gt; Gini:  0.5665388\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No Yes    Error       Rate\n#&gt; No     2787 583 0.172997  =583/3370\n#&gt; Yes     206 235 0.467120   =206/441\n#&gt; Totals 2993 818 0.207032  =789/3811\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.279567    0.373312 160\n#&gt; 2                       max f2  0.178134    0.519587 255\n#&gt; 3                 max f0point5  0.346859    0.398887 104\n#&gt; 4                 max accuracy  0.467183    0.891367  61\n#&gt; 5                max precision  0.981485    1.000000   0\n#&gt; 6                   max recall  0.010533    1.000000 390\n#&gt; 7              max specificity  0.981485    1.000000   0\n#&gt; 8             max absolute_mcc  0.346859    0.291993 104\n#&gt; 9   max min_per_class_accuracy  0.240179    0.703264 200\n#&gt; 10 max mean_per_class_accuracy  0.235977    0.712207 204\n#&gt; 11                     max tns  0.981485 3370.000000   0\n#&gt; 12                     max fns  0.981485  439.000000   0\n#&gt; 13                     max fps  0.000127 3370.000000 399\n#&gt; 14                     max tps  0.010533  441.000000 390\n#&gt; 15                     max tnr  0.981485    1.000000   0\n#&gt; 16                     max fnr  0.981485    0.995465   0\n#&gt; 17                     max fpr  0.000127    1.000000 399\n#&gt; 18                     max tpr  0.010533    1.000000 390\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n\n\nVisualizing the trade of between the precision and the recall and the optimal threshold\n\nstacked_ensemble_h2o &lt;- h2o.loadModel(\"04_Modeling/StackedEnsemble_AllModels_1_AutoML_6_20230614_111147\")\ndeeplearning_h2o     &lt;- h2o.loadModel(\"04_Modeling/DeepLearning_grid_1_AutoML_6_20230614_111147_model_1\")\nglm_h2o              &lt;- h2o.loadModel(\"04_Modeling/GLM_1_AutoML_6_20230614_111147\")\n\nperformance_h2o &lt;- h2o.performance(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(performance_h2o)\n\n#&gt; [1] \"S4\"\n\nperformance_h2o %&gt;% slotNames()\n\n#&gt; [1] \"algorithm\" \"on_train\"  \"on_valid\"  \"on_xval\"   \"metrics\"\n\nperformance_h2o@metrics\n\n#&gt; $model\n#&gt; $model$`__meta`\n#&gt; $model$`__meta`$schema_version\n#&gt; [1] 3\n#&gt; \n#&gt; $model$`__meta`$schema_name\n#&gt; [1] \"ModelKeyV3\"\n#&gt; \n#&gt; $model$`__meta`$schema_type\n#&gt; [1] \"Key&lt;Model&gt;\"\n#&gt; \n#&gt; \n#&gt; $model$name\n#&gt; [1] \"StackedEnsemble_AllModels_1_AutoML_6_20230614_111147\"\n#&gt; \n#&gt; $model$type\n#&gt; [1] \"Key&lt;Model&gt;\"\n#&gt; \n#&gt; $model$URL\n#&gt; [1] \"/3/Models/StackedEnsemble_AllModels_1_AutoML_6_20230614_111147\"\n#&gt; \n#&gt; \n#&gt; $model_checksum\n#&gt; [1] \"-5561831513927090160\"\n#&gt; \n#&gt; $frame\n#&gt; $frame$name\n#&gt; [1] \"test_tbl_sid_878b_182\"\n#&gt; \n#&gt; \n#&gt; $frame_checksum\n#&gt; [1] \"-8541855890973580992\"\n#&gt; \n#&gt; $description\n#&gt; NULL\n#&gt; \n#&gt; $scoring_time\n#&gt; [1] 1.686735e+12\n#&gt; \n#&gt; $predictions\n#&gt; NULL\n#&gt; \n#&gt; $MSE\n#&gt; [1] 0.03689769\n#&gt; \n#&gt; $RMSE\n#&gt; [1] 0.1920877\n#&gt; \n#&gt; $nobs\n#&gt; [1] 3811\n#&gt; \n#&gt; $custom_metric_name\n#&gt; NULL\n#&gt; \n#&gt; $custom_metric_value\n#&gt; [1] 0\n#&gt; \n#&gt; $r2\n#&gt; [1] 0.6394143\n#&gt; \n#&gt; $logloss\n#&gt; [1] 0.1288173\n#&gt; \n#&gt; $AUC\n#&gt; [1] 0.9742671\n#&gt; \n#&gt; $pr_auc\n#&gt; [1] 0.8511284\n#&gt; \n#&gt; $Gini\n#&gt; [1] 0.9485342\n#&gt; \n#&gt; $mean_per_class_error\n#&gt; [1] 0.1094394\n#&gt; \n#&gt; $domain\n#&gt; [1] \"No\"  \"Yes\"\n#&gt; \n#&gt; $cm\n#&gt; $cm$`__meta`\n#&gt; $cm$`__meta`$schema_version\n#&gt; [1] 3\n#&gt; \n#&gt; $cm$`__meta`$schema_name\n#&gt; [1] \"ConfusionMatrixV3\"\n#&gt; \n#&gt; $cm$`__meta`$schema_type\n#&gt; [1] \"ConfusionMatrix\"\n#&gt; \n#&gt; \n#&gt; $cm$table\n#&gt; Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#&gt;          No Yes  Error          Rate\n#&gt; No     3259 111 0.0329 = 111 / 3,370\n#&gt; Yes      82 359 0.1859 =    82 / 441\n#&gt; Totals 3341 470 0.0506 = 193 / 3,811\n#&gt; \n#&gt; \n#&gt; $thresholds_and_metric_scores\n#&gt; Metrics for Thresholds: Binomial metrics as a function of classification thresholds\n#&gt;   threshold       f1       f2 f0point5 accuracy precision   recall specificity\n#&gt; 1  0.992559 0.009029 0.005663 0.022272 0.884807  1.000000 0.004535    1.000000\n#&gt; 2  0.989094 0.013514 0.008489 0.033113 0.885070  1.000000 0.006803    1.000000\n#&gt; 3  0.981971 0.026846 0.016949 0.064516 0.885857  1.000000 0.013605    1.000000\n#&gt; 4  0.978826 0.035556 0.022561 0.083857 0.886119  0.888889 0.018141    0.999703\n#&gt; 5  0.977303 0.048565 0.030968 0.112474 0.886906  0.916667 0.024943    0.999703\n#&gt;   absolute_mcc min_per_class_accuracy mean_per_class_accuracy  tns fns fps tps\n#&gt; 1     0.063344               0.004535                0.502268 3370 439   0   2\n#&gt; 2     0.077590               0.006803                0.503401 3370 438   0   3\n#&gt; 3     0.109773               0.013605                0.506803 3370 435   0   6\n#&gt; 4     0.117597               0.018141                0.508922 3369 433   1   8\n#&gt; 5     0.140723               0.024943                0.512323 3369 430   1  11\n#&gt;        tnr      fnr      fpr      tpr idx\n#&gt; 1 1.000000 0.995465 0.000000 0.004535   0\n#&gt; 2 1.000000 0.993197 0.000000 0.006803   1\n#&gt; 3 1.000000 0.986395 0.000000 0.013605   2\n#&gt; 4 0.999703 0.981859 0.000297 0.018141   3\n#&gt; 5 0.999703 0.975057 0.000297 0.024943   4\n#&gt; \n#&gt; ---\n#&gt;     threshold       f1       f2 f0point5 accuracy precision   recall\n#&gt; 395  0.001204 0.217188 0.409547 0.147778 0.165836  0.121823 1.000000\n#&gt; 396  0.001026 0.216282 0.408258 0.147108 0.161375  0.121254 1.000000\n#&gt; 397  0.000893 0.212530 0.402887 0.144335 0.142482  0.118900 1.000000\n#&gt; 398  0.000651 0.209700 0.398806 0.142249 0.127788  0.117131 1.000000\n#&gt; 399  0.000374 0.208264 0.396725 0.141192 0.120178  0.116236 1.000000\n#&gt; 400  0.000188 0.207432 0.395516 0.140580 0.115718  0.115718 1.000000\n#&gt;     specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy tns\n#&gt; 395    0.056677     0.083093               0.056677                0.528338 191\n#&gt; 396    0.051632     0.079124               0.051632                0.525816 174\n#&gt; 397    0.030267     0.059990               0.030267                0.515134 102\n#&gt; 398    0.013650     0.039985               0.013650                0.506825  46\n#&gt; 399    0.005045     0.024215               0.005045                0.502522  17\n#&gt; 400    0.000000     0.000000               0.000000                0.500000   0\n#&gt;     fns  fps tps      tnr      fnr      fpr      tpr idx\n#&gt; 395   0 3179 441 0.056677 0.000000 0.943323 1.000000 394\n#&gt; 396   0 3196 441 0.051632 0.000000 0.948368 1.000000 395\n#&gt; 397   0 3268 441 0.030267 0.000000 0.969733 1.000000 396\n#&gt; 398   0 3324 441 0.013650 0.000000 0.986350 1.000000 397\n#&gt; 399   0 3353 441 0.005045 0.000000 0.994955 1.000000 398\n#&gt; 400   0 3370 441 0.000000 0.000000 1.000000 1.000000 399\n#&gt; \n#&gt; $max_criteria_and_metric_scores\n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.377772    0.788145 182\n#&gt; 2                       max f2  0.165549    0.834016 253\n#&gt; 3                 max f0point5  0.620920    0.822350 118\n#&gt; 4                 max accuracy  0.417654    0.950407 174\n#&gt; 5                max precision  0.992559    1.000000   0\n#&gt; 6                   max recall  0.013551    1.000000 365\n#&gt; 7              max specificity  0.992559    1.000000   0\n#&gt; 8             max absolute_mcc  0.377772    0.759916 182\n#&gt; 9   max min_per_class_accuracy  0.165549    0.920178 253\n#&gt; 10 max mean_per_class_accuracy  0.165549    0.921540 253\n#&gt; 11                     max tns  0.992559 3370.000000   0\n#&gt; 12                     max fns  0.992559  439.000000   0\n#&gt; 13                     max fps  0.000188 3370.000000 399\n#&gt; 14                     max tps  0.013551  441.000000 365\n#&gt; 15                     max tnr  0.992559    1.000000   0\n#&gt; 16                     max fnr  0.992559    0.995465   0\n#&gt; 17                     max fpr  0.000188    1.000000 399\n#&gt; 18                     max tpr  0.013551    1.000000 365\n#&gt; \n#&gt; $gains_lift_table\n#&gt; Gains/Lift Table: Avg response rate: 11.57 %, avg score: 12.12 %\n#&gt;    group cumulative_data_fraction lower_threshold     lift cumulative_lift\n#&gt; 1      1               0.01023353        0.949197 8.198558        8.198558\n#&gt; 2      2               0.02020467        0.915571 8.186896        8.192803\n#&gt; 3      3               0.03017581        0.887427 8.186896        8.190851\n#&gt; 4      4               0.04014694        0.855672 8.186896        8.189869\n#&gt; 5      5               0.05011808        0.802852 8.186896        8.189277\n#&gt; 6      6               0.10023616        0.527388 5.972290        7.080784\n#&gt; 7      7               0.15009184        0.250677 3.320241        5.831652\n#&gt; 8      8               0.20020992        0.116333 1.176360        4.666304\n#&gt; 9      9               0.30018368        0.043680 0.430952        3.255754\n#&gt; 10    10               0.40015744        0.021231 0.158772        2.482016\n#&gt; 11    11               0.50013120        0.013838 0.045363        1.994941\n#&gt; 12    12               0.60010496        0.009279 0.022682        1.666375\n#&gt; 13    13               0.70007872        0.005912 0.000000        1.428411\n#&gt; 14    14               0.80005248        0.003321 0.000000        1.249918\n#&gt; 15    15               0.90002624        0.001716 0.000000        1.111079\n#&gt; 16    16               1.00000000        0.000024 0.000000        1.000000\n#&gt;    response_rate    score cumulative_response_rate cumulative_score\n#&gt; 1       0.948718 0.970236                 0.948718         0.970236\n#&gt; 2       0.947368 0.929677                 0.948052         0.950220\n#&gt; 3       0.947368 0.901340                 0.947826         0.934068\n#&gt; 4       0.947368 0.873928                 0.947712         0.919131\n#&gt; 5       0.947368 0.828063                 0.947644         0.901013\n#&gt; 6       0.691099 0.676556                 0.819372         0.788785\n#&gt; 7       0.384211 0.373218                 0.674825         0.650747\n#&gt; 8       0.136126 0.173154                 0.539974         0.531192\n#&gt; 9       0.049869 0.074023                 0.376748         0.378936\n#&gt; 10      0.018373 0.030023                 0.287213         0.291765\n#&gt; 11      0.005249 0.017048                 0.230850         0.236850\n#&gt; 12      0.002625 0.011398                 0.192829         0.199291\n#&gt; 13      0.000000 0.007612                 0.165292         0.171919\n#&gt; 14      0.000000 0.004471                 0.144638         0.150995\n#&gt; 15      0.000000 0.002532                 0.128571         0.134504\n#&gt; 16      0.000000 0.001035                 0.115718         0.121160\n#&gt;    capture_rate cumulative_capture_rate        gain cumulative_gain\n#&gt; 1      0.083900                0.083900  719.855806      719.855806\n#&gt; 2      0.081633                0.165533  718.689581      719.280266\n#&gt; 3      0.081633                0.247166  718.689581      719.085083\n#&gt; 4      0.081633                0.328798  718.689581      718.986854\n#&gt; 5      0.081633                0.410431  718.689581      718.927711\n#&gt; 6      0.299320                0.709751  497.229049      608.078380\n#&gt; 7      0.165533                0.875283  232.024108      483.165247\n#&gt; 8      0.058957                0.934240   17.636025      366.630409\n#&gt; 9      0.043084                0.977324  -56.904792      225.575417\n#&gt; 10     0.015873                0.993197  -84.122818      148.201628\n#&gt; 11     0.004535                0.997732  -95.463662       99.494138\n#&gt; 12     0.002268                1.000000  -97.731831       66.637516\n#&gt; 13     0.000000                1.000000 -100.000000       42.841079\n#&gt; 14     0.000000                1.000000 -100.000000       24.991801\n#&gt; 15     0.000000                1.000000 -100.000000       11.107872\n#&gt; 16     0.000000                1.000000 -100.000000        0.000000\n#&gt;    kolmogorov_smirnov\n#&gt; 1            0.083307\n#&gt; 2            0.164346\n#&gt; 3            0.245385\n#&gt; 4            0.326424\n#&gt; 5            0.407463\n#&gt; 6            0.689276\n#&gt; 7            0.820091\n#&gt; 8            0.830086\n#&gt; 9            0.765752\n#&gt; 10           0.670645\n#&gt; 11           0.562718\n#&gt; 12           0.452226\n#&gt; 13           0.339169\n#&gt; 14           0.226113\n#&gt; 15           0.113056\n#&gt; 16           0.000000\n#&gt; \n#&gt; $residual_deviance\n#&gt; [1] 981.8458\n#&gt; \n#&gt; $null_deviance\n#&gt; [1] 2731.013\n#&gt; \n#&gt; $AIC\n#&gt; [1] 993.8458\n#&gt; \n#&gt; $null_degrees_of_freedom\n#&gt; [1] 3810\n#&gt; \n#&gt; $residual_degrees_of_freedom\n#&gt; [1] 3805\n\nh2o.auc(performance_h2o, train = T, valid = T, xval = T)\n\n#&gt; [1] 0.9742671\n\nh2o.auc(stacked_ensemble_h2o, train = T, valid = T, xval = T)\n\n#&gt;     train     valid      xval \n#&gt; 0.9882040 0.9529088 0.9512211\n\nh2o.giniCoef(performance_h2o)\n\n#&gt; [1] 0.9485342\n\nh2o.logloss(performance_h2o)\n\n#&gt; [1] 0.1288173\n\nh2o.confusionMatrix(stacked_ensemble_h2o)\n\n\n\n  \n\n\nh2o.confusionMatrix(performance_h2o)\n\n\n\n  \n\n\nperformance_tbl &lt;- performance_h2o %&gt;%\n    h2o.metric() %&gt;% as.tibble()\ntheme_new &lt;- theme(\n      legend.position  = \"bottom\",\n      legend.key       = element_blank(),\n      panel.background = element_rect(fill   = \"transparent\"),\n      panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n      panel.grid.major = element_line(color = \"grey\", size = 0.333))\nperformance_tbl %&gt;% filter(f1 == max(f1))\n\n\n\n  \n\n\nperformance_tbl %&gt;%\n    ggplot(aes(x = threshold)) +\n    geom_line(aes(y = precision), color = \"blue\", size = 1) +\n    geom_line(aes(y = recall), color = \"red\", size = 1) +\n    geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, \"f1\")) +\n    labs(title = \"Precision vs Recall\", y = \"value\") +\n    theme_new\n\n\n\n\n\n\n\nROC Plot\n\npath &lt;- \"04_Modeling/StackedEnsemble_AllModels_1_AutoML_6_20230614_111147\"\n\nload_model_performance_metrics &lt;- function(path, test_tbl) {\n    model_h2o &lt;- h2o.loadModel(path)\n    perf_h2o  &lt;- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n    \n    perf_h2o %&gt;%\n        h2o.metric() %&gt;%\n        as_tibble() %&gt;%\n        mutate(auc = h2o.auc(perf_h2o)) %&gt;%\n        select(tpr, fpr, auc)}\n\nmodel_metrics_tbl &lt;- fs::dir_info(path = \"04_Modeling/\") %&gt;%\n    select(path) %&gt;%\n    mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %&gt;%\n  unnest(cols = metrics)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nmodel_metrics_tbl %&gt;% mutate(\n        path = str_split(path, pattern = \"/\", simplify = T)[,2] %&gt;% as_factor(),\n        auc  = auc %&gt;% round(3) %&gt;% as.character() %&gt;% as_factor()) %&gt;%\n    ggplot(aes(fpr, tpr, color = path, linetype = auc)) +\n    geom_line(size = 1) +\n    geom_abline(color = \"red\", linetype = \"dotted\") +\n    theme_new +\n    theme(legend.direction = \"vertical\",) +\n    labs(title = \"ROC Plot\",\n         subtitle = \"Performance of 3 Top Performing Models\")\n\n\n\n\n\n\n\nPrecision vs Recall Plot\n\nload_model_performance_metrics &lt;- function(path, test_tbl) {\n    model_h2o &lt;- h2o.loadModel(path)\n    perf_h2o  &lt;- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n    perf_h2o %&gt;%\n        h2o.metric() %&gt;%\n        as_tibble() %&gt;%\n        mutate(auc = h2o.auc(perf_h2o)) %&gt;%\n        select(tpr, fpr, auc, precision, recall)}\n\nmodel_metrics_tbl &lt;- fs::dir_info(path = \"04_Modeling/\") %&gt;%\n    select(path) %&gt;%\n    mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %&gt;% unnest(cols = metrics)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nmodel_metrics_tbl %&gt;% mutate(\n        path = str_split(path, pattern = \"/\", simplify = T)[,2] %&gt;% as_factor(),\n        auc  = auc %&gt;% round(3) %&gt;% as.character() %&gt;% as_factor()) %&gt;%\n    ggplot(aes(recall, precision, color = path, linetype = auc)) +\n    geom_line(size = 1) +\n    theme_new + \n    theme(legend.direction = \"vertical\",) +\n    labs(title = \"Precision vs Recall Plot\",\n         subtitle = \"Performance of 3 Top Performing Models\")\n\n\n\n\n\n\n\nGain & Lift Plot\n\nranked_predictions_tbl &lt;- predictions_tbl %&gt;%\n    bind_cols(test_tbl) %&gt;%\n    select(predict:Yes, went_on_backorder) %&gt;%\n    arrange(desc(Yes))\n\ncalculated_gain_lift_tbl &lt;- ranked_predictions_tbl %&gt;%\n    mutate(ntile = ntile(Yes, n = 10)) %&gt;%\n    group_by(ntile) %&gt;%\n    summarise(cases = n(),\n              responses = sum(went_on_backorder == \"Yes\")) %&gt;%\n    arrange(desc(ntile)) %&gt;%\n    mutate(group = row_number()) %&gt;%\n    select(group, cases, responses) %&gt;%\n    mutate(\n        cumulative_responses = cumsum(responses),\n        pct_responses        = responses / sum(responses),\n        gain                 = cumsum(pct_responses),\n        cumulative_pct_cases = cumsum(cases) / sum(cases),\n        lift                 = gain / cumulative_pct_cases,\n        gain_baseline        = cumulative_pct_cases,\n        lift_baseline        = gain_baseline / cumulative_pct_cases)\ncalculated_gain_lift_tbl \n\n\n\n  \n\n\ngain_lift_tbl &lt;- performance_h2o %&gt;%\n    h2o.gainsLift() %&gt;%\n    as.tibble()\n\n# Gain Chart\ngain_transformed_tbl &lt;- gain_lift_tbl %&gt;% \n    select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %&gt;%\n    select(-contains(\"lift\")) %&gt;%\n    mutate(baseline = cumulative_data_fraction) %&gt;%\n    rename(gain     = cumulative_capture_rate) %&gt;%\n    pivot_longer(cols = c(gain, baseline), values_to = \"value\", names_to = \"key\")\ngain_transformed_tbl %&gt;%\n    ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n    geom_line(size = 1.5) +\n    labs(title = \"Gain Chart\",\n         x = \"Cumulative Data Fraction\",\n         y = \"Gain\") + theme_new\n\n\n\n\n\n\n# Lift Plot\nlift_transformed_tbl &lt;- gain_lift_tbl %&gt;% \n    select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %&gt;%\n    select(-contains(\"capture\")) %&gt;%\n    mutate(baseline = 1) %&gt;%\n    rename(lift = cumulative_lift) %&gt;%\n    pivot_longer(cols = c(lift, baseline), values_to = \"value\", names_to = \"key\")\n\nlift_transformed_tbl %&gt;%\n    ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n    geom_line(size = 1.5) +\n    labs(title = \"Lift Chart\",\n         x = \"Cumulative Data Fraction\",\n         y = \"Lift\") + theme_new\n\n\n\n\n\n\n\nDashboard with cowplot\n\nh2o_leaderboard &lt;- automl_models_h2o@leaderboard\nnewdata &lt;- test_tbl\norder_by &lt;- \"auc\"\nmax_models &lt;- 4\nsize &lt;- 1\n\nplot_h2o_performance &lt;- function(h2o_leaderboard, newdata, order_by = c(\"auc\", \"logloss\"),\n                                 max_models = 3, size = 1.5) {\n    leaderboard_tbl &lt;- h2o_leaderboard %&gt;%\n        as_tibble() %&gt;%\n        slice(1:max_models)\n    newdata_tbl &lt;- newdata %&gt;%\n        as_tibble()\n    order_by      &lt;- tolower(order_by[[1]])\n    order_by_expr &lt;- rlang::sym(order_by)\n    h2o.no_progress()\n    \n    get_model_performance_metrics &lt;- function(model_id, test_tbl) {\n        model_h2o &lt;- h2o.getModel(model_id)\n        perf_h2o  &lt;- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))\n        perf_h2o %&gt;% h2o.metric() %&gt;% as.tibble() %&gt;%\n          select(threshold, tpr, fpr, precision, recall)\n    }\n    \n    model_metrics_tbl &lt;- leaderboard_tbl %&gt;%\n        mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %&gt;%\n        unnest(cols = metrics) %&gt;%\n        mutate(model_id = as_factor(model_id) %&gt;% \n                 fct_reorder(!! order_by_expr, \n                             .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n               auc = auc %&gt;% round(3) %&gt;% as.character() %&gt;% as_factor() %&gt;%\n                 fct_reorder(as.numeric(model_id)),\n               logloss  = logloss %&gt;%  round(4) %&gt;%  as.character() %&gt;%\n                 as_factor() %&gt;% fct_reorder(as.numeric(model_id)))\n\n    p1 &lt;- model_metrics_tbl %&gt;%\n        ggplot(aes(fpr, tpr, color = model_id, linetype = !! order_by_expr)) +\n        geom_line(size = size) +\n        theme_new +\n        labs(title = \"ROC\", x = \"FPR\", y = \"TPR\") +\n        theme(legend.direction = \"vertical\") \n\n    p2 &lt;- model_metrics_tbl %&gt;%\n        ggplot(aes(recall, precision, color = model_id, linetype = !! order_by_expr)) +\n        geom_line(size = size) +\n        theme_new +\n        labs(title = \"Precision Vs Recall\", x = \"Recall\", y = \"Precision\") +\n        theme(legend.position = \"none\") \n\n    get_gain_lift &lt;- function(model_id, test_tbl) {\n        model_h2o &lt;- h2o.getModel(model_id)\n        perf_h2o  &lt;- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n        perf_h2o %&gt;%\n            h2o.gainsLift() %&gt;%\n            as.tibble() %&gt;%\n            select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)\n    }\n    \n    gain_lift_tbl &lt;- leaderboard_tbl %&gt;%\n        mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %&gt;%\n        unnest(cols = metrics) %&gt;%\n        mutate(model_id = as_factor(model_id) %&gt;%\n                 fct_reorder(!! order_by_expr,\n                             .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n               auc = auc %&gt;% round(3) %&gt;% as.character() %&gt;%\n                 as_factor() %&gt;% fct_reorder(as.numeric(model_id)),\n               logloss = logloss %&gt;% round(4) %&gt;% as.character() %&gt;%\n                 as_factor() %&gt;% fct_reorder(as.numeric(model_id))) %&gt;%\n        rename(gain = cumulative_capture_rate,\n               lift = cumulative_lift) \n\n    p3 &lt;- gain_lift_tbl %&gt;%\n        ggplot(aes(cumulative_data_fraction, gain, \n                          color = model_id, linetype = !! order_by_expr)) +\n        geom_line(size = size,) +\n        geom_segment(x = 0, y = 0, xend = 1, yend = 1, \n                     color = \"red\", size = size, linetype = \"dotted\") +\n        theme_new +\n        expand_limits(x = c(0, 1), y = c(0, 1)) +\n        labs(title = \"Gain\",\n             x = \"Cumulative Data Fraction\", y = \"Gain\") +\n        theme(legend.position = \"none\")\n\n    p4 &lt;- gain_lift_tbl %&gt;%\n        ggplot(aes(cumulative_data_fraction, lift, \n                          color = model_id, linetype = !! order_by_expr)) +\n        geom_line(size = size) +\n        geom_segment(x = 0, y = 1, xend = 1, yend = 1, \n                     color = \"red\", size = size, linetype = \"dotted\") +\n        theme_new +\n        expand_limits(x = c(0, 1), y = c(0, 1)) +\n        labs(title = \"Lift\",\n             x = \"Cumulative Data Fraction\", y = \"Lift\") +\n        theme(legend.position = \"none\") \n  \n    p_legend &lt;- get_legend(p1)\n    p1 &lt;- p1 + theme(legend.position = \"none\")\n    p &lt;- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)\n    p_title &lt;- ggdraw() +\n      draw_label(\"H2O Model Metrics\", size = 18, fontface = \"bold\",\n                 color = \"#2C3E50\")\n    p_subtitle &lt;- ggdraw() +\n      draw_label(glue(\"Ordered by {toupper(order_by)}\"), size = 10,\n                 color = \"#2C3E50\")\n    \n    ret &lt;- plot_grid(p_title, p_subtitle, p, p_legend,\n                     ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))\n    h2o.show_progress()\n    return(ret)\n}\n\nautoml_models_h2o@leaderboard %&gt;%\n    plot_h2o_performance(newdata = test_tbl, order_by = \"logloss\", \n                         size = 0.5, max_models = 4)"
  },
  {
    "objectID": "content/01_journal/04_H20_II.html",
    "href": "content/01_journal/04_H20_II.html",
    "title": "Automated Machine Learning with H20 (II)",
    "section": "",
    "text": "Challenge: The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales. It’s a classic Binary Classification problem.\nCode:\nLoading libraries\n\nlibrary(tidymodels)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(h2o)\n\nLoading the training & test dataset\n\nproduct_backorders_tbl &lt;- read.csv(\"product_backorders.csv\")\nsplit_obj &lt;- initial_split(product_backorders_tbl, prop = 0.8)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\n\nSpecifying the response and predictor variables\nrunning AutoML specifying the stopping criterion\nView the leaderboard\nPredicting using Leader Model\nSave the leader model"
  },
  {
    "objectID": "content/01_journal/03_H20_I.html",
    "href": "content/01_journal/03_H20_I.html",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "",
    "text": "Challenge: IBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. In this regard, we are going to answer several questions based on the visualizations.\nCode:\n\n1 Business & Data Understanding: Department and Job Role\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(stringr)\nemployee_attrition_tbl &lt;- read.csv(\"Employee_Attrition.csv\")\ndept_job_role_tbl &lt;- employee_attrition_tbl %&gt;%\n  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\ndept_job_role_tbl %&gt;%\n  group_by(Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n2 Attrition by department\n\ndept_job_role_tbl %&gt;%\n    group_by(Department, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department) %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n3 Attrition by job role\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\")\n\n\n\n  \n\n\n\n\n4 Develop KPI\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\") %&gt;%\n  arrange(desc(pct)) %&gt;%\n  mutate(\n    above_industry_avg = case_when(\n      pct &gt; 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"))\n\n\n\n  \n\n\n\n\n5 Function to calculate attrition cost\n\ncalculate_attrition_cost &lt;- function(\n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50) {\n  # Direct Costs\n  direct_cost &lt;- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  # Lost Productivity Costs\n  productivity_cost &lt;- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction &lt;- salary / workdays_per_year * workdays_position_open\n  # Estimated Turnover Per Employee\n  cost_per_employee &lt;- direct_cost + productivity_cost - salary_benefit_reduction\n  # Total Cost of Employee Turnover\n  total_cost &lt;- n * cost_per_employee\n  return(total_cost)}\n\n\n6 Function to convert counts to percentages.\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition)\n\n\n\n  \n\n\ncount_to_pct &lt;- function(data, ..., col = n) {\n  grouping_vars_expr &lt;- quos(...)\n  col_expr &lt;- enquo(col)\n  ret &lt;- data %&gt;%\n    group_by(!!! grouping_vars_expr) %&gt;%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %&gt;%\n    ungroup()\n  return(ret)}\ndept_job_role_tbl %&gt;%\n  count(JobRole, Attrition) %&gt;%\n  count_to_pct(JobRole)\n\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) \n\n\n\n  \n\n\n\n\n7 Assess Attrition Function\n\nassess_attrition &lt;- function(data, attrition_col, attrition_value, baseline_pct) {\n  attrition_col_expr &lt;- enquo(attrition_col)\n  data %&gt;%\n    filter((!! attrition_col_expr) %in% attrition_value) %&gt;%\n    arrange(desc(pct)) %&gt;%\n    mutate(above_industry_avg = case_when(\n        pct &gt; baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"))}\n\n\n8 Visualization\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)) %&gt;%\n  mutate(name = str_c(Department, JobRole, sep = \": \") %&gt;% as_factor()) %&gt;%\n  mutate(name      = fct_reorder(name, cost_of_attrition)) %&gt;%\n  mutate(cost_text = str_c(\"$\", format(cost_of_attrition / 1e6, digits = 2),\n                           \"M\", sep = \"\")) %&gt;%\n  ggplot(aes(cost_of_attrition, y = name)) +\n  geom_segment(aes(xend = 0, yend = name),    color = \"#2dc6d6\") +\n  geom_point(  aes(size = cost_of_attrition), color = \"#2dc6d6\") +\n  scale_x_continuous(labels = scales::dollar) +\n  geom_label(aes(label = cost_text, size = cost_of_attrition),\n             hjust = \"inward\", color = \"#2dc6d6\") +\n  scale_size(range = c(3, 5)) +\n  labs(title = \"Estimated cost of Attrition: By Dept and Job Role\",\n       y = \"\", x = \"Cost of attrition\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# Descriptive Features\nemployee_attrition_tbl %&gt;% select(Age, DistanceFromHome, Gender, MaritalStatus, NumCompaniesWorked, Over18)\n\n\n\n  \n\n\n\n\n# Employment Features\nemployee_attrition_tbl %&gt;% select(Department, EmployeeCount, EmployeeNumber, JobInvolvement, JobLevel, JobRole, JobSatisfaction)\n\n\n\n  \n\n\n\n\n# Compensation Features\nemployee_attrition_tbl %&gt;% select(DailyRate, HourlyRate, MonthlyIncome, MonthlyRate, PercentSalaryHike, StockOptionLevel)\n\n\n\n  \n\n\n\n\n# Survery Results\nemployee_attrition_tbl %&gt;% select(EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance)\n\n\n\n  \n\n\n\n\n# Performance Data\nemployee_attrition_tbl %&gt;% select(JobInvolvement, PerformanceRating)\n\n\n\n  \n\n\n\n\n# Work-Life Features\nemployee_attrition_tbl %&gt;% select(BusinessTravel, OverTime)\n\n\n\n  \n\n\n\n\n# Training & Education\nemployee_attrition_tbl %&gt;% select(Education, EducationField, TrainingTimesLastYear)\n\n\n\n  \n\n\n\n\n# Time-Based Features\nemployee_attrition_tbl %&gt;% select(TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)\n\n\n\n  \n\n\n\n\nlibrary(GGally)\nemployee_attrition_tbl %&gt;%\n  select(Attrition,  Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n9 Explore Features by Category\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n  color_expr &lt;- enquo(color)\n  if (rlang::quo_is_null(color_expr)) {\n    g &lt;- data %&gt;%\n      ggpairs(lower = \"blank\") \n    } else {\n    color_name &lt;- quo_name(color_expr)\n    g &lt;- data %&gt;%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")}\n  return(g)}\n\n\n10 Challanges\nDescriptive features: age, gender, marital status\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nEmployment features: department, job role, job level\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nCompensation features: HourlyRate, MonthlyIncome, StockOptionLevel\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 1: What can you deduce about the interaction between Monthly Income and Attrition?\nAnswer: Those that are leaving have a lower Monthly Income\nQuestion 2:What can you deduce about the interaction between Percent Salary Hike and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization\nQuestion 3:What can you deduce about the interaction between Stock Option Level and Attrition?\nAnswer: Those that are staying have a higher stock option level\nSurvey Results: Satisfaction level, Work Life Balance\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 4: What can you deduce about the interaction between Environment Satisfaction and Attrition?\nAnswer: A higher proportion of those leaving have a low environment satisfaction level\nQuestion 5:What can you deduce about the interaction between Work Life Balance and Attrition.\nAnswer: Those that are staying have a higher density of 2’s and 3’s\nPerformance Data: Job Involvement, Performance Rating\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 6: What Can you deduce about the interaction between Job Involvement and Attrition?\nAnswer: Those that are leaving have a lower density of 3’s and 4’s\nWork-Life Features\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 7: What can you deduce about the interaction between Over Time and Attrition?\nAnswer: The proportion of those staying that are working Over Time are high compared to those that are not staying\nTraining and Education\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 8: What can you deduce about the interaction between Training Times Last Year and Attrition.\nAnswer: People that leave tend to have less annual training\nTime-Based Features: Years at company, years in current role\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"years\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 9: What can you deduce about the interaction between Years At Company and Attrition.\nAnswer: People that leave tend to have less working years at the company\nQuestion 10: What can you deduce about the interaction between Years Since Last Promotion and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/02_Regression.html",
    "href": "content/01_journal/02_Regression.html",
    "title": "Supervised ML - Regression",
    "section": "",
    "text": "Challenge: Our goal is to figure out what gaps exist in the products and come up with a pricing algorithm that will help us to determine a price, if we were to come up with products in that product category.\nCode:\nloading libraries\n\nlibrary(tidyverse)\nlibrary(parsnip)\nlibrary(tidymodels)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(rpart.plot)\n\nBuilding the model\n\nbike_orderlines_tbl &lt;- readRDS(\"bike_orderlines.rds\")\nmodel_sales_tbl &lt;- bike_orderlines_tbl %&gt;%\n  select(total_price, model, category_2, frame_material) %&gt;%\n  group_by(model, category_2, frame_material) %&gt;%\n  summarise(total_sales = sum(total_price)) %&gt;%\n  ungroup() %&gt;% arrange(desc(total_sales))\nmodel_sales_tbl %&gt;% mutate(category_2 = as_factor(category_2) %&gt;% \n                             fct_reorder(total_sales, .fun = max) %&gt;% \n                             fct_rev()) %&gt;%\n  ggplot(aes(frame_material, total_sales)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2c3e50\") +\n  facet_wrap(~ category_2) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\", accuracy = 0.1)) +\n  tidyquant::theme_tq() +\n  labs(title = \"Total Sales for Each Model\",x = \"Frame Material\", y = \"Revenue\")\n\n\n\n\n\n\nbike_features_tbl &lt;- readRDS(\"bike_features_tbl.rds\")\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  select(frame_material:gender, `Rear Derailleur`, `Shift Lever`) %&gt;% \n  mutate(`shimano dura-ace`        = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano dura-ace \") %&gt;% as.numeric(),\n         `shimano ultegra`         = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano ultegra \") %&gt;% as.numeric(),\n         `shimano 105`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano 105 \") %&gt;% as.numeric(),\n         `shimano tiagra`          = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano tiagra \") %&gt;% as.numeric(),\n         `Shimano sora`            = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano sora\") %&gt;% as.numeric(),\n         `shimano deore`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore(?! xt)\") %&gt;% as.numeric(),\n         `shimano slx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano slx\") %&gt;% as.numeric(),\n         `shimano grx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano grx\") %&gt;% as.numeric(),\n         `Shimano xt`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore xt |shimano xt \") %&gt;% as.numeric(),\n         `Shimano xtr`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano xtr\") %&gt;% as.numeric(),\n         `Shimano saint`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano saint\") %&gt;% as.numeric(),\n         `SRAM red`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram red\") %&gt;% as.numeric(),\n         `SRAM force`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram force\") %&gt;% as.numeric(),\n         `SRAM rival`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram rival\") %&gt;% as.numeric(),\n         `SRAM apex`               = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram apex\") %&gt;% as.numeric(),\n         `SRAM xx1`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram xx1\") %&gt;% as.numeric(),\n         `SRAM x01`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram x01|sram xo1\") %&gt;% as.numeric(),\n         `SRAM gx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram gx\") %&gt;% as.numeric(),\n         `SRAM nx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram nx\") %&gt;% as.numeric(),\n         `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n         `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n         `Campagnolo potenza`      = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo potenza\") %&gt;% as.numeric(),\n         `Campagnolo super record` = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo super record\") %&gt;% as.numeric(),\n         `shimano nexus`           = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano nexus\") %&gt;% as.numeric(),\n         `shimano alfine`          = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano alfine\") %&gt;% as.numeric()) %&gt;%  \n  select(-c(`Rear Derailleur`, `Shift Lever`)) %&gt;% \n  mutate_if(is.numeric, ~replace(., is.na(.), 0)) \n\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  mutate(id = row_number()) %&gt;% \n  mutate(frame_material = factor(frame_material)) %&gt;%\n  select(id, everything()) \n\nbike_features_tbl %&gt;% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj &lt;- initial_split(bike_features_tbl, prop   = 0.80, strata = \"category_2\")\nsplit_obj %&gt;% training() %&gt;% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj %&gt;% testing() %&gt;% distinct(category_2)\n\n\n\n  \n\n\ntrain_tbl &lt;- training(split_obj)\ntest_tbl  &lt;- testing(split_obj)\ntrain_tbl &lt;- train_tbl %&gt;% set_names(str_replace_all(names(train_tbl), \" |-\", \"_\"))\ntest_tbl  &lt;- test_tbl  %&gt;% set_names(str_replace_all(names(test_tbl), \" |-\", \"_\"))\n\nCreateing features with the recipes package\n\nbike_rec &lt;- recipe(frame_material ~ ., data = train_tbl) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  step_zv(all_predictors()) \n\nBundling the model and recipe with the workflow package\n\nlr_mod &lt;- logistic_reg() %&gt;% set_engine(\"glm\")\nbike_wflow &lt;- workflow() %&gt;%add_model(lr_mod) %&gt;% add_recipe(bike_rec)\nbike_fit &lt;- bike_wflow %&gt;% fit(data = train_tbl)\nbike_fit %&gt;% pull_workflow_fit() %&gt;% tidy()\n\n\n\n  \n\n\nbike_predict &lt;- predict(bike_fit, test_tbl, type=\"prob\") %&gt;% \n  bind_cols(test_tbl %&gt;% select(frame_material, category_2)) \nbike_predict %&gt;% roc_curve(truth = frame_material, .pred_aluminium) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_predict %&gt;% \n  roc_curve(truth = frame_material, .pred_carbon) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_predict\n\n\n\n  \n\n\nbike_predict %&gt;% roc_auc(truth = frame_material, .pred_aluminium)\n\n\n\n  \n\n\nroc_car &lt;- bike_predict %&gt;% roc_auc(truth = frame_material, .pred_carbon)\n\nEvaluating the model with the yardstick package\n\nmodel_01_linear_lm_simple &lt;- linear_reg(mode = \"regression\") %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ category_2 + frame_material, data = train_tbl)\n\nmodel_01_linear_lm_simple %&gt;%\n  predict(new_data = test_tbl) %&gt;%\n  bind_cols(test_tbl %&gt;% select(price)) %&gt;%\n  metrics(truth = price, estimate = .pred)\n\n\n\n  \n\n\ng1 &lt;- bike_features_tbl %&gt;% \n  mutate(category_2 = as.factor(category_2) %&gt;% \n           fct_reorder(price)) %&gt;% \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2dc6d6\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs( title = \"Unit Price for Each Model\", y = \"\", x = \"Category 2\")\ng1\n\n\n\n\n\n\nnew_trail &lt;- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Trail\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0)\nnew_trail\n\n\n\n  \n\n\npredict(model_01_linear_lm_simple, new_data = new_trail)\n\n\n\n  \n\n\nmodels_tbl &lt;- tibble(\n  model_id = str_c(\"Model 0\", 1:1),\n  model = list(\n    model_01_linear_lm_simple))\n\nmodels_tbl\n\n\n\n  \n\n\npredictions_new_trail_tbl &lt;- models_tbl %&gt;%\n  mutate(predictions = map(model, predict, new_data = new_trail)) %&gt;%\n  unnest(predictions) %&gt;%\n  mutate(category_2 = \"Trail\") %&gt;%\n  left_join(new_trail, by = \"category_2\")\npredictions_new_trail_tbl\n\n\n\n  \n\n\ng2 &lt;- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_trail_tbl) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 5,\n                           data = predictions_new_trail_tbl)\ng2"
  },
  {
    "objectID": "content/01_journal/03_H2O_I.html",
    "href": "content/01_journal/03_H2O_I.html",
    "title": "Automated Machine Learning with H2O (I)",
    "section": "",
    "text": "Challenge: IBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. In this regard, we are going to answer several questions based on the visualizations.\nCode:\n\n1 Business & Data Understanding: Department and Job Role\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(stringr)\nemployee_attrition_tbl &lt;- read.csv(\"Employee_Attrition.csv\")\ndept_job_role_tbl &lt;- employee_attrition_tbl %&gt;%\n  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\ndept_job_role_tbl %&gt;%\n  group_by(Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n2 Attrition by department\n\ndept_job_role_tbl %&gt;%\n    group_by(Department, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department) %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n3 Attrition by job role\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\")\n\n\n\n  \n\n\n\n\n4 Develop KPI\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\") %&gt;%\n  arrange(desc(pct)) %&gt;%\n  mutate(\n    above_industry_avg = case_when(\n      pct &gt; 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"))\n\n\n\n  \n\n\n\n\n5 Function to calculate attrition cost\n\ncalculate_attrition_cost &lt;- function(\n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50) {\n  # Direct Costs\n  direct_cost &lt;- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  # Lost Productivity Costs\n  productivity_cost &lt;- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction &lt;- salary / workdays_per_year * workdays_position_open\n  # Estimated Turnover Per Employee\n  cost_per_employee &lt;- direct_cost + productivity_cost - salary_benefit_reduction\n  # Total Cost of Employee Turnover\n  total_cost &lt;- n * cost_per_employee\n  return(total_cost)}\n\n\n6 Function to convert counts to percentages.\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition)\n\n\n\n  \n\n\ncount_to_pct &lt;- function(data, ..., col = n) {\n  grouping_vars_expr &lt;- quos(...)\n  col_expr &lt;- enquo(col)\n  ret &lt;- data %&gt;%\n    group_by(!!! grouping_vars_expr) %&gt;%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %&gt;%\n    ungroup()\n  return(ret)}\ndept_job_role_tbl %&gt;%\n  count(JobRole, Attrition) %&gt;%\n  count_to_pct(JobRole)\n\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) \n\n\n\n  \n\n\n\n\n7 Assess Attrition Function\n\nassess_attrition &lt;- function(data, attrition_col, attrition_value, baseline_pct) {\n  attrition_col_expr &lt;- enquo(attrition_col)\n  data %&gt;%\n    filter((!! attrition_col_expr) %in% attrition_value) %&gt;%\n    arrange(desc(pct)) %&gt;%\n    mutate(above_industry_avg = case_when(\n        pct &gt; baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"))}\n\n\n8 Visualization\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)) %&gt;%\n  mutate(name = str_c(Department, JobRole, sep = \": \") %&gt;% as_factor()) %&gt;%\n  mutate(name      = fct_reorder(name, cost_of_attrition)) %&gt;%\n  mutate(cost_text = str_c(\"$\", format(cost_of_attrition / 1e6, digits = 2),\n                           \"M\", sep = \"\")) %&gt;%\n  ggplot(aes(cost_of_attrition, y = name)) +\n  geom_segment(aes(xend = 0, yend = name),    color = \"#2dc6d6\") +\n  geom_point(  aes(size = cost_of_attrition), color = \"#2dc6d6\") +\n  scale_x_continuous(labels = scales::dollar) +\n  geom_label(aes(label = cost_text, size = cost_of_attrition),\n             hjust = \"inward\", color = \"#2dc6d6\") +\n  scale_size(range = c(3, 5)) +\n  labs(title = \"Estimated cost of Attrition: By Dept and Job Role\",\n       y = \"\", x = \"Cost of attrition\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# Descriptive Features\nemployee_attrition_tbl %&gt;% select(Age, DistanceFromHome, Gender, MaritalStatus, NumCompaniesWorked, Over18)\n\n\n\n  \n\n\n\n\n# Employment Features\nemployee_attrition_tbl %&gt;% select(Department, EmployeeCount, EmployeeNumber, JobInvolvement, JobLevel, JobRole, JobSatisfaction)\n\n\n\n  \n\n\n\n\n# Compensation Features\nemployee_attrition_tbl %&gt;% select(DailyRate, HourlyRate, MonthlyIncome, MonthlyRate, PercentSalaryHike, StockOptionLevel)\n\n\n\n  \n\n\n\n\n# Survery Results\nemployee_attrition_tbl %&gt;% select(EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance)\n\n\n\n  \n\n\n\n\n# Performance Data\nemployee_attrition_tbl %&gt;% select(JobInvolvement, PerformanceRating)\n\n\n\n  \n\n\n\n\n# Work-Life Features\nemployee_attrition_tbl %&gt;% select(BusinessTravel, OverTime)\n\n\n\n  \n\n\n\n\n# Training & Education\nemployee_attrition_tbl %&gt;% select(Education, EducationField, TrainingTimesLastYear)\n\n\n\n  \n\n\n\n\n# Time-Based Features\nemployee_attrition_tbl %&gt;% select(TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)\n\n\n\n  \n\n\n\n\nlibrary(GGally)\nemployee_attrition_tbl %&gt;%\n  select(Attrition,  Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n9 Explore Features by Category\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n  color_expr &lt;- enquo(color)\n  if (rlang::quo_is_null(color_expr)) {\n    g &lt;- data %&gt;%\n      ggpairs(lower = \"blank\") \n    } else {\n    color_name &lt;- quo_name(color_expr)\n    g &lt;- data %&gt;%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")}\n  return(g)}\n\n\n10 Challanges\nDescriptive features: age, gender, marital status\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nEmployment features: department, job role, job level\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nCompensation features: HourlyRate, MonthlyIncome, StockOptionLevel\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 1: What can you deduce about the interaction between Monthly Income and Attrition?\nAnswer: Those that are leaving have a lower Monthly Income\nQuestion 2:What can you deduce about the interaction between Percent Salary Hike and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization\nQuestion 3:What can you deduce about the interaction between Stock Option Level and Attrition?\nAnswer: Those that are staying have a higher stock option level\nSurvey Results: Satisfaction level, Work Life Balance\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 4: What can you deduce about the interaction between Environment Satisfaction and Attrition?\nAnswer: A higher proportion of those leaving have a low environment satisfaction level\nQuestion 5:What can you deduce about the interaction between Work Life Balance and Attrition.\nAnswer: Those that are staying have a higher density of 2’s and 3’s\nPerformance Data: Job Involvement, Performance Rating\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 6: What Can you deduce about the interaction between Job Involvement and Attrition?\nAnswer: Those that are leaving have a lower density of 3’s and 4’s\nWork-Life Features\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 7: What can you deduce about the interaction between Over Time and Attrition?\nAnswer: The proportion of those staying that are working Over Time are high compared to those that are not staying\nTraining and Education\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 8: What can you deduce about the interaction between Training Times Last Year and Attrition.\nAnswer: People that leave tend to have less annual training\nTime-Based Features: Years at company, years in current role\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"years\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 9: What can you deduce about the interaction between Years At Company and Attrition.\nAnswer: People that leave tend to have less working years at the company\nQuestion 10: What can you deduce about the interaction between Years Since Last Promotion and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/04_H2O_II.html",
    "href": "content/01_journal/04_H2O_II.html",
    "title": "Automated Machine Learning with H2O (II)",
    "section": "",
    "text": "Challenge: The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales. It’s a classic Binary Classification problem.\nCode:\nLoading libraries\n\nlibrary(tidymodels)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(h2o)\n\nLoading the training & test dataset\n\nproduct_backorders_tbl &lt;- read.csv(\"product_backorders.csv\")\nsplit_obj &lt;- initial_split(product_backorders_tbl, prop = 0.8)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\n\nSpecifying the response and predictor variables\n\nfactor_name &lt;- \"went_on_backorder\"\nrecipe_obj &lt;- \n  recipe(went_on_backorder ~ ., data = train_readable_tbl) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_mutate_at(factor_name, fn = as.factor) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  prep()\ntrain_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  &lt;- bake(recipe_obj, new_data = test_readable_tbl)\n\nh2o.init()\n\n#&gt;  Connection successful!\n#&gt; \n#&gt; R is connected to the H2O cluster: \n#&gt;     H2O cluster uptime:         1 hours 21 minutes \n#&gt;     H2O cluster timezone:       Europe/Berlin \n#&gt;     H2O data parsing timezone:  UTC \n#&gt;     H2O cluster version:        3.40.0.4 \n#&gt;     H2O cluster version age:    1 month and 16 days \n#&gt;     H2O cluster name:           H2O_started_from_R_arash_mtu845 \n#&gt;     H2O cluster total nodes:    1 \n#&gt;     H2O cluster total memory:   2.95 GB \n#&gt;     H2O cluster total cores:    16 \n#&gt;     H2O cluster allowed cores:  16 \n#&gt;     H2O cluster healthy:        TRUE \n#&gt;     H2O Connection ip:          localhost \n#&gt;     H2O Connection port:        54321 \n#&gt;     H2O Connection proxy:       NA \n#&gt;     H2O Internal Security:      FALSE \n#&gt;     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\nsplit_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.8), seed = 1234)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o &lt;- split_h2o[[1]]\nvalid_h2o &lt;- split_h2o[[2]]\ntest_h2o  &lt;- as.h2o(test_tbl)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ny &lt;- \"went_on_backorder\"\nx &lt;- setdiff(names(train_h2o), y)\n\nrunning AutoML specifying the stopping criterion\n\nautoml_models_h2o &lt;- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 60,\n  nfolds            = 5)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n#&gt; 11:05:38.915: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#&gt; 11:05:38.924: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nView the leaderboard\n\ntypeof(automl_models_h2o)\n\n#&gt; [1] \"S4\"\n\nslotNames(automl_models_h2o)\n\n#&gt; [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#&gt; [5] \"modeling_steps\" \"training_info\"\n\nautoml_models_h2o@leaderboard \n\n#&gt;                                                  model_id       auc   logloss\n#&gt; 1 StackedEnsemble_BestOfFamily_3_AutoML_4_20230614_110538 0.9488040 0.1799906\n#&gt; 2    StackedEnsemble_AllModels_2_AutoML_4_20230614_110538 0.9479030 0.1793181\n#&gt; 3 StackedEnsemble_BestOfFamily_2_AutoML_4_20230614_110538 0.9473351 0.1818747\n#&gt; 4    StackedEnsemble_AllModels_1_AutoML_4_20230614_110538 0.9470606 0.1803002\n#&gt; 5                          GBM_4_AutoML_4_20230614_110538 0.9455496 0.1867345\n#&gt; 6                          GBM_3_AutoML_4_20230614_110538 0.9438204 0.1886154\n#&gt;       aucpr mean_per_class_error      rmse        mse\n#&gt; 1 0.7347243            0.1577340 0.2321545 0.05389569\n#&gt; 2 0.7377050            0.1553757 0.2319841 0.05381662\n#&gt; 3 0.7317575            0.1548483 0.2338136 0.05466878\n#&gt; 4 0.7364481            0.1524900 0.2327440 0.05416978\n#&gt; 5 0.7259580            0.1598236 0.2371759 0.05625239\n#&gt; 6 0.7119992            0.1642018 0.2383700 0.05682025\n#&gt; \n#&gt; [20 rows x 7 columns]\n\nautoml_models_h2o@leader\n\n#&gt; Model Details:\n#&gt; ==============\n#&gt; \n#&gt; H2OBinomialModel: stackedensemble\n#&gt; Model ID:  StackedEnsemble_BestOfFamily_3_AutoML_4_20230614_110538 \n#&gt; Model Summary for Stacked Ensemble: \n#&gt;                                          key            value\n#&gt; 1                          Stacking strategy cross_validation\n#&gt; 2       Number of base models (used / total)              4/5\n#&gt; 3           # GBM base models (used / total)              1/1\n#&gt; 4           # DRF base models (used / total)              2/2\n#&gt; 5  # DeepLearning base models (used / total)              0/1\n#&gt; 6           # GLM base models (used / total)              1/1\n#&gt; 7                      Metalearner algorithm              GLM\n#&gt; 8         Metalearner fold assignment scheme           Random\n#&gt; 9                         Metalearner nfolds                5\n#&gt; 10                   Metalearner fold_column               NA\n#&gt; 11        Custom metalearner hyperparameters             None\n#&gt; \n#&gt; \n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on training data. **\n#&gt; \n#&gt; MSE:  0.01988672\n#&gt; RMSE:  0.1410203\n#&gt; LogLoss:  0.08327443\n#&gt; Mean Per-Class Error:  0.05088512\n#&gt; AUC:  0.9940476\n#&gt; AUCPR:  0.9674395\n#&gt; Gini:  0.9880951\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No  Yes    Error        Rate\n#&gt; No     8736  114 0.012881   =114/8850\n#&gt; Yes     104 1066 0.088889   =104/1170\n#&gt; Totals 8840 1180 0.021756  =218/10020\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.413106    0.907234 175\n#&gt; 2                       max f2  0.347586    0.923219 194\n#&gt; 3                 max f0point5  0.559144    0.934236 136\n#&gt; 4                 max accuracy  0.413106    0.978244 175\n#&gt; 5                max precision  0.996312    1.000000   0\n#&gt; 6                   max recall  0.030973    1.000000 336\n#&gt; 7              max specificity  0.996312    1.000000   0\n#&gt; 8             max absolute_mcc  0.413106    0.894921 175\n#&gt; 9   max min_per_class_accuracy  0.251787    0.961538 220\n#&gt; 10 max mean_per_class_accuracy  0.251787    0.962577 220\n#&gt; 11                     max tns  0.996312 8850.000000   0\n#&gt; 12                     max fns  0.996312 1168.000000   0\n#&gt; 13                     max fps  0.000891 8850.000000 399\n#&gt; 14                     max tps  0.030973 1170.000000 336\n#&gt; 15                     max tnr  0.996312    1.000000   0\n#&gt; 16                     max fnr  0.996312    0.998291   0\n#&gt; 17                     max fpr  0.000891    1.000000 399\n#&gt; 18                     max tpr  0.030973    1.000000 336\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on validation data. **\n#&gt; \n#&gt; MSE:  0.05172776\n#&gt; RMSE:  0.2274374\n#&gt; LogLoss:  0.1743846\n#&gt; Mean Per-Class Error:  0.136351\n#&gt; AUC:  0.9523361\n#&gt; AUCPR:  0.7530712\n#&gt; Gini:  0.9046723\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No Yes    Error       Rate\n#&gt; No     2470 157 0.059764  =157/2627\n#&gt; Yes      79 292 0.212938    =79/371\n#&gt; Totals 2549 449 0.078719  =236/2998\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.261585    0.712195 205\n#&gt; 2                       max f2  0.099105    0.784133 276\n#&gt; 3                 max f0point5  0.590540    0.749141 117\n#&gt; 4                 max accuracy  0.590540    0.931288 117\n#&gt; 5                max precision  0.985271    1.000000   0\n#&gt; 6                   max recall  0.006434    1.000000 384\n#&gt; 7              max specificity  0.985271    1.000000   0\n#&gt; 8             max absolute_mcc  0.261585    0.671153 205\n#&gt; 9   max min_per_class_accuracy  0.119492    0.884097 264\n#&gt; 10 max mean_per_class_accuracy  0.099105    0.892747 276\n#&gt; 11                     max tns  0.985271 2627.000000   0\n#&gt; 12                     max fns  0.985271  370.000000   0\n#&gt; 13                     max fps  0.001346 2627.000000 399\n#&gt; 14                     max tps  0.006434  371.000000 384\n#&gt; 15                     max tnr  0.985271    1.000000   0\n#&gt; 16                     max fnr  0.985271    0.997305   0\n#&gt; 17                     max fpr  0.001346    1.000000 399\n#&gt; 18                     max tpr  0.006434    1.000000 384\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on cross-validation data. **\n#&gt; ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#&gt; \n#&gt; MSE:  0.05067493\n#&gt; RMSE:  0.2251109\n#&gt; LogLoss:  0.170884\n#&gt; Mean Per-Class Error:  0.1483047\n#&gt; AUC:  0.9500179\n#&gt; AUCPR:  0.7442311\n#&gt; Gini:  0.9000357\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;           No  Yes    Error        Rate\n#&gt; No     10227  583 0.053932  =583/10810\n#&gt; Yes      348 1086 0.242678   =348/1434\n#&gt; Totals 10575 1669 0.076037  =931/12244\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold        value idx\n#&gt; 1                       max f1  0.284034     0.699968 217\n#&gt; 2                       max f2  0.100082     0.778271 296\n#&gt; 3                 max f0point5  0.613413     0.724352 119\n#&gt; 4                 max accuracy  0.524193     0.930170 143\n#&gt; 5                max precision  0.992399     1.000000   0\n#&gt; 6                   max recall  0.000664     1.000000 399\n#&gt; 7              max specificity  0.992399     1.000000   0\n#&gt; 8             max absolute_mcc  0.284034     0.659198 217\n#&gt; 9   max min_per_class_accuracy  0.112525     0.887727 288\n#&gt; 10 max mean_per_class_accuracy  0.089940     0.893685 302\n#&gt; 11                     max tns  0.992399 10810.000000   0\n#&gt; 12                     max fns  0.992399  1430.000000   0\n#&gt; 13                     max fps  0.000664 10810.000000 399\n#&gt; 14                     max tps  0.000664  1434.000000 399\n#&gt; 15                     max tnr  0.992399     1.000000   0\n#&gt; 16                     max fnr  0.992399     0.997211   0\n#&gt; 17                     max fpr  0.000664     1.000000 399\n#&gt; 18                     max tpr  0.000664     1.000000 399\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; Cross-Validation Metrics Summary: \n#&gt;                 mean       sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#&gt; accuracy    0.925490 0.004220   0.918786   0.930344   0.925558   0.927139\n#&gt; auc         0.950425 0.005823   0.941138   0.948722   0.953207   0.952942\n#&gt; err         0.074510 0.004220   0.081214   0.069656   0.074442   0.072861\n#&gt; err_count 182.400000 9.208692 198.000000 174.000000 180.000000 178.000000\n#&gt; f0point5    0.676313 0.015459   0.678694   0.699395   0.671506   0.675501\n#&gt;           cv_5_valid\n#&gt; accuracy    0.925623\n#&gt; auc         0.956115\n#&gt; err         0.074377\n#&gt; err_count 182.000000\n#&gt; f0point5    0.656469\n#&gt; \n#&gt; ---\n#&gt;                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#&gt; precision           0.659609  0.023032   0.662011   0.695652   0.647230\n#&gt; r2                  0.510163  0.011120   0.511868   0.510526   0.522359\n#&gt; recall              0.754899  0.027372   0.754777   0.714777   0.790036\n#&gt; residual_deviance 836.364440 50.107117 918.040100 851.288000 802.788300\n#&gt; rmse                0.224927  0.005224   0.234033   0.224450   0.221488\n#&gt; specificity         0.948127  0.006581   0.943032   0.958768   0.943379\n#&gt;                   cv_4_valid cv_5_valid\n#&gt; precision           0.659306   0.633846\n#&gt; r2                  0.514012   0.492052\n#&gt; recall              0.749104   0.765799\n#&gt; residual_deviance 809.092600 800.613100\n#&gt; rmse                0.221728   0.222936\n#&gt; specificity         0.950092   0.945363\n\nextract_h2o_model_name_by_position &lt;- function(h2o_leaderboard,\n                                                n= 1, verbose = T){\n   model_name &lt;- h2o_leaderboard %&gt;%\n     as_tibble() %&gt;%\n     slice(n) %&gt;%\n     pull(model_id)\n   if (verbose) message(model_name)\n   return(model_name)}\n\nPredicting using Leader Model\n\n# StackedEnsemble_BestOfFamily_1_AutoML_3_20230614_110021\nstacked_ensemble_h2o &lt;- automl_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(11) %&gt;% \n  h2o.getModel()\n\npredictions &lt;- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#&gt; [1] \"environment\"\n\npredictions_tbl &lt;- predictions %&gt;% as_tibble()\npredictions_tbl\n\n\n\n  \n\n\n\nSave the leader model\n\n# StackedEnsemble_BestOfFamily_1_AutoML_3_20230614_110021\nautoml_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(11) %&gt;% \n  h2o.getModel() %&gt;%\n  h2o.saveModel(path = \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\")\n\n#&gt; [1] \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\\\\GBM_5_AutoML_4_20230614_110538\""
  }
]