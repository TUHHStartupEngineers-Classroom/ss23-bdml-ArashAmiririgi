[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\nThis is a .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#header-2",
    "href": "content/01_journal/01_tidyverse.html#header-2",
    "title": "Tidyverse",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "content/03_other/06_links.html#r-and-r-studio",
    "href": "content/03_other/06_links.html#r-and-r-studio",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual ."
  },
  {
    "objectID": "content/03_other/06_links.html#additional-r-resources",
    "href": "content/03_other/06_links.html#additional-r-resources",
    "title": "Links",
    "section": "",
    "text": "Google is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MyLabJournal",
    "section": "",
    "text": "09— title: “My Lab Journal” subtitle: “Business Decisions with Machine Learning” author: “Arash Amiririgi”"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/01_journal/01_Fundamentals.html",
    "href": "content/01_journal/01_Fundamentals.html",
    "title": "Machine Learning Fundamentals",
    "section": "",
    "text": "Challenge:\nYour organization wants to know which companies are similar to each other to help in identifying potential customers of a SAAS software solution (e.g. Sales force CRM or equivalent) in various segments of the market. The Sales Department is very interested in this analysis, which will help them more easily penetrate various market segments.\nCode:\n\nknitr::opts_chunk$set(\n    echo = TRUE,\n    message = FALSE,\n    warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(broom)\nlibrary(umap)\nsp_500_prices_tbl &lt;- read_rds(\"sp_500_prices_tbl.rds\")\nsp_500_prices_tbl\n\n\n\n  \n\n\nsp_500_index_tbl &lt;- read_rds(\"sp_500_index_tbl.rds\")\nsp_500_index_tbl\n\n\n\n  \n\n\n## Step 1 - Convert stock prices to a standardized format (daily returns)\nsp_500_daily_returns_tbl &lt;- sp_500_prices_tbl %&gt;% select(symbol,date, adjusted) %&gt;% filter(year(date) &gt;= '2018') %&gt;% group_by(symbol) %&gt;%\n  mutate(lag = lag(adjusted, n = 1)) %&gt;% filter(!(is.na(lag))) %&gt;% mutate(pct_return = (adjusted - lag)/lag) %&gt;%\n  select(symbol, date, pct_return)\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\n## Step 2 - Convert to User-Item Format\nsp_500_daily_returns_tbl &lt;- read_rds(\"sp_500_daily_returns_tbl.rds\")\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\nstock_date_matrix_tbl &lt;- sp_500_daily_returns_tbl %&gt;% pivot_wider(names_from = date, values_from = pct_return, values_fill = 0)\nstock_date_matrix_tbl\n\n\n\n  \n\n\n## Step 3 - Perform K-Means Clustering\nstock_date_matrix_tbl &lt;- read_rds(\"stock_date_matrix_tbl.rds\")\nkmeans_obj &lt;- stock_date_matrix_tbl %&gt;% select(-symbol) %&gt;% kmeans(centers = 4, nstart = 20)\n\n## Step 4 - Find the optimal value of K\nkmeans_mapper &lt;- function(center = 3) {\n    stock_date_matrix_tbl %&gt;%\n        select(-symbol) %&gt;%\n        kmeans(centers = center, nstart = 20)}\nk_means_mapped_tbl &lt;- tibble(centers = 1:30) %&gt;%\n    mutate(k_means = centers %&gt;% map(kmeans_mapper)) %&gt;%\n    mutate(glance  = k_means %&gt;% map(glance))\nk_means_mapped_tbl\n\n\n\n  \n\n\nk_means_mapped_tbl %&gt;% unnest(glance) %&gt;% select(centers, tot.withinss) %&gt;%  ggplot(aes(centers, tot.withinss)) +\n    geom_point(color = \"blue\", size = 4) +\n    geom_line(color = \"blue\", size = 1) +\n    ggrepel::geom_label_repel(aes(label = centers), color = \"blue\")\n\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n## Step 5 - Apply UMAP\nk_means_mapped_tbl &lt;- read_rds(\"k_means_mapped_tbl.rds\")\numap_results &lt;- stock_date_matrix_tbl %&gt;% select(-symbol) %&gt;% umap()\numap_results\n\n#&gt; umap embedding of 502 items in 2 dimensions\n#&gt; object components: layout, data, knn, config\n\numap_results_tbl &lt;- umap_results$layou %&gt;% as_tibble() %&gt;% set_names(c(\"x\", \"y\")) %&gt;% bind_cols(stock_date_matrix_tbl %&gt;% select(symbol))\n\n#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#&gt; `.name_repair` is omitted as of tibble 2.0.0.\n#&gt; ℹ Using compatibility `.name_repair`.\n\numap_results_tbl\n\n\n\n  \n\n\numap_results_tbl %&gt;% ggplot(aes(x, y)) + geom_point(alpha = 0.5) + theme_tq() + labs(title = \"UMAP Projection\")\n\n\n\n\n\n\n## Step 6 - Combine K-Means and UMAP\nk_means_mapped_tbl &lt;- read_rds(\"k_means_mapped_tbl.rds\")\numap_results_tbl   &lt;- read_rds(\"umap_results_tbl.rds\")\nk_means_obj &lt;- k_means_mapped_tbl %&gt;% pull(k_means) %&gt;% pluck(10)\numap_kmeans_results_tbl &lt;- k_means_obj %&gt;% augment(stock_date_matrix_tbl) %&gt;% select(symbol, .cluster) %&gt;% left_join(umap_results_tbl) %&gt;% left_join(sp_500_index_tbl %&gt;% select(symbol, company, sector))\n\n#&gt; Joining with `by = join_by(symbol)`\n\n\n#&gt; Joining with `by = join_by(symbol)`\n\numap_kmeans_results_tbl\n\n\n\n  \n\n\numap_kmeans_results_tbl %&gt;% ggplot(aes(V1, V2, color = .cluster)) + geom_point(alpha = 0.5)"
  },
  {
    "objectID": "content/01_journal/06_LIME.html",
    "href": "content/01_journal/06_LIME.html",
    "title": "Explaining Black-Box Models With LIME",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/05_Performance_Measures.html",
    "href": "content/01_journal/05_Performance_Measures.html",
    "title": "Performance Measures",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_H20_II.html",
    "href": "content/01_journal/04_H20_II.html",
    "title": "Automated Machine Learning with H20 (II)",
    "section": "",
    "text": "Challenge: The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales. It’s a classic Binary Classification problem.\nCode:\nLoading libraries\n\nlibrary(tidymodels)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(h2o)\n\nLoading the training & test dataset\n\nproduct_backorders_tbl &lt;- read.csv(\"product_backorders.csv\")\nsplit_obj &lt;- initial_split(product_backorders_tbl, prop = 0.8)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\n\nSpecifying the response and predictor variables\n\nfactor_name &lt;- \"went_on_backorder\"\nrecipe_obj &lt;- \n  recipe(went_on_backorder ~ ., data = train_readable_tbl) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_mutate_at(factor_name, fn = as.factor) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  prep()\ntrain_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  &lt;- bake(recipe_obj, new_data = test_readable_tbl)\n\nh2o.init()\n\n#&gt;  Connection successful!\n#&gt; \n#&gt; R is connected to the H2O cluster: \n#&gt;     H2O cluster uptime:         1 hours 27 minutes \n#&gt;     H2O cluster timezone:       Europe/Berlin \n#&gt;     H2O data parsing timezone:  UTC \n#&gt;     H2O cluster version:        3.40.0.4 \n#&gt;     H2O cluster version age:    1 month and 16 days \n#&gt;     H2O cluster name:           H2O_started_from_R_arash_xuq175 \n#&gt;     H2O cluster total nodes:    1 \n#&gt;     H2O cluster total memory:   3.43 GB \n#&gt;     H2O cluster total cores:    16 \n#&gt;     H2O cluster allowed cores:  16 \n#&gt;     H2O cluster healthy:        TRUE \n#&gt;     H2O Connection ip:          localhost \n#&gt;     H2O Connection port:        54321 \n#&gt;     H2O Connection proxy:       NA \n#&gt;     H2O Internal Security:      FALSE \n#&gt;     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\nsplit_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.8), seed = 1234)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o &lt;- split_h2o[[1]]\nvalid_h2o &lt;- split_h2o[[2]]\ntest_h2o  &lt;- as.h2o(test_tbl)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ny &lt;- \"went_on_backorder\"\nx &lt;- setdiff(names(train_h2o), y)\n\nrunning AutoML specifying the stopping criterion\n\nautoml_models_h2o &lt;- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 60,\n  nfolds            = 5)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n#&gt; 20:43:37.212: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#&gt; 20:43:37.212: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\n\nView the leaderboard\n\ntypeof(automl_models_h2o)\n\n#&gt; [1] \"S4\"\n\nslotNames(automl_models_h2o)\n\n#&gt; [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#&gt; [5] \"modeling_steps\" \"training_info\"\n\nautoml_models_h2o@leaderboard \n\n#&gt;                                                  model_id       auc   logloss\n#&gt; 1    StackedEnsemble_AllModels_2_AutoML_8_20230613_204337 0.9595313 0.1588653\n#&gt; 2 StackedEnsemble_BestOfFamily_3_AutoML_8_20230613_204337 0.9594498 0.1589417\n#&gt; 3    StackedEnsemble_AllModels_1_AutoML_8_20230613_204337 0.9591020 0.1594432\n#&gt; 4    StackedEnsemble_AllModels_3_AutoML_8_20230613_204337 0.9589611 0.1612780\n#&gt; 5 StackedEnsemble_BestOfFamily_2_AutoML_8_20230613_204337 0.9588411 0.1595268\n#&gt; 6                          GBM_4_AutoML_8_20230613_204337 0.9573232 0.1637474\n#&gt;       aucpr mean_per_class_error      rmse        mse\n#&gt; 1 0.7796274            0.1421152 0.2177478 0.04741409\n#&gt; 2 0.7796499            0.1348532 0.2174852 0.04729982\n#&gt; 3 0.7779701            0.1511687 0.2179775 0.04751420\n#&gt; 4 0.7777847            0.1260473 0.2186593 0.04781191\n#&gt; 5 0.7780271            0.1374406 0.2177811 0.04742859\n#&gt; 6 0.7732219            0.1300775 0.2201066 0.04844691\n#&gt; \n#&gt; [21 rows x 7 columns]\n\nautoml_models_h2o@leader\n\n#&gt; Model Details:\n#&gt; ==============\n#&gt; \n#&gt; H2OBinomialModel: stackedensemble\n#&gt; Model ID:  StackedEnsemble_AllModels_2_AutoML_8_20230613_204337 \n#&gt; Model Summary for Stacked Ensemble: \n#&gt;                                          key            value\n#&gt; 1                          Stacking strategy cross_validation\n#&gt; 2       Number of base models (used / total)              6/9\n#&gt; 3           # GBM base models (used / total)              3/5\n#&gt; 4           # DRF base models (used / total)              2/2\n#&gt; 5  # DeepLearning base models (used / total)              1/1\n#&gt; 6           # GLM base models (used / total)              0/1\n#&gt; 7                      Metalearner algorithm              GLM\n#&gt; 8         Metalearner fold assignment scheme           Random\n#&gt; 9                         Metalearner nfolds                5\n#&gt; 10                   Metalearner fold_column               NA\n#&gt; 11        Custom metalearner hyperparameters             None\n#&gt; \n#&gt; \n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on training data. **\n#&gt; \n#&gt; MSE:  0.02208611\n#&gt; RMSE:  0.148614\n#&gt; LogLoss:  0.0844433\n#&gt; Mean Per-Class Error:  0.06377655\n#&gt; AUC:  0.9931539\n#&gt; AUCPR:  0.9593032\n#&gt; Gini:  0.9863078\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No  Yes    Error       Rate\n#&gt; No     8643  136 0.015492  =136/8779\n#&gt; Yes     131 1038 0.112062  =131/1169\n#&gt; Totals 8774 1174 0.026840  =267/9948\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.432403    0.886044 175\n#&gt; 2                       max f2  0.269531    0.907159 223\n#&gt; 3                 max f0point5  0.613002    0.917219 126\n#&gt; 4                 max accuracy  0.458611    0.973462 166\n#&gt; 5                max precision  0.990972    1.000000   0\n#&gt; 6                   max recall  0.045404    1.000000 326\n#&gt; 7              max specificity  0.990972    1.000000   0\n#&gt; 8             max absolute_mcc  0.432403    0.870835 175\n#&gt; 9   max min_per_class_accuracy  0.239943    0.955804 232\n#&gt; 10 max mean_per_class_accuracy  0.225716    0.957488 237\n#&gt; 11                     max tns  0.990972 8779.000000   0\n#&gt; 12                     max fns  0.990972 1168.000000   0\n#&gt; 13                     max fps  0.000212 8779.000000 399\n#&gt; 14                     max tps  0.045404 1169.000000 326\n#&gt; 15                     max tnr  0.990972    1.000000   0\n#&gt; 16                     max fnr  0.990972    0.999145   0\n#&gt; 17                     max fpr  0.000212    1.000000 399\n#&gt; 18                     max tpr  0.045404    1.000000 326\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on validation data. **\n#&gt; \n#&gt; MSE:  0.05129111\n#&gt; RMSE:  0.2264754\n#&gt; LogLoss:  0.1704496\n#&gt; Mean Per-Class Error:  0.1428026\n#&gt; AUC:  0.9553275\n#&gt; AUCPR:  0.7515667\n#&gt; Gini:  0.910655\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No Yes    Error       Rate\n#&gt; No     2499 125 0.047637  =125/2624\n#&gt; Yes      89 285 0.237968    =89/374\n#&gt; Totals 2588 410 0.071381  =214/2998\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.347739    0.727041 192\n#&gt; 2                       max f2  0.192548    0.804316 245\n#&gt; 3                 max f0point5  0.653719    0.733979 106\n#&gt; 4                 max accuracy  0.545778    0.928619 134\n#&gt; 5                max precision  0.976795    1.000000   0\n#&gt; 6                   max recall  0.003897    1.000000 386\n#&gt; 7              max specificity  0.976795    1.000000   0\n#&gt; 8             max absolute_mcc  0.347739    0.687040 192\n#&gt; 9   max min_per_class_accuracy  0.140085    0.895722 264\n#&gt; 10 max mean_per_class_accuracy  0.152531    0.898887 259\n#&gt; 11                     max tns  0.976795 2624.000000   0\n#&gt; 12                     max fns  0.976795  373.000000   0\n#&gt; 13                     max fps  0.000217 2624.000000 399\n#&gt; 14                     max tps  0.003897  374.000000 386\n#&gt; 15                     max tnr  0.976795    1.000000   0\n#&gt; 16                     max fnr  0.976795    0.997326   0\n#&gt; 17                     max fpr  0.000217    1.000000 399\n#&gt; 18                     max tpr  0.003897    1.000000 386\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on cross-validation data. **\n#&gt; ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#&gt; \n#&gt; MSE:  0.0499158\n#&gt; RMSE:  0.2234185\n#&gt; LogLoss:  0.1676912\n#&gt; Mean Per-Class Error:  0.1537879\n#&gt; AUC:  0.9520227\n#&gt; AUCPR:  0.7486144\n#&gt; Gini:  0.9040453\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;           No  Yes    Error        Rate\n#&gt; No     10294  515 0.047645  =515/10809\n#&gt; Yes      373 1062 0.259930   =373/1435\n#&gt; Totals 10667 1577 0.072525  =888/12244\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold        value idx\n#&gt; 1                       max f1  0.347310     0.705179 198\n#&gt; 2                       max f2  0.135952     0.774683 275\n#&gt; 3                 max f0point5  0.614063     0.731340 114\n#&gt; 4                 max accuracy  0.491468     0.932375 151\n#&gt; 5                max precision  0.984015     1.000000   0\n#&gt; 6                   max recall  0.001859     1.000000 395\n#&gt; 7              max specificity  0.984015     1.000000   0\n#&gt; 8             max absolute_mcc  0.347310     0.664896 198\n#&gt; 9   max min_per_class_accuracy  0.122282     0.886411 282\n#&gt; 10 max mean_per_class_accuracy  0.102987     0.888581 292\n#&gt; 11                     max tns  0.984015 10809.000000   0\n#&gt; 12                     max fns  0.984015  1431.000000   0\n#&gt; 13                     max fps  0.000102 10809.000000 399\n#&gt; 14                     max tps  0.001859  1435.000000 395\n#&gt; 15                     max tnr  0.984015     1.000000   0\n#&gt; 16                     max fnr  0.984015     0.997213   0\n#&gt; 17                     max fpr  0.000102     1.000000 399\n#&gt; 18                     max tpr  0.001859     1.000000 395\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; Cross-Validation Metrics Summary: \n#&gt;                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#&gt; accuracy    0.927167  0.005826   0.921292   0.930734   0.930024   0.933225\n#&gt; auc         0.952258  0.005371   0.952334   0.951327   0.955209   0.958407\n#&gt; err         0.072833  0.005826   0.078708   0.069266   0.069976   0.066775\n#&gt; err_count 178.400000 15.306861 190.000000 167.000000 172.000000 164.000000\n#&gt; f0point5    0.682188  0.027246   0.694137   0.679887   0.695995   0.704779\n#&gt;           cv_5_valid\n#&gt; accuracy    0.920559\n#&gt; auc         0.944011\n#&gt; err         0.079441\n#&gt; err_count 199.000000\n#&gt; f0point5    0.636140\n#&gt; \n#&gt; ---\n#&gt;                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#&gt; precision           0.665103  0.030222   0.674731   0.668990   0.686084\n#&gt; r2                  0.516936  0.034313   0.537159   0.507442   0.524587\n#&gt; recall              0.761856  0.032759   0.784375   0.727273   0.738676\n#&gt; residual_deviance 820.240970 44.830433 860.299560 786.196170 805.291500\n#&gt; rmse                0.223183  0.006483   0.230697   0.219154   0.221424\n#&gt; specificity         0.948941  0.006944   0.942216   0.955752   0.955320\n#&gt;                   cv_4_valid cv_5_valid\n#&gt; precision           0.683284   0.612426\n#&gt; r2                  0.552374   0.463120\n#&gt; recall              0.806228   0.752727\n#&gt; residual_deviance 774.649300 874.768430\n#&gt; rmse                0.215580   0.229060\n#&gt; specificity         0.950161   0.941256\n\nextract_h2o_model_name_by_position &lt;- function(h2o_leaderboard,\n                                                n= 1, verbose = T){\n   model_name &lt;- h2o_leaderboard %&gt;%\n     as_tibble() %&gt;%\n     slice(n) %&gt;%\n     pull(model_id)\n   if (verbose) message(model_name)\n   return(model_name)}\nautoml_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(6) %&gt;% \n  h2o.getModel()\n\n#&gt; Model Details:\n#&gt; ==============\n#&gt; \n#&gt; H2OBinomialModel: gbm\n#&gt; Model ID:  GBM_4_AutoML_8_20230613_204337 \n#&gt; Model Summary: \n#&gt;   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n#&gt; 1              55                       55              107974        10\n#&gt;   max_depth mean_depth min_leaves max_leaves mean_leaves\n#&gt; 1        10   10.00000         55        249   151.20000\n#&gt; \n#&gt; \n#&gt; H2OBinomialMetrics: gbm\n#&gt; ** Reported on training data. **\n#&gt; \n#&gt; MSE:  0.02242102\n#&gt; RMSE:  0.1497365\n#&gt; LogLoss:  0.08753481\n#&gt; Mean Per-Class Error:  0.07261029\n#&gt; AUC:  0.9927806\n#&gt; AUCPR:  0.9597237\n#&gt; Gini:  0.9855611\n#&gt; R^2:  0.7832972\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;           No  Yes    Error        Rate\n#&gt; No     10678  131 0.012120  =131/10809\n#&gt; Yes      191 1244 0.133101   =191/1435\n#&gt; Totals 10869 1375 0.026299  =322/12244\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold        value idx\n#&gt; 1                       max f1  0.428652     0.885409 181\n#&gt; 2                       max f2  0.250936     0.906171 233\n#&gt; 3                 max f0point5  0.558882     0.924988 145\n#&gt; 4                 max accuracy  0.474829     0.974028 169\n#&gt; 5                max precision  0.986735     1.000000   0\n#&gt; 6                   max recall  0.031988     1.000000 344\n#&gt; 7              max specificity  0.986735     1.000000   0\n#&gt; 8             max absolute_mcc  0.428652     0.870816 181\n#&gt; 9   max min_per_class_accuracy  0.213294     0.956055 247\n#&gt; 10 max mean_per_class_accuracy  0.210451     0.957100 248\n#&gt; 11                     max tns  0.986735 10809.000000   0\n#&gt; 12                     max fns  0.986735  1434.000000   0\n#&gt; 13                     max fps  0.001752 10809.000000 399\n#&gt; 14                     max tps  0.031988  1435.000000 344\n#&gt; 15                     max tnr  0.986735     1.000000   0\n#&gt; 16                     max fnr  0.986735     0.999303   0\n#&gt; 17                     max fpr  0.001752     1.000000 399\n#&gt; 18                     max tpr  0.031988     1.000000 344\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: gbm\n#&gt; ** Reported on validation data. **\n#&gt; \n#&gt; MSE:  0.05472785\n#&gt; RMSE:  0.2339398\n#&gt; LogLoss:  0.1823299\n#&gt; Mean Per-Class Error:  0.1540663\n#&gt; AUC:  0.9494766\n#&gt; AUCPR:  0.723596\n#&gt; Gini:  0.8989531\n#&gt; R^2:  0.498771\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No Yes    Error       Rate\n#&gt; No     2489 135 0.051448  =135/2624\n#&gt; Yes      96 278 0.256684    =96/374\n#&gt; Totals 2585 413 0.077051  =231/2998\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.321381    0.706480 199\n#&gt; 2                       max f2  0.124314    0.792389 267\n#&gt; 3                 max f0point5  0.502383    0.724592 145\n#&gt; 4                 max accuracy  0.479034    0.927952 150\n#&gt; 5                max precision  0.988803    1.000000   0\n#&gt; 6                   max recall  0.006860    1.000000 381\n#&gt; 7              max specificity  0.988803    1.000000   0\n#&gt; 8             max absolute_mcc  0.165848    0.669320 250\n#&gt; 9   max min_per_class_accuracy  0.099901    0.891768 279\n#&gt; 10 max mean_per_class_accuracy  0.080503    0.893951 290\n#&gt; 11                     max tns  0.988803 2624.000000   0\n#&gt; 12                     max fns  0.988803  373.000000   0\n#&gt; 13                     max fps  0.001630 2624.000000 399\n#&gt; 14                     max tps  0.006860  374.000000 381\n#&gt; 15                     max tnr  0.988803    1.000000   0\n#&gt; 16                     max fnr  0.988803    0.997326   0\n#&gt; 17                     max fpr  0.001630    1.000000 399\n#&gt; 18                     max tpr  0.006860    1.000000 381\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: gbm\n#&gt; ** Reported on cross-validation data. **\n#&gt; ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#&gt; \n#&gt; MSE:  0.05102825\n#&gt; RMSE:  0.2258943\n#&gt; LogLoss:  0.1733956\n#&gt; Mean Per-Class Error:  0.1564798\n#&gt; AUC:  0.9481252\n#&gt; AUCPR:  0.740813\n#&gt; Gini:  0.8962504\n#&gt; R^2:  0.5068036\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;           No  Yes    Error        Rate\n#&gt; No     10281  528 0.048848  =528/10809\n#&gt; Yes      379 1056 0.264111   =379/1435\n#&gt; Totals 10660 1584 0.074077  =907/12244\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold        value idx\n#&gt; 1                       max f1  0.300217     0.699569 215\n#&gt; 2                       max f2  0.086434     0.771957 303\n#&gt; 3                 max f0point5  0.590689     0.730076 126\n#&gt; 4                 max accuracy  0.480750     0.932538 156\n#&gt; 5                max precision  0.989847     1.000000   0\n#&gt; 6                   max recall  0.002283     1.000000 397\n#&gt; 7              max specificity  0.989847     1.000000   0\n#&gt; 8             max absolute_mcc  0.300217     0.658483 215\n#&gt; 9   max min_per_class_accuracy  0.092731     0.885017 299\n#&gt; 10 max mean_per_class_accuracy  0.072946     0.888071 311\n#&gt; 11                     max tns  0.989847 10809.000000   0\n#&gt; 12                     max fns  0.989847  1433.000000   0\n#&gt; 13                     max fps  0.001249 10809.000000 399\n#&gt; 14                     max tps  0.002283  1435.000000 397\n#&gt; 15                     max tnr  0.989847     1.000000   0\n#&gt; 16                     max fnr  0.989847     0.998606   0\n#&gt; 17                     max fpr  0.001249     1.000000 399\n#&gt; 18                     max tpr  0.002283     1.000000 397\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; Cross-Validation Metrics Summary: \n#&gt;                               mean        sd cv_1_valid cv_2_valid cv_3_valid\n#&gt; accuracy                  0.927965  0.008431   0.940384   0.923234   0.930584\n#&gt; auc                       0.948390  0.004722   0.956375   0.945382   0.949059\n#&gt; err                       0.072035  0.008431   0.059616   0.076766   0.069416\n#&gt; err_count               176.400000 20.647034 146.000000 188.000000 170.000000\n#&gt; f0point5                  0.690766  0.026133   0.708576   0.669731   0.718330\n#&gt; f1                        0.703144  0.017439   0.727612   0.684564   0.704861\n#&gt; f2                        0.716649  0.021832   0.747699   0.700069   0.691888\n#&gt; lift_top_group            7.648982  0.473367   7.653125   8.191777   7.915959\n#&gt; logloss                   0.174107  0.014082   0.149118   0.181448   0.177804\n#&gt; max_per_class_error       0.273663  0.030734   0.238281   0.289199   0.316498\n#&gt; mcc                       0.663200  0.021778   0.695095   0.641498   0.665989\n#&gt; mean_per_class_accuracy   0.840584  0.014271   0.861479   0.831118   0.824093\n#&gt; mean_per_class_error      0.159416  0.014271   0.138520   0.168882   0.175907\n#&gt; mse                       0.051301  0.004925   0.042622   0.054340   0.052701\n#&gt; pr_auc                    0.741621  0.010905   0.745964   0.724769   0.743965\n#&gt; precision                 0.683089  0.034737   0.696429   0.660194   0.727599\n#&gt; r2                        0.504781  0.026031   0.544663   0.474758   0.505468\n#&gt; recall                    0.726337  0.030734   0.761719   0.710801   0.683502\n#&gt; rmse                      0.226274  0.011233   0.206451   0.233109   0.229566\n#&gt; specificity               0.954831  0.009070   0.961240   0.951434   0.964684\n#&gt;                         cv_4_valid cv_5_valid\n#&gt; accuracy                  0.917926   0.927696\n#&gt; auc                       0.945597   0.945537\n#&gt; err                       0.082074   0.072304\n#&gt; err_count               201.000000 177.000000\n#&gt; f0point5                  0.657194   0.700000\n#&gt; f1                        0.688372   0.710311\n#&gt; f2                        0.722656   0.720930\n#&gt; lift_top_group            6.926465   7.557584\n#&gt; logloss                   0.182481   0.179684\n#&gt; max_per_class_error       0.252525   0.271812\n#&gt; mcc                       0.644122   0.669296\n#&gt; mean_per_class_accuracy   0.844462   0.841768\n#&gt; mean_per_class_error      0.155538   0.158232\n#&gt; mse                       0.054296   0.052546\n#&gt; pr_auc                    0.739100   0.754307\n#&gt; precision                 0.637931   0.693291\n#&gt; r2                        0.490497   0.508520\n#&gt; recall                    0.747475   0.728188\n#&gt; rmse                      0.233015   0.229229\n#&gt; specificity               0.941450   0.955349\n\n\nPredicting using Leader Model\n\nstacked_ensemble_h2o &lt;- automl_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(6) %&gt;% \n  h2o.getModel()\n\npredictions &lt;- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#&gt; [1] \"environment\"\n\npredictions_tbl &lt;- predictions %&gt;% as_tibble()\npredictions_tbl\n\n\n\n  \n\n\n\nSave the leader model\n\nautoml_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(6) %&gt;% \n  h2o.getModel() %&gt;%\n  h2o.saveModel(path = \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\")\n\n#&gt; [1] \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\\\\GBM_4_AutoML_8_20230613_204337\""
  },
  {
    "objectID": "content/01_journal/03_H20_I.html",
    "href": "content/01_journal/03_H20_I.html",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "",
    "text": "Challenge: IBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. In this regard, we are going to answer several questions based on the visualizations.\nCode:\n\n1 Business & Data Understanding: Department and Job Role\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(stringr)\nemployee_attrition_tbl &lt;- read.csv(\"Employee_Attrition.csv\")\ndept_job_role_tbl &lt;- employee_attrition_tbl %&gt;%\n  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\ndept_job_role_tbl %&gt;%\n  group_by(Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n2 Attrition by department\n\ndept_job_role_tbl %&gt;%\n    group_by(Department, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department) %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n3 Attrition by job role\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\")\n\n\n\n  \n\n\n\n\n4 Develop KPI\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\") %&gt;%\n  arrange(desc(pct)) %&gt;%\n  mutate(\n    above_industry_avg = case_when(\n      pct &gt; 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"))\n\n\n\n  \n\n\n\n\n5 Function to calculate attrition cost\n\ncalculate_attrition_cost &lt;- function(\n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50) {\n  # Direct Costs\n  direct_cost &lt;- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  # Lost Productivity Costs\n  productivity_cost &lt;- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction &lt;- salary / workdays_per_year * workdays_position_open\n  # Estimated Turnover Per Employee\n  cost_per_employee &lt;- direct_cost + productivity_cost - salary_benefit_reduction\n  # Total Cost of Employee Turnover\n  total_cost &lt;- n * cost_per_employee\n  return(total_cost)}\n\n\n6 Function to convert counts to percentages.\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition)\n\n\n\n  \n\n\ncount_to_pct &lt;- function(data, ..., col = n) {\n  grouping_vars_expr &lt;- quos(...)\n  col_expr &lt;- enquo(col)\n  ret &lt;- data %&gt;%\n    group_by(!!! grouping_vars_expr) %&gt;%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %&gt;%\n    ungroup()\n  return(ret)}\ndept_job_role_tbl %&gt;%\n  count(JobRole, Attrition) %&gt;%\n  count_to_pct(JobRole)\n\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) \n\n\n\n  \n\n\n\n\n7 Assess Attrition Function\n\nassess_attrition &lt;- function(data, attrition_col, attrition_value, baseline_pct) {\n  attrition_col_expr &lt;- enquo(attrition_col)\n  data %&gt;%\n    filter((!! attrition_col_expr) %in% attrition_value) %&gt;%\n    arrange(desc(pct)) %&gt;%\n    mutate(above_industry_avg = case_when(\n        pct &gt; baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"))}\n\n\n8 Visualization\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)) %&gt;%\n  mutate(name = str_c(Department, JobRole, sep = \": \") %&gt;% as_factor()) %&gt;%\n  mutate(name      = fct_reorder(name, cost_of_attrition)) %&gt;%\n  mutate(cost_text = str_c(\"$\", format(cost_of_attrition / 1e6, digits = 2),\n                           \"M\", sep = \"\")) %&gt;%\n  ggplot(aes(cost_of_attrition, y = name)) +\n  geom_segment(aes(xend = 0, yend = name),    color = \"#2dc6d6\") +\n  geom_point(  aes(size = cost_of_attrition), color = \"#2dc6d6\") +\n  scale_x_continuous(labels = scales::dollar) +\n  geom_label(aes(label = cost_text, size = cost_of_attrition),\n             hjust = \"inward\", color = \"#2dc6d6\") +\n  scale_size(range = c(3, 5)) +\n  labs(title = \"Estimated cost of Attrition: By Dept and Job Role\",\n       y = \"\", x = \"Cost of attrition\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# Descriptive Features\nemployee_attrition_tbl %&gt;% select(Age, DistanceFromHome, Gender, MaritalStatus, NumCompaniesWorked, Over18)\n\n\n\n  \n\n\n\n\n# Employment Features\nemployee_attrition_tbl %&gt;% select(Department, EmployeeCount, EmployeeNumber, JobInvolvement, JobLevel, JobRole, JobSatisfaction)\n\n\n\n  \n\n\n\n\n# Compensation Features\nemployee_attrition_tbl %&gt;% select(DailyRate, HourlyRate, MonthlyIncome, MonthlyRate, PercentSalaryHike, StockOptionLevel)\n\n\n\n  \n\n\n\n\n# Survery Results\nemployee_attrition_tbl %&gt;% select(EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance)\n\n\n\n  \n\n\n\n\n# Performance Data\nemployee_attrition_tbl %&gt;% select(JobInvolvement, PerformanceRating)\n\n\n\n  \n\n\n\n\n# Work-Life Features\nemployee_attrition_tbl %&gt;% select(BusinessTravel, OverTime)\n\n\n\n  \n\n\n\n\n# Training & Education\nemployee_attrition_tbl %&gt;% select(Education, EducationField, TrainingTimesLastYear)\n\n\n\n  \n\n\n\n\n# Time-Based Features\nemployee_attrition_tbl %&gt;% select(TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)\n\n\n\n  \n\n\n\n\nlibrary(GGally)\nemployee_attrition_tbl %&gt;%\n  select(Attrition,  Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n9 Explore Features by Category\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n  color_expr &lt;- enquo(color)\n  if (rlang::quo_is_null(color_expr)) {\n    g &lt;- data %&gt;%\n      ggpairs(lower = \"blank\") \n    } else {\n    color_name &lt;- quo_name(color_expr)\n    g &lt;- data %&gt;%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")}\n  return(g)}\n\n\n10 Challanges\nDescriptive features: age, gender, marital status\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nEmployment features: department, job role, job level\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nCompensation features: HourlyRate, MonthlyIncome, StockOptionLevel\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 1: What can you deduce about the interaction between Monthly Income and Attrition?\nAnswer: Those that are leaving have a lower Monthly Income\nQuestion 2:What can you deduce about the interaction between Percent Salary Hike and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization\nQuestion 3:What can you deduce about the interaction between Stock Option Level and Attrition?\nAnswer: Those that are staying have a higher stock option level\nSurvey Results: Satisfaction level, Work Life Balance\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 4: What can you deduce about the interaction between Environment Satisfaction and Attrition?\nAnswer: A higher proportion of those leaving have a low environment satisfaction level\nQuestion 5:What can you deduce about the interaction between Work Life Balance and Attrition.\nAnswer: Those that are staying have a higher density of 2’s and 3’s\nPerformance Data: Job Involvement, Performance Rating\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 6: What Can you deduce about the interaction between Job Involvement and Attrition?\nAnswer: Those that are leaving have a lower density of 3’s and 4’s\nWork-Life Features\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 7: What can you deduce about the interaction between Over Time and Attrition?\nAnswer: The proportion of those staying that are working Over Time are high compared to those that are not staying\nTraining and Education\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 8: What can you deduce about the interaction between Training Times Last Year and Attrition.\nAnswer: People that leave tend to have less annual training\nTime-Based Features: Years at company, years in current role\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"years\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 9: What can you deduce about the interaction between Years At Company and Attrition.\nAnswer: People that leave tend to have less working years at the company\nQuestion 10: What can you deduce about the interaction between Years Since Last Promotion and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/02_Regression.html",
    "href": "content/01_journal/02_Regression.html",
    "title": "Supervised ML - Regression",
    "section": "",
    "text": "Challenge: Our goal is to figure out what gaps exist in the products and come up with a pricing algorithm that will help us to determine a price, if we were to come up with products in that product cateogry.\nCode:\n\n# load libraries\nlibrary(tidyverse)\nlibrary(parsnip)\nlibrary(tidymodels)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(rpart.plot)\n\n\n# Build the model\nknitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)\noptions(dplyr.summarise.inform = FALSE)\noptions(warn=-1)\nbike_orderlines_tbl &lt;- readRDS(\"bike_orderlines.rds\")\nmodel_sales_tbl &lt;- bike_orderlines_tbl %&gt;%\n  select(total_price, model, category_2, frame_material) %&gt;%\n  group_by(model, category_2, frame_material) %&gt;%\n  summarise(total_sales = sum(total_price)) %&gt;%\n  ungroup() %&gt;% arrange(desc(total_sales))\nmodel_sales_tbl %&gt;% mutate(category_2 = as_factor(category_2) %&gt;% \n                             fct_reorder(total_sales, .fun = max) %&gt;% \n                             fct_rev()) %&gt;%\n  ggplot(aes(frame_material, total_sales)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2c3e50\") +\n  facet_wrap(~ category_2) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\", accuracy = 0.1)) +\n  tidyquant::theme_tq() +\n  labs(title = \"Total Sales for Each Model\",x = \"Frame Material\", y = \"Revenue\")\n\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\n\n\n\n\n\n\n\nbike_features_tbl &lt;- readRDS(\"bike_features_tbl.rds\")\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  select(frame_material:gender, `Rear Derailleur`, `Shift Lever`) %&gt;% \n  mutate(`shimano dura-ace`        = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano dura-ace \") %&gt;% as.numeric(),\n         `shimano ultegra`         = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano ultegra \") %&gt;% as.numeric(),\n         `shimano 105`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano 105 \") %&gt;% as.numeric(),\n         `shimano tiagra`          = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano tiagra \") %&gt;% as.numeric(),\n         `Shimano sora`            = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano sora\") %&gt;% as.numeric(),\n         `shimano deore`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore(?! xt)\") %&gt;% as.numeric(),\n         `shimano slx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano slx\") %&gt;% as.numeric(),\n         `shimano grx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano grx\") %&gt;% as.numeric(),\n         `Shimano xt`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore xt |shimano xt \") %&gt;% as.numeric(),\n         `Shimano xtr`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano xtr\") %&gt;% as.numeric(),\n         `Shimano saint`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano saint\") %&gt;% as.numeric(),\n         `SRAM red`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram red\") %&gt;% as.numeric(),\n         `SRAM force`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram force\") %&gt;% as.numeric(),\n         `SRAM rival`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram rival\") %&gt;% as.numeric(),\n         `SRAM apex`               = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram apex\") %&gt;% as.numeric(),\n         `SRAM xx1`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram xx1\") %&gt;% as.numeric(),\n         `SRAM x01`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram x01|sram xo1\") %&gt;% as.numeric(),\n         `SRAM gx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram gx\") %&gt;% as.numeric(),\n         `SRAM nx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram nx\") %&gt;% as.numeric(),\n         `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n         `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n         `Campagnolo potenza`      = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo potenza\") %&gt;% as.numeric(),\n         `Campagnolo super record` = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo super record\") %&gt;% as.numeric(),\n         `shimano nexus`           = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano nexus\") %&gt;% as.numeric(),\n         `shimano alfine`          = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano alfine\") %&gt;% as.numeric()) %&gt;%  \n  select(-c(`Rear Derailleur`, `Shift Lever`)) %&gt;% \n  mutate_if(is.numeric, ~replace(., is.na(.), 0)) \n\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  mutate(id = row_number()) %&gt;% \n  mutate(frame_material = factor(frame_material)) %&gt;%\n  select(id, everything()) \n\nbike_features_tbl %&gt;% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj &lt;- initial_split(bike_features_tbl, prop   = 0.80, strata = \"category_2\")\nsplit_obj %&gt;% training() %&gt;% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj %&gt;% testing() %&gt;% distinct(category_2)\n\n\n\n  \n\n\ntrain_tbl &lt;- training(split_obj)\ntest_tbl  &lt;- testing(split_obj)\ntrain_tbl &lt;- train_tbl %&gt;% set_names(str_replace_all(names(train_tbl), \" |-\", \"_\"))\ntest_tbl  &lt;- test_tbl  %&gt;% set_names(str_replace_all(names(test_tbl), \" |-\", \"_\"))\n\n\n# Create features with the recipes package\noptions(warn=-1)\nbike_rec &lt;- recipe(frame_material ~ ., data = train_tbl) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  step_zv(all_predictors()) \n\n\n# Bundle the model and recipe with the workflow package\noptions(warn=-1)\nlr_mod &lt;- logistic_reg() %&gt;% set_engine(\"glm\")\nbike_wflow &lt;- workflow() %&gt;%add_model(lr_mod) %&gt;% add_recipe(bike_rec)\nbike_fit &lt;- bike_wflow %&gt;% fit(data = train_tbl)\nbike_fit %&gt;% pull_workflow_fit() %&gt;% tidy()\n\n\n\n  \n\n\nbike_predict &lt;- predict(bike_fit, test_tbl, type=\"prob\") %&gt;% \n  bind_cols(test_tbl %&gt;% select(frame_material, category_2)) \nbike_predict %&gt;% roc_curve(truth = frame_material, .pred_aluminium) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_predict %&gt;% \n  roc_curve(truth = frame_material, .pred_carbon) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_predict\n\n\n\n  \n\n\nbike_predict %&gt;% roc_auc(truth = frame_material, .pred_aluminium)\n\n\n\n  \n\n\nroc_car &lt;- bike_predict %&gt;% roc_auc(truth = frame_material, .pred_carbon)\n\n\n# Evaluate your model with the yardstick package\noptions(warn=-1)\nmodel_01_linear_lm_simple &lt;- linear_reg(mode = \"regression\") %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ category_2 + frame_material, data = train_tbl)\nmodel_01_linear_lm_simple\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = price ~ category_2 + frame_material, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;              (Intercept)        category_2All-Road            category_2City  \n#&gt;                  2064.64                    -70.69                  -1123.87  \n#&gt;  category_2Cross-Country      category_2Cyclocross       category_2Dirt Jump  \n#&gt;                   422.78                   -319.38                   -798.97  \n#&gt;       category_2Downhill          category_2E-City       category_2E-Fitness  \n#&gt;                  2282.18                   1004.79                    977.70  \n#&gt;       category_2E-Gravel      category_2E-Mountain          category_2E-Road  \n#&gt;                  1696.67                   1316.48                    854.36  \n#&gt;     category_2E-Trekking       category_2Endurance          category_2Enduro  \n#&gt;                  1260.08                   -469.88                    630.39  \n#&gt;      category_2Fat Bikes            category_2Race         category_2Touring  \n#&gt;                  -970.00                   1096.92                   -851.35  \n#&gt;          category_2Trail  category_2Triathlon Bike      frame_materialcarbon  \n#&gt;                  -177.79                    702.31                   1534.36\n\ntest_tbl &lt;- test_tbl %&gt;% filter(category_2 != \"Fat Bikes\")\n\nmodel_01_linear_lm_simple %&gt;%\n  predict(new_data = test_tbl) %&gt;%\n  bind_cols(test_tbl %&gt;% select(price)) %&gt;%\n  metrics(truth = price, estimate = .pred)\n\n\n\n  \n\n\ng1 &lt;- bike_features_tbl %&gt;% \n  mutate(category_2 = as.factor(category_2) %&gt;% \n           fct_reorder(price)) %&gt;% \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2dc6d6\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs( title = \"Unit Price for Each Model\", y = \"\", x = \"Category 2\")\ng1\n\n\n\n\n\n\nnew_trail &lt;- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Trail\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0)\nnew_trail\n\n\n\n  \n\n\npredict(model_01_linear_lm_simple, new_data = new_trail)\n\n\n\n  \n\n\nmodels_tbl &lt;- tibble(\n  model_id = str_c(\"Model 0\", 1:1),\n  model = list(\n    model_01_linear_lm_simple))\n\nmodels_tbl\n\n\n\n  \n\n\npredictions_new_trail_tbl &lt;- models_tbl %&gt;%\n  mutate(predictions = map(model, predict, new_data = new_trail)) %&gt;%\n  unnest(predictions) %&gt;%\n  mutate(category_2 = \"Trail\") %&gt;%\n  left_join(new_trail, by = \"category_2\")\npredictions_new_trail_tbl\n\n\n\n  \n\n\ng2 &lt;- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_trail_tbl) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 5,\n                           data = predictions_new_trail_tbl)\ng2"
  }
]