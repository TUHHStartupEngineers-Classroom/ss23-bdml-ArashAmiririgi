[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\nThis is a .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#header-2",
    "href": "content/01_journal/01_tidyverse.html#header-2",
    "title": "Tidyverse",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "content/03_other/06_links.html#r-and-r-studio",
    "href": "content/03_other/06_links.html#r-and-r-studio",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual ."
  },
  {
    "objectID": "content/03_other/06_links.html#additional-r-resources",
    "href": "content/03_other/06_links.html#additional-r-resources",
    "title": "Links",
    "section": "",
    "text": "Google is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "sdvdyxv"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/01_journal/01_Fundamentals.html",
    "href": "content/01_journal/01_Fundamentals.html",
    "title": "Machine Learning Fundamentals",
    "section": "",
    "text": "Challenge:\nYour organization wants to know which companies are similar to each other to help in identifying potential customers of a SAAS software solution (e.g. Sales force CRM or equivalent) in various segments of the market. The Sales Department is very interested in this analysis, which will help them more easily penetrate various market segments.\nCode:\n\nknitr::opts_chunk$set(\n    echo = TRUE,\n    message = FALSE,\n    warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(broom)\nlibrary(umap)\nsp_500_prices_tbl &lt;- read_rds(\"sp_500_prices_tbl.rds\")\nsp_500_prices_tbl\n\n\n\n  \n\n\nsp_500_index_tbl &lt;- read_rds(\"sp_500_index_tbl.rds\")\nsp_500_index_tbl\n\n\n\n  \n\n\n## Step 1 - Convert stock prices to a standardized format (daily returns)\nsp_500_daily_returns_tbl &lt;- sp_500_prices_tbl %&gt;% select(symbol,date, adjusted) %&gt;% filter(year(date) &gt;= '2018') %&gt;% group_by(symbol) %&gt;%\n  mutate(lag = lag(adjusted, n = 1)) %&gt;% filter(!(is.na(lag))) %&gt;% mutate(pct_return = (adjusted - lag)/lag) %&gt;%\n  select(symbol, date, pct_return)\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\n## Step 2 - Convert to User-Item Format\nsp_500_daily_returns_tbl &lt;- read_rds(\"sp_500_daily_returns_tbl.rds\")\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\nstock_date_matrix_tbl &lt;- sp_500_daily_returns_tbl %&gt;% pivot_wider(names_from = date, values_from = pct_return, values_fill = 0)\nstock_date_matrix_tbl\n\n\n\n  \n\n\n## Step 3 - Perform K-Means Clustering\nstock_date_matrix_tbl &lt;- read_rds(\"stock_date_matrix_tbl.rds\")\nkmeans_obj &lt;- stock_date_matrix_tbl %&gt;% select(-symbol) %&gt;% kmeans(centers = 4, nstart = 20)\n\n## Step 4 - Find the optimal value of K\nkmeans_mapper &lt;- function(center = 3) {\n    stock_date_matrix_tbl %&gt;%\n        select(-symbol) %&gt;%\n        kmeans(centers = center, nstart = 20)}\nk_means_mapped_tbl &lt;- tibble(centers = 1:30) %&gt;%\n    mutate(k_means = centers %&gt;% map(kmeans_mapper)) %&gt;%\n    mutate(glance  = k_means %&gt;% map(glance))\nk_means_mapped_tbl\n\n\n\n  \n\n\nk_means_mapped_tbl %&gt;% unnest(glance) %&gt;% select(centers, tot.withinss) %&gt;%  ggplot(aes(centers, tot.withinss)) +\n    geom_point(color = \"blue\", size = 4) +\n    geom_line(color = \"blue\", size = 1) +\n    ggrepel::geom_label_repel(aes(label = centers), color = \"blue\")\n\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n## Step 5 - Apply UMAP\nk_means_mapped_tbl &lt;- read_rds(\"k_means_mapped_tbl.rds\")\numap_results &lt;- stock_date_matrix_tbl %&gt;% select(-symbol) %&gt;% umap()\numap_results\n\n#&gt; umap embedding of 502 items in 2 dimensions\n#&gt; object components: layout, data, knn, config\n\numap_results_tbl &lt;- umap_results$layou %&gt;% as_tibble() %&gt;% set_names(c(\"x\", \"y\")) %&gt;% bind_cols(stock_date_matrix_tbl %&gt;% select(symbol))\n\n#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#&gt; `.name_repair` is omitted as of tibble 2.0.0.\n#&gt; ℹ Using compatibility `.name_repair`.\n\numap_results_tbl\n\n\n\n  \n\n\numap_results_tbl %&gt;% ggplot(aes(x, y)) + geom_point(alpha = 0.5) + theme_tq() + labs(title = \"UMAP Projection\")\n\n\n\n\n\n\n## Step 6 - Combine K-Means and UMAP\nk_means_mapped_tbl &lt;- read_rds(\"k_means_mapped_tbl.rds\")\numap_results_tbl   &lt;- read_rds(\"umap_results_tbl.rds\")\nk_means_obj &lt;- k_means_mapped_tbl %&gt;% pull(k_means) %&gt;% pluck(10)\numap_kmeans_results_tbl &lt;- k_means_obj %&gt;% augment(stock_date_matrix_tbl) %&gt;% select(symbol, .cluster) %&gt;% left_join(umap_results_tbl) %&gt;% left_join(sp_500_index_tbl %&gt;% select(symbol, company, sector))\n\n#&gt; Joining with `by = join_by(symbol)`\n\n\n#&gt; Joining with `by = join_by(symbol)`\n\numap_kmeans_results_tbl\n\n\n\n  \n\n\numap_kmeans_results_tbl %&gt;% ggplot(aes(V1, V2, color = .cluster)) + geom_point(alpha = 0.5)"
  },
  {
    "objectID": "content/01_journal/06_LIME.html",
    "href": "content/01_journal/06_LIME.html",
    "title": "Explaining Black-Box Models With LIME",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/05_Performance_Measures.html",
    "href": "content/01_journal/05_Performance_Measures.html",
    "title": "Performance Measures",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_H20_II.html",
    "href": "content/01_journal/04_H20_II.html",
    "title": "Automated Machine Learning with H20 (II)",
    "section": "",
    "text": "Challenge: The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales. It’s a classic Binary Classification problem.\nCode:\nLoading libraries\n\nlibrary(tidymodels)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(h2o)\n\nLoading the training & test dataset\n\nproduct_backorders_tbl &lt;- read.csv(\"product_backorders.csv\")\nsplit_obj &lt;- initial_split(product_backorders_tbl, prop = 0.8)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\n\nSpecifying the response and predictor variables\nrunning AutoML specifying the stopping criterion\nView the leaderboard\nPredicting using Leader Model\nSave the leader model"
  },
  {
    "objectID": "content/01_journal/03_H20_I.html",
    "href": "content/01_journal/03_H20_I.html",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "",
    "text": "Challenge: IBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. In this regard, we are going to answer several questions based on the visualizations.\nCode:\n\n1 Business & Data Understanding: Department and Job Role\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(stringr)\nemployee_attrition_tbl &lt;- read.csv(\"Employee_Attrition.csv\")\ndept_job_role_tbl &lt;- employee_attrition_tbl %&gt;%\n  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\ndept_job_role_tbl %&gt;%\n  group_by(Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n2 Attrition by department\n\ndept_job_role_tbl %&gt;%\n    group_by(Department, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department) %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n3 Attrition by job role\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\")\n\n\n\n  \n\n\n\n\n4 Develop KPI\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\") %&gt;%\n  arrange(desc(pct)) %&gt;%\n  mutate(\n    above_industry_avg = case_when(\n      pct &gt; 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"))\n\n\n\n  \n\n\n\n\n5 Function to calculate attrition cost\n\ncalculate_attrition_cost &lt;- function(\n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50) {\n  # Direct Costs\n  direct_cost &lt;- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  # Lost Productivity Costs\n  productivity_cost &lt;- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction &lt;- salary / workdays_per_year * workdays_position_open\n  # Estimated Turnover Per Employee\n  cost_per_employee &lt;- direct_cost + productivity_cost - salary_benefit_reduction\n  # Total Cost of Employee Turnover\n  total_cost &lt;- n * cost_per_employee\n  return(total_cost)}\n\n\n6 Function to convert counts to percentages.\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition)\n\n\n\n  \n\n\ncount_to_pct &lt;- function(data, ..., col = n) {\n  grouping_vars_expr &lt;- quos(...)\n  col_expr &lt;- enquo(col)\n  ret &lt;- data %&gt;%\n    group_by(!!! grouping_vars_expr) %&gt;%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %&gt;%\n    ungroup()\n  return(ret)}\ndept_job_role_tbl %&gt;%\n  count(JobRole, Attrition) %&gt;%\n  count_to_pct(JobRole)\n\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) \n\n\n\n  \n\n\n\n\n7 Assess Attrition Function\n\nassess_attrition &lt;- function(data, attrition_col, attrition_value, baseline_pct) {\n  attrition_col_expr &lt;- enquo(attrition_col)\n  data %&gt;%\n    filter((!! attrition_col_expr) %in% attrition_value) %&gt;%\n    arrange(desc(pct)) %&gt;%\n    mutate(above_industry_avg = case_when(\n        pct &gt; baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"))}\n\n\n8 Visualization\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)) %&gt;%\n  mutate(name = str_c(Department, JobRole, sep = \": \") %&gt;% as_factor()) %&gt;%\n  mutate(name      = fct_reorder(name, cost_of_attrition)) %&gt;%\n  mutate(cost_text = str_c(\"$\", format(cost_of_attrition / 1e6, digits = 2),\n                           \"M\", sep = \"\")) %&gt;%\n  ggplot(aes(cost_of_attrition, y = name)) +\n  geom_segment(aes(xend = 0, yend = name),    color = \"#2dc6d6\") +\n  geom_point(  aes(size = cost_of_attrition), color = \"#2dc6d6\") +\n  scale_x_continuous(labels = scales::dollar) +\n  geom_label(aes(label = cost_text, size = cost_of_attrition),\n             hjust = \"inward\", color = \"#2dc6d6\") +\n  scale_size(range = c(3, 5)) +\n  labs(title = \"Estimated cost of Attrition: By Dept and Job Role\",\n       y = \"\", x = \"Cost of attrition\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# Descriptive Features\nemployee_attrition_tbl %&gt;% select(Age, DistanceFromHome, Gender, MaritalStatus, NumCompaniesWorked, Over18)\n\n\n\n  \n\n\n\n\n# Employment Features\nemployee_attrition_tbl %&gt;% select(Department, EmployeeCount, EmployeeNumber, JobInvolvement, JobLevel, JobRole, JobSatisfaction)\n\n\n\n  \n\n\n\n\n# Compensation Features\nemployee_attrition_tbl %&gt;% select(DailyRate, HourlyRate, MonthlyIncome, MonthlyRate, PercentSalaryHike, StockOptionLevel)\n\n\n\n  \n\n\n\n\n# Survery Results\nemployee_attrition_tbl %&gt;% select(EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance)\n\n\n\n  \n\n\n\n\n# Performance Data\nemployee_attrition_tbl %&gt;% select(JobInvolvement, PerformanceRating)\n\n\n\n  \n\n\n\n\n# Work-Life Features\nemployee_attrition_tbl %&gt;% select(BusinessTravel, OverTime)\n\n\n\n  \n\n\n\n\n# Training & Education\nemployee_attrition_tbl %&gt;% select(Education, EducationField, TrainingTimesLastYear)\n\n\n\n  \n\n\n\n\n# Time-Based Features\nemployee_attrition_tbl %&gt;% select(TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)\n\n\n\n  \n\n\n\n\nlibrary(GGally)\nemployee_attrition_tbl %&gt;%\n  select(Attrition,  Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n9 Explore Features by Category\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n  color_expr &lt;- enquo(color)\n  if (rlang::quo_is_null(color_expr)) {\n    g &lt;- data %&gt;%\n      ggpairs(lower = \"blank\") \n    } else {\n    color_name &lt;- quo_name(color_expr)\n    g &lt;- data %&gt;%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")}\n  return(g)}\n\n\n10 Challanges\nDescriptive features: age, gender, marital status\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nEmployment features: department, job role, job level\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nCompensation features: HourlyRate, MonthlyIncome, StockOptionLevel\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 1: What can you deduce about the interaction between Monthly Income and Attrition?\nAnswer: Those that are leaving have a lower Monthly Income\nQuestion 2:What can you deduce about the interaction between Percent Salary Hike and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization\nQuestion 3:What can you deduce about the interaction between Stock Option Level and Attrition?\nAnswer: Those that are staying have a higher stock option level\nSurvey Results: Satisfaction level, Work Life Balance\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 4: What can you deduce about the interaction between Environment Satisfaction and Attrition?\nAnswer: A higher proportion of those leaving have a low environment satisfaction level\nQuestion 5:What can you deduce about the interaction between Work Life Balance and Attrition.\nAnswer: Those that are staying have a higher density of 2’s and 3’s\nPerformance Data: Job Involvement, Performance Rating\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 6: What Can you deduce about the interaction between Job Involvement and Attrition?\nAnswer: Those that are leaving have a lower density of 3’s and 4’s\nWork-Life Features\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 7: What can you deduce about the interaction between Over Time and Attrition?\nAnswer: The proportion of those staying that are working Over Time are high compared to those that are not staying\nTraining and Education\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 8: What can you deduce about the interaction between Training Times Last Year and Attrition.\nAnswer: People that leave tend to have less annual training\nTime-Based Features: Years at company, years in current role\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"years\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 9: What can you deduce about the interaction between Years At Company and Attrition.\nAnswer: People that leave tend to have less working years at the company\nQuestion 10: What can you deduce about the interaction between Years Since Last Promotion and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/02_Regression.html",
    "href": "content/01_journal/02_Regression.html",
    "title": "Supervised ML - Regression",
    "section": "",
    "text": "Challenge: Our goal is to figure out what gaps exist in the products and come up with a pricing algorithm that will help us to determine a price, if we were to come up with products in that product cateogry.\nCode:\n\n# load libraries\nlibrary(tidyverse)\nlibrary(parsnip)\nlibrary(tidymodels)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(rpart.plot)\n\n\n# Build the model\nknitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)\noptions(dplyr.summarise.inform = FALSE)\noptions(warn=-1)\nbike_orderlines_tbl &lt;- readRDS(\"bike_orderlines.rds\")\nmodel_sales_tbl &lt;- bike_orderlines_tbl %&gt;%\n  select(total_price, model, category_2, frame_material) %&gt;%\n  group_by(model, category_2, frame_material) %&gt;%\n  summarise(total_sales = sum(total_price)) %&gt;%\n  ungroup() %&gt;% arrange(desc(total_sales))\nmodel_sales_tbl %&gt;% mutate(category_2 = as_factor(category_2) %&gt;% \n                             fct_reorder(total_sales, .fun = max) %&gt;% \n                             fct_rev()) %&gt;%\n  ggplot(aes(frame_material, total_sales)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2c3e50\") +\n  facet_wrap(~ category_2) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\", accuracy = 0.1)) +\n  tidyquant::theme_tq() +\n  labs(title = \"Total Sales for Each Model\",x = \"Frame Material\", y = \"Revenue\")\n\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\n\n\n\n\n\n\n\nbike_features_tbl &lt;- readRDS(\"bike_features_tbl.rds\")\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  select(frame_material:gender, `Rear Derailleur`, `Shift Lever`) %&gt;% \n  mutate(`shimano dura-ace`        = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano dura-ace \") %&gt;% as.numeric(),\n         `shimano ultegra`         = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano ultegra \") %&gt;% as.numeric(),\n         `shimano 105`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano 105 \") %&gt;% as.numeric(),\n         `shimano tiagra`          = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano tiagra \") %&gt;% as.numeric(),\n         `Shimano sora`            = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano sora\") %&gt;% as.numeric(),\n         `shimano deore`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore(?! xt)\") %&gt;% as.numeric(),\n         `shimano slx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano slx\") %&gt;% as.numeric(),\n         `shimano grx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano grx\") %&gt;% as.numeric(),\n         `Shimano xt`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore xt |shimano xt \") %&gt;% as.numeric(),\n         `Shimano xtr`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano xtr\") %&gt;% as.numeric(),\n         `Shimano saint`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano saint\") %&gt;% as.numeric(),\n         `SRAM red`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram red\") %&gt;% as.numeric(),\n         `SRAM force`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram force\") %&gt;% as.numeric(),\n         `SRAM rival`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram rival\") %&gt;% as.numeric(),\n         `SRAM apex`               = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram apex\") %&gt;% as.numeric(),\n         `SRAM xx1`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram xx1\") %&gt;% as.numeric(),\n         `SRAM x01`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram x01|sram xo1\") %&gt;% as.numeric(),\n         `SRAM gx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram gx\") %&gt;% as.numeric(),\n         `SRAM nx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram nx\") %&gt;% as.numeric(),\n         `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n         `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n         `Campagnolo potenza`      = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo potenza\") %&gt;% as.numeric(),\n         `Campagnolo super record` = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo super record\") %&gt;% as.numeric(),\n         `shimano nexus`           = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano nexus\") %&gt;% as.numeric(),\n         `shimano alfine`          = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano alfine\") %&gt;% as.numeric()) %&gt;%  \n  select(-c(`Rear Derailleur`, `Shift Lever`)) %&gt;% \n  mutate_if(is.numeric, ~replace(., is.na(.), 0)) \n\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  mutate(id = row_number()) %&gt;% \n  mutate(frame_material = factor(frame_material)) %&gt;%\n  select(id, everything()) \n\nbike_features_tbl %&gt;% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj &lt;- initial_split(bike_features_tbl, prop   = 0.80, strata = \"category_2\")\nsplit_obj %&gt;% training() %&gt;% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj %&gt;% testing() %&gt;% distinct(category_2)\n\n\n\n  \n\n\ntrain_tbl &lt;- training(split_obj)\ntest_tbl  &lt;- testing(split_obj)\ntrain_tbl &lt;- train_tbl %&gt;% set_names(str_replace_all(names(train_tbl), \" |-\", \"_\"))\ntest_tbl  &lt;- test_tbl  %&gt;% set_names(str_replace_all(names(test_tbl), \" |-\", \"_\"))\n\n\n# Create features with the recipes package\noptions(warn=-1)\nbike_rec &lt;- recipe(frame_material ~ ., data = train_tbl) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  step_zv(all_predictors()) \n\n\n# Bundle the model and recipe with the workflow package\noptions(warn=-1)\nlr_mod &lt;- logistic_reg() %&gt;% set_engine(\"glm\")\nbike_wflow &lt;- workflow() %&gt;%add_model(lr_mod) %&gt;% add_recipe(bike_rec)\nbike_fit &lt;- bike_wflow %&gt;% fit(data = train_tbl)\nbike_fit %&gt;% pull_workflow_fit() %&gt;% tidy()\n\n\n\n  \n\n\nbike_predict &lt;- predict(bike_fit, test_tbl, type=\"prob\") %&gt;% \n  bind_cols(test_tbl %&gt;% select(frame_material, category_2)) \nbike_predict %&gt;% roc_curve(truth = frame_material, .pred_aluminium) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_predict %&gt;% \n  roc_curve(truth = frame_material, .pred_carbon) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_predict\n\n\n\n  \n\n\nbike_predict %&gt;% roc_auc(truth = frame_material, .pred_aluminium)\n\n\n\n  \n\n\nroc_car &lt;- bike_predict %&gt;% roc_auc(truth = frame_material, .pred_carbon)\n\n\n# Evaluate your model with the yardstick package\noptions(warn=-1)\nmodel_01_linear_lm_simple &lt;- linear_reg(mode = \"regression\") %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ category_2 + frame_material, data = train_tbl)\nmodel_01_linear_lm_simple\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = price ~ category_2 + frame_material, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;              (Intercept)        category_2All-Road            category_2City  \n#&gt;                  2064.64                    -70.69                  -1123.87  \n#&gt;  category_2Cross-Country      category_2Cyclocross       category_2Dirt Jump  \n#&gt;                   422.78                   -319.38                   -798.97  \n#&gt;       category_2Downhill          category_2E-City       category_2E-Fitness  \n#&gt;                  2282.18                   1004.79                    977.70  \n#&gt;       category_2E-Gravel      category_2E-Mountain          category_2E-Road  \n#&gt;                  1696.67                   1316.48                    854.36  \n#&gt;     category_2E-Trekking       category_2Endurance          category_2Enduro  \n#&gt;                  1260.08                   -469.88                    630.39  \n#&gt;      category_2Fat Bikes            category_2Race         category_2Touring  \n#&gt;                  -970.00                   1096.92                   -851.35  \n#&gt;          category_2Trail  category_2Triathlon Bike      frame_materialcarbon  \n#&gt;                  -177.79                    702.31                   1534.36\n\ntest_tbl &lt;- test_tbl %&gt;% filter(category_2 != \"Fat Bikes\")\n\nmodel_01_linear_lm_simple %&gt;%\n  predict(new_data = test_tbl) %&gt;%\n  bind_cols(test_tbl %&gt;% select(price)) %&gt;%\n  metrics(truth = price, estimate = .pred)\n\n\n\n  \n\n\ng1 &lt;- bike_features_tbl %&gt;% \n  mutate(category_2 = as.factor(category_2) %&gt;% \n           fct_reorder(price)) %&gt;% \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2dc6d6\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs( title = \"Unit Price for Each Model\", y = \"\", x = \"Category 2\")\ng1\n\n\n\n\n\n\nnew_trail &lt;- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Trail\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0)\nnew_trail\n\n\n\n  \n\n\npredict(model_01_linear_lm_simple, new_data = new_trail)\n\n\n\n  \n\n\nmodels_tbl &lt;- tibble(\n  model_id = str_c(\"Model 0\", 1:1),\n  model = list(\n    model_01_linear_lm_simple))\n\nmodels_tbl\n\n\n\n  \n\n\npredictions_new_trail_tbl &lt;- models_tbl %&gt;%\n  mutate(predictions = map(model, predict, new_data = new_trail)) %&gt;%\n  unnest(predictions) %&gt;%\n  mutate(category_2 = \"Trail\") %&gt;%\n  left_join(new_trail, by = \"category_2\")\npredictions_new_trail_tbl\n\n\n\n  \n\n\ng2 &lt;- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_trail_tbl) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 5,\n                           data = predictions_new_trail_tbl)\ng2"
  },
  {
    "objectID": "content/01_journal/03_H2O_I.html",
    "href": "content/01_journal/03_H2O_I.html",
    "title": "Automated Machine Learning with H2O (I)",
    "section": "",
    "text": "Challenge: IBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. In this regard, we are going to answer several questions based on the visualizations.\nCode:\n\n1 Business & Data Understanding: Department and Job Role\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(stringr)\nemployee_attrition_tbl &lt;- read.csv(\"Employee_Attrition.csv\")\ndept_job_role_tbl &lt;- employee_attrition_tbl %&gt;%\n  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\ndept_job_role_tbl %&gt;%\n  group_by(Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n2 Attrition by department\n\ndept_job_role_tbl %&gt;%\n    group_by(Department, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department) %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n3 Attrition by job role\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\")\n\n\n\n  \n\n\n\n\n4 Develop KPI\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\") %&gt;%\n  arrange(desc(pct)) %&gt;%\n  mutate(\n    above_industry_avg = case_when(\n      pct &gt; 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"))\n\n\n\n  \n\n\n\n\n5 Function to calculate attrition cost\n\ncalculate_attrition_cost &lt;- function(\n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50) {\n  # Direct Costs\n  direct_cost &lt;- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  # Lost Productivity Costs\n  productivity_cost &lt;- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction &lt;- salary / workdays_per_year * workdays_position_open\n  # Estimated Turnover Per Employee\n  cost_per_employee &lt;- direct_cost + productivity_cost - salary_benefit_reduction\n  # Total Cost of Employee Turnover\n  total_cost &lt;- n * cost_per_employee\n  return(total_cost)}\n\n\n6 Function to convert counts to percentages.\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition)\n\n\n\n  \n\n\ncount_to_pct &lt;- function(data, ..., col = n) {\n  grouping_vars_expr &lt;- quos(...)\n  col_expr &lt;- enquo(col)\n  ret &lt;- data %&gt;%\n    group_by(!!! grouping_vars_expr) %&gt;%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %&gt;%\n    ungroup()\n  return(ret)}\ndept_job_role_tbl %&gt;%\n  count(JobRole, Attrition) %&gt;%\n  count_to_pct(JobRole)\n\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) \n\n\n\n  \n\n\n\n\n7 Assess Attrition Function\n\nassess_attrition &lt;- function(data, attrition_col, attrition_value, baseline_pct) {\n  attrition_col_expr &lt;- enquo(attrition_col)\n  data %&gt;%\n    filter((!! attrition_col_expr) %in% attrition_value) %&gt;%\n    arrange(desc(pct)) %&gt;%\n    mutate(above_industry_avg = case_when(\n        pct &gt; baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"))}\n\n\n8 Visualization\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)) %&gt;%\n  mutate(name = str_c(Department, JobRole, sep = \": \") %&gt;% as_factor()) %&gt;%\n  mutate(name      = fct_reorder(name, cost_of_attrition)) %&gt;%\n  mutate(cost_text = str_c(\"$\", format(cost_of_attrition / 1e6, digits = 2),\n                           \"M\", sep = \"\")) %&gt;%\n  ggplot(aes(cost_of_attrition, y = name)) +\n  geom_segment(aes(xend = 0, yend = name),    color = \"#2dc6d6\") +\n  geom_point(  aes(size = cost_of_attrition), color = \"#2dc6d6\") +\n  scale_x_continuous(labels = scales::dollar) +\n  geom_label(aes(label = cost_text, size = cost_of_attrition),\n             hjust = \"inward\", color = \"#2dc6d6\") +\n  scale_size(range = c(3, 5)) +\n  labs(title = \"Estimated cost of Attrition: By Dept and Job Role\",\n       y = \"\", x = \"Cost of attrition\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# Descriptive Features\nemployee_attrition_tbl %&gt;% select(Age, DistanceFromHome, Gender, MaritalStatus, NumCompaniesWorked, Over18)\n\n\n\n  \n\n\n\n\n# Employment Features\nemployee_attrition_tbl %&gt;% select(Department, EmployeeCount, EmployeeNumber, JobInvolvement, JobLevel, JobRole, JobSatisfaction)\n\n\n\n  \n\n\n\n\n# Compensation Features\nemployee_attrition_tbl %&gt;% select(DailyRate, HourlyRate, MonthlyIncome, MonthlyRate, PercentSalaryHike, StockOptionLevel)\n\n\n\n  \n\n\n\n\n# Survery Results\nemployee_attrition_tbl %&gt;% select(EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance)\n\n\n\n  \n\n\n\n\n# Performance Data\nemployee_attrition_tbl %&gt;% select(JobInvolvement, PerformanceRating)\n\n\n\n  \n\n\n\n\n# Work-Life Features\nemployee_attrition_tbl %&gt;% select(BusinessTravel, OverTime)\n\n\n\n  \n\n\n\n\n# Training & Education\nemployee_attrition_tbl %&gt;% select(Education, EducationField, TrainingTimesLastYear)\n\n\n\n  \n\n\n\n\n# Time-Based Features\nemployee_attrition_tbl %&gt;% select(TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)\n\n\n\n  \n\n\n\n\nlibrary(GGally)\nemployee_attrition_tbl %&gt;%\n  select(Attrition,  Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n9 Explore Features by Category\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n  color_expr &lt;- enquo(color)\n  if (rlang::quo_is_null(color_expr)) {\n    g &lt;- data %&gt;%\n      ggpairs(lower = \"blank\") \n    } else {\n    color_name &lt;- quo_name(color_expr)\n    g &lt;- data %&gt;%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")}\n  return(g)}\n\n\n10 Challanges\nDescriptive features: age, gender, marital status\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nEmployment features: department, job role, job level\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nCompensation features: HourlyRate, MonthlyIncome, StockOptionLevel\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 1: What can you deduce about the interaction between Monthly Income and Attrition?\nAnswer: Those that are leaving have a lower Monthly Income\nQuestion 2:What can you deduce about the interaction between Percent Salary Hike and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization\nQuestion 3:What can you deduce about the interaction between Stock Option Level and Attrition?\nAnswer: Those that are staying have a higher stock option level\nSurvey Results: Satisfaction level, Work Life Balance\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 4: What can you deduce about the interaction between Environment Satisfaction and Attrition?\nAnswer: A higher proportion of those leaving have a low environment satisfaction level\nQuestion 5:What can you deduce about the interaction between Work Life Balance and Attrition.\nAnswer: Those that are staying have a higher density of 2’s and 3’s\nPerformance Data: Job Involvement, Performance Rating\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 6: What Can you deduce about the interaction between Job Involvement and Attrition?\nAnswer: Those that are leaving have a lower density of 3’s and 4’s\nWork-Life Features\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 7: What can you deduce about the interaction between Over Time and Attrition?\nAnswer: The proportion of those staying that are working Over Time are high compared to those that are not staying\nTraining and Education\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 8: What can you deduce about the interaction between Training Times Last Year and Attrition.\nAnswer: People that leave tend to have less annual training\nTime-Based Features: Years at company, years in current role\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"years\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 9: What can you deduce about the interaction between Years At Company and Attrition.\nAnswer: People that leave tend to have less working years at the company\nQuestion 10: What can you deduce about the interaction between Years Since Last Promotion and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/04_H2O_II.html",
    "href": "content/01_journal/04_H2O_II.html",
    "title": "Automated Machine Learning with H20 (II)",
    "section": "",
    "text": "Challenge: The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales. It’s a classic Binary Classification problem.\nCode:\nLoading libraries\n\nlibrary(tidymodels)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(h2o)\n\nLoading the training & test dataset\n\nproduct_backorders_tbl &lt;- read.csv(\"product_backorders.csv\")\nsplit_obj &lt;- initial_split(product_backorders_tbl, prop = 0.8)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\n\nSpecifying the response and predictor variables\n\nfactor_name &lt;- \"went_on_backorder\"\nrecipe_obj &lt;- \n  recipe(went_on_backorder ~ ., data = train_readable_tbl) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_mutate_at(factor_name, fn = as.factor) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  prep()\ntrain_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  &lt;- bake(recipe_obj, new_data = test_readable_tbl)\n\nh2o.init()\n\n#&gt;  Connection successful!\n#&gt; \n#&gt; R is connected to the H2O cluster: \n#&gt;     H2O cluster uptime:         3 hours 4 minutes \n#&gt;     H2O cluster timezone:       Europe/Berlin \n#&gt;     H2O data parsing timezone:  UTC \n#&gt;     H2O cluster version:        3.40.0.4 \n#&gt;     H2O cluster version age:    1 month and 16 days \n#&gt;     H2O cluster name:           H2O_started_from_R_arash_sau427 \n#&gt;     H2O cluster total nodes:    1 \n#&gt;     H2O cluster total memory:   3.55 GB \n#&gt;     H2O cluster total cores:    16 \n#&gt;     H2O cluster allowed cores:  16 \n#&gt;     H2O cluster healthy:        TRUE \n#&gt;     H2O Connection ip:          localhost \n#&gt;     H2O Connection port:        54321 \n#&gt;     H2O Connection proxy:       NA \n#&gt;     H2O Internal Security:      FALSE \n#&gt;     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\nsplit_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.8), seed = 1234)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o &lt;- split_h2o[[1]]\nvalid_h2o &lt;- split_h2o[[2]]\ntest_h2o  &lt;- as.h2o(test_tbl)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ny &lt;- \"went_on_backorder\"\nx &lt;- setdiff(names(train_h2o), y)\n\nrunning AutoML specifying the stopping criterion\n\nautoml_models_h2o &lt;- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 60,\n  nfolds            = 5)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n#&gt; 00:10:48.655: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#&gt; 00:10:48.663: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nView the leaderboard\n\ntypeof(automl_models_h2o)\n\n#&gt; [1] \"S4\"\n\nslotNames(automl_models_h2o)\n\n#&gt; [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#&gt; [5] \"modeling_steps\" \"training_info\"\n\nautoml_models_h2o@leaderboard \n\n#&gt;                                                 model_id       auc   logloss\n#&gt; 1    StackedEnsemble_AllModels_3_AutoML_6_20230614_01048 0.9449987 0.1798197\n#&gt; 2 StackedEnsemble_BestOfFamily_3_AutoML_6_20230614_01048 0.9448799 0.1810533\n#&gt; 3 StackedEnsemble_BestOfFamily_4_AutoML_6_20230614_01048 0.9448428 0.1810753\n#&gt; 4    StackedEnsemble_AllModels_2_AutoML_6_20230614_01048 0.9445582 0.1801268\n#&gt; 5    StackedEnsemble_AllModels_1_AutoML_6_20230614_01048 0.9441805 0.1812303\n#&gt; 6                          GBM_3_AutoML_6_20230614_01048 0.9438419 0.1837504\n#&gt;       aucpr mean_per_class_error      rmse        mse\n#&gt; 1 0.7459681            0.1477869 0.2326526 0.05412724\n#&gt; 2 0.7378710            0.1826993 0.2335213 0.05453220\n#&gt; 3 0.7379370            0.1817567 0.2335370 0.05453954\n#&gt; 4 0.7451674            0.1598858 0.2330390 0.05430720\n#&gt; 5 0.7429578            0.1585493 0.2337236 0.05462672\n#&gt; 6 0.7342687            0.1784337 0.2348528 0.05515586\n#&gt; \n#&gt; [22 rows x 7 columns]\n\nautoml_models_h2o@leader\n\n#&gt; Model Details:\n#&gt; ==============\n#&gt; \n#&gt; H2OBinomialModel: stackedensemble\n#&gt; Model ID:  StackedEnsemble_AllModels_3_AutoML_6_20230614_01048 \n#&gt; Model Summary for Stacked Ensemble: \n#&gt;                                          key            value\n#&gt; 1                          Stacking strategy cross_validation\n#&gt; 2       Number of base models (used / total)             8/15\n#&gt; 3           # GBM base models (used / total)              7/9\n#&gt; 4           # DRF base models (used / total)              1/2\n#&gt; 5  # DeepLearning base models (used / total)              0/3\n#&gt; 6           # GLM base models (used / total)              0/1\n#&gt; 7                      Metalearner algorithm              GLM\n#&gt; 8         Metalearner fold assignment scheme           Random\n#&gt; 9                         Metalearner nfolds                5\n#&gt; 10                   Metalearner fold_column               NA\n#&gt; 11        Custom metalearner hyperparameters             None\n#&gt; \n#&gt; \n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on training data. **\n#&gt; \n#&gt; MSE:  0.02588882\n#&gt; RMSE:  0.1609\n#&gt; LogLoss:  0.09783619\n#&gt; Mean Per-Class Error:  0.06818398\n#&gt; AUC:  0.9901896\n#&gt; AUCPR:  0.9455238\n#&gt; Gini:  0.9803792\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No  Yes    Error        Rate\n#&gt; No     8637  170 0.019303   =170/8807\n#&gt; Yes     142 1071 0.117065   =142/1213\n#&gt; Totals 8779 1241 0.031138  =312/10020\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.401825    0.872861 185\n#&gt; 2                       max f2  0.287006    0.892801 218\n#&gt; 3                 max f0point5  0.579762    0.901784 136\n#&gt; 4                 max accuracy  0.472380    0.968862 166\n#&gt; 5                max precision  0.992503    1.000000   0\n#&gt; 6                   max recall  0.032254    1.000000 338\n#&gt; 7              max specificity  0.992503    1.000000   0\n#&gt; 8             max absolute_mcc  0.401825    0.855194 185\n#&gt; 9   max min_per_class_accuracy  0.227725    0.948063 239\n#&gt; 10 max mean_per_class_accuracy  0.220796    0.948812 241\n#&gt; 11                     max tns  0.992503 8807.000000   0\n#&gt; 12                     max fns  0.992503 1212.000000   0\n#&gt; 13                     max fps  0.000666 8807.000000 399\n#&gt; 14                     max tps  0.032254 1213.000000 338\n#&gt; 15                     max tnr  0.992503    1.000000   0\n#&gt; 16                     max fnr  0.992503    0.999176   0\n#&gt; 17                     max fpr  0.000666    1.000000 399\n#&gt; 18                     max tpr  0.032254    1.000000 338\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on validation data. **\n#&gt; \n#&gt; MSE:  0.05345448\n#&gt; RMSE:  0.2312023\n#&gt; LogLoss:  0.1767577\n#&gt; Mean Per-Class Error:  0.1528373\n#&gt; AUC:  0.9500131\n#&gt; AUCPR:  0.7241913\n#&gt; Gini:  0.9000263\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No Yes    Error       Rate\n#&gt; No     2508 136 0.051437  =136/2644\n#&gt; Yes      90 264 0.254237    =90/354\n#&gt; Totals 2598 400 0.075384  =226/2998\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.304899    0.700265 201\n#&gt; 2                       max f2  0.113833    0.769042 274\n#&gt; 3                 max f0point5  0.596030    0.705795 119\n#&gt; 4                 max accuracy  0.418630    0.926951 167\n#&gt; 5                max precision  0.983909    1.000000   0\n#&gt; 6                   max recall  0.011002    1.000000 370\n#&gt; 7              max specificity  0.983909    1.000000   0\n#&gt; 8             max absolute_mcc  0.304899    0.658940 201\n#&gt; 9   max min_per_class_accuracy  0.113833    0.884181 274\n#&gt; 10 max mean_per_class_accuracy  0.113833    0.884224 274\n#&gt; 11                     max tns  0.983909 2644.000000   0\n#&gt; 12                     max fns  0.983909  353.000000   0\n#&gt; 13                     max fps  0.000552 2644.000000 399\n#&gt; 14                     max tps  0.011002  354.000000 370\n#&gt; 15                     max tnr  0.983909    1.000000   0\n#&gt; 16                     max fnr  0.983909    0.997175   0\n#&gt; 17                     max fpr  0.000552    1.000000 399\n#&gt; 18                     max tpr  0.011002    1.000000 370\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on cross-validation data. **\n#&gt; ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#&gt; \n#&gt; MSE:  0.05043474\n#&gt; RMSE:  0.2245768\n#&gt; LogLoss:  0.1691188\n#&gt; Mean Per-Class Error:  0.1472587\n#&gt; AUC:  0.9529542\n#&gt; AUCPR:  0.7408824\n#&gt; Gini:  0.9059085\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;           No  Yes    Error        Rate\n#&gt; No     10254  536 0.049676  =536/10790\n#&gt; Yes      356 1098 0.244842   =356/1454\n#&gt; Totals 10610 1634 0.072852  =892/12244\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold        value idx\n#&gt; 1                       max f1  0.330064     0.711140 204\n#&gt; 2                       max f2  0.126038     0.782103 281\n#&gt; 3                 max f0point5  0.650452     0.732608 110\n#&gt; 4                 max accuracy  0.522037     0.931803 146\n#&gt; 5                max precision  0.987745     1.000000   0\n#&gt; 6                   max recall  0.003513     1.000000 391\n#&gt; 7              max specificity  0.987745     1.000000   0\n#&gt; 8             max absolute_mcc  0.330064     0.671113 204\n#&gt; 9   max min_per_class_accuracy  0.117961     0.889620 285\n#&gt; 10 max mean_per_class_accuracy  0.126038     0.890731 281\n#&gt; 11                     max tns  0.987745 10790.000000   0\n#&gt; 12                     max fns  0.987745  1453.000000   0\n#&gt; 13                     max fps  0.000522 10790.000000 399\n#&gt; 14                     max tps  0.003513  1454.000000 391\n#&gt; 15                     max tnr  0.987745     1.000000   0\n#&gt; 16                     max fnr  0.987745     0.999312   0\n#&gt; 17                     max fpr  0.000522     1.000000 399\n#&gt; 18                     max tpr  0.003513     1.000000 391\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; Cross-Validation Metrics Summary: \n#&gt;                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#&gt; accuracy    0.924699  0.008023   0.923802   0.934486   0.930532   0.914297\n#&gt; auc         0.953255  0.005205   0.958878   0.952201   0.957922   0.946268\n#&gt; err         0.075301  0.008023   0.076198   0.065514   0.069468   0.085703\n#&gt; err_count 184.400000 19.894722 186.000000 163.000000 167.000000 211.000000\n#&gt; f0point5    0.675970  0.020073   0.683230   0.697279   0.689763   0.651837\n#&gt;           cv_5_valid\n#&gt; accuracy    0.920376\n#&gt; auc         0.951007\n#&gt; err         0.079624\n#&gt; err_count 195.000000\n#&gt; f0point5    0.657742\n#&gt; \n#&gt; ---\n#&gt;                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#&gt; precision           0.653287  0.026276   0.659400   0.685619   0.667674\n#&gt; r2                  0.518501  0.023482   0.548002   0.529270   0.526187\n#&gt; recall              0.787333  0.025397   0.798680   0.748175   0.794964\n#&gt; residual_deviance 827.848900 55.573612 803.868400 793.001400 775.632500\n#&gt; rmse                0.224371  0.008326   0.221679   0.214783   0.220127\n#&gt; specificity         0.943026  0.010792   0.941534   0.957543   0.948260\n#&gt;                   cv_4_valid cv_5_valid\n#&gt; precision           0.620690   0.633053\n#&gt; r2                  0.492257   0.496788\n#&gt; recall              0.815534   0.779310\n#&gt; residual_deviance 912.287600 854.454300\n#&gt; rmse                0.236067   0.229199\n#&gt; specificity         0.928472   0.939324\n\nextract_h2o_model_name_by_position &lt;- function(h2o_leaderboard,\n                                                n= 1, verbose = T){\n   model_name &lt;- h2o_leaderboard %&gt;%\n     as_tibble() %&gt;%\n     slice(n) %&gt;%\n     pull(model_id)\n   if (verbose) message(model_name)\n   return(model_name)}\n\nPredicting using Leader Model\n\n# StackedEnsemble_BestOfFamily_1_AutoML_4_20230613_234130\nstacked_ensemble_h2o &lt;- automl_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(8) %&gt;% \n  h2o.getModel()\n\npredictions &lt;- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#&gt; [1] \"environment\"\n\npredictions_tbl &lt;- predictions %&gt;% as_tibble()\npredictions_tbl\n\n\n\n  \n\n\n\nSave the leader model\n\n# DeepLearning_grid_1_AutoML_4_20230613_234130_model_1\nautoml_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(18) %&gt;% \n  h2o.getModel() %&gt;%\n  h2o.saveModel(path = \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\")\n\n#&gt; [1] \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\\\\GBM_grid_1_AutoML_6_20230614_01048_model_4\""
  }
]