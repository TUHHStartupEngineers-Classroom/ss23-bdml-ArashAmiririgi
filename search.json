[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\nThis is a .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#header-2",
    "href": "content/01_journal/01_tidyverse.html#header-2",
    "title": "Tidyverse",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "content/03_other/06_links.html#r-and-r-studio",
    "href": "content/03_other/06_links.html#r-and-r-studio",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual ."
  },
  {
    "objectID": "content/03_other/06_links.html#additional-r-resources",
    "href": "content/03_other/06_links.html#additional-r-resources",
    "title": "Links",
    "section": "",
    "text": "Google is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MyLabJournal",
    "section": "",
    "text": "09— title: “My Lab Journal” subtitle: “Business Decisions with Machine Learning” author: “Arash Amiririgi”"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/01_journal/01_Fundamentals.html",
    "href": "content/01_journal/01_Fundamentals.html",
    "title": "Machine Learning Fundamentals",
    "section": "",
    "text": "Challenge:\nYour organization wants to know which companies are similar to each other to help in identifying potential customers of a SAAS software solution (e.g. Sales force CRM or equivalent) in various segments of the market. The Sales Department is very interested in this analysis, which will help them more easily penetrate various market segments.\nCode:\n\nknitr::opts_chunk$set(\n    echo = TRUE,\n    message = FALSE,\n    warning = FALSE)\n\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(broom)\nlibrary(umap)\nsp_500_prices_tbl &lt;- read_rds(\"sp_500_prices_tbl.rds\")\nsp_500_prices_tbl\n\n\n\n  \n\n\nsp_500_index_tbl &lt;- read_rds(\"sp_500_index_tbl.rds\")\nsp_500_index_tbl\n\n\n\n  \n\n\n## Step 1 - Convert stock prices to a standardized format (daily returns)\nsp_500_daily_returns_tbl &lt;- sp_500_prices_tbl %&gt;% select(symbol,date, adjusted) %&gt;% filter(year(date) &gt;= '2018') %&gt;% group_by(symbol) %&gt;%\n  mutate(lag = lag(adjusted, n = 1)) %&gt;% filter(!(is.na(lag))) %&gt;% mutate(pct_return = (adjusted - lag)/lag) %&gt;%\n  select(symbol, date, pct_return)\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\n## Step 2 - Convert to User-Item Format\nsp_500_daily_returns_tbl &lt;- read_rds(\"sp_500_daily_returns_tbl.rds\")\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\nstock_date_matrix_tbl &lt;- sp_500_daily_returns_tbl %&gt;% pivot_wider(names_from = date, values_from = pct_return, values_fill = 0)\nstock_date_matrix_tbl\n\n\n\n  \n\n\n## Step 3 - Perform K-Means Clustering\nstock_date_matrix_tbl &lt;- read_rds(\"stock_date_matrix_tbl.rds\")\nkmeans_obj &lt;- stock_date_matrix_tbl %&gt;% select(-symbol) %&gt;% kmeans(centers = 4, nstart = 20)\n\n## Step 4 - Find the optimal value of K\nkmeans_mapper &lt;- function(center = 3) {\n    stock_date_matrix_tbl %&gt;%\n        select(-symbol) %&gt;%\n        kmeans(centers = center, nstart = 20)}\nk_means_mapped_tbl &lt;- tibble(centers = 1:30) %&gt;%\n    mutate(k_means = centers %&gt;% map(kmeans_mapper)) %&gt;%\n    mutate(glance  = k_means %&gt;% map(glance))\nk_means_mapped_tbl\n\n\n\n  \n\n\nk_means_mapped_tbl %&gt;% unnest(glance) %&gt;% select(centers, tot.withinss) %&gt;%  ggplot(aes(centers, tot.withinss)) +\n    geom_point(color = \"blue\", size = 4) +\n    geom_line(color = \"blue\", size = 1) +\n    ggrepel::geom_label_repel(aes(label = centers), color = \"blue\")\n\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n## Step 5 - Apply UMAP\nk_means_mapped_tbl &lt;- read_rds(\"k_means_mapped_tbl.rds\")\numap_results &lt;- stock_date_matrix_tbl %&gt;% select(-symbol) %&gt;% umap()\numap_results\n\n#&gt; umap embedding of 502 items in 2 dimensions\n#&gt; object components: layout, data, knn, config\n\numap_results_tbl &lt;- umap_results$layou %&gt;% as_tibble() %&gt;% set_names(c(\"x\", \"y\")) %&gt;% bind_cols(stock_date_matrix_tbl %&gt;% select(symbol))\n\n#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#&gt; `.name_repair` is omitted as of tibble 2.0.0.\n#&gt; ℹ Using compatibility `.name_repair`.\n\numap_results_tbl\n\n\n\n  \n\n\numap_results_tbl %&gt;% ggplot(aes(x, y)) + geom_point(alpha = 0.5) + theme_tq() + labs(title = \"UMAP Projection\")\n\n\n\n\n\n\n## Step 6 - Combine K-Means and UMAP\nk_means_mapped_tbl &lt;- read_rds(\"k_means_mapped_tbl.rds\")\numap_results_tbl   &lt;- read_rds(\"umap_results_tbl.rds\")\nk_means_obj &lt;- k_means_mapped_tbl %&gt;% pull(k_means) %&gt;% pluck(10)\numap_kmeans_results_tbl &lt;- k_means_obj %&gt;% augment(stock_date_matrix_tbl) %&gt;% select(symbol, .cluster) %&gt;% left_join(umap_results_tbl) %&gt;% left_join(sp_500_index_tbl %&gt;% select(symbol, company, sector))\n\n#&gt; Joining with `by = join_by(symbol)`\n\n\n#&gt; Joining with `by = join_by(symbol)`\n\numap_kmeans_results_tbl\n\n\n\n  \n\n\numap_kmeans_results_tbl %&gt;% ggplot(aes(V1, V2, color = .cluster)) + geom_point(alpha = 0.5)"
  },
  {
    "objectID": "content/01_journal/06_LIME.html",
    "href": "content/01_journal/06_LIME.html",
    "title": "Explaining Black-Box Models With LIME",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/05_Performance_Measures.html",
    "href": "content/01_journal/05_Performance_Measures.html",
    "title": "Performance Measures",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_H20_II.html",
    "href": "content/01_journal/04_H20_II.html",
    "title": "Automated Machine Learning with H20 (II)",
    "section": "",
    "text": "Challenge: The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales. It’s a classic Binary Classification problem.\nCode:\nLoading libraries\n\nlibrary(tidymodels)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(h2o)\n\nLoading the training & test dataset\n\nproduct_backorders_tbl &lt;- read.csv(\"product_backorders.csv\")\nsplit_obj &lt;- initial_split(product_backorders_tbl, prop = 0.8)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\n\nSpecifying the response and predictor variables\nrunning AutoML specifying the stopping criterion\nView the leaderboard\nPredicting using Leader Model\nSave the leader model"
  },
  {
    "objectID": "content/01_journal/03_H20_I.html",
    "href": "content/01_journal/03_H20_I.html",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "",
    "text": "Challenge: IBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. In this regard, we are going to answer several questions based on the visualizations.\nCode:\n\n1 Business & Data Understanding: Department and Job Role\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(stringr)\nemployee_attrition_tbl &lt;- read.csv(\"Employee_Attrition.csv\")\ndept_job_role_tbl &lt;- employee_attrition_tbl %&gt;%\n  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\ndept_job_role_tbl %&gt;%\n  group_by(Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n2 Attrition by department\n\ndept_job_role_tbl %&gt;%\n    group_by(Department, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department) %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n3 Attrition by job role\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\")\n\n\n\n  \n\n\n\n\n4 Develop KPI\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\") %&gt;%\n  arrange(desc(pct)) %&gt;%\n  mutate(\n    above_industry_avg = case_when(\n      pct &gt; 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"))\n\n\n\n  \n\n\n\n\n5 Function to calculate attrition cost\n\ncalculate_attrition_cost &lt;- function(\n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50) {\n  # Direct Costs\n  direct_cost &lt;- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  # Lost Productivity Costs\n  productivity_cost &lt;- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction &lt;- salary / workdays_per_year * workdays_position_open\n  # Estimated Turnover Per Employee\n  cost_per_employee &lt;- direct_cost + productivity_cost - salary_benefit_reduction\n  # Total Cost of Employee Turnover\n  total_cost &lt;- n * cost_per_employee\n  return(total_cost)}\n\n\n6 Function to convert counts to percentages.\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition)\n\n\n\n  \n\n\ncount_to_pct &lt;- function(data, ..., col = n) {\n  grouping_vars_expr &lt;- quos(...)\n  col_expr &lt;- enquo(col)\n  ret &lt;- data %&gt;%\n    group_by(!!! grouping_vars_expr) %&gt;%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %&gt;%\n    ungroup()\n  return(ret)}\ndept_job_role_tbl %&gt;%\n  count(JobRole, Attrition) %&gt;%\n  count_to_pct(JobRole)\n\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) \n\n\n\n  \n\n\n\n\n7 Assess Attrition Function\n\nassess_attrition &lt;- function(data, attrition_col, attrition_value, baseline_pct) {\n  attrition_col_expr &lt;- enquo(attrition_col)\n  data %&gt;%\n    filter((!! attrition_col_expr) %in% attrition_value) %&gt;%\n    arrange(desc(pct)) %&gt;%\n    mutate(above_industry_avg = case_when(\n        pct &gt; baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"))}\n\n\n8 Visualization\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)) %&gt;%\n  mutate(name = str_c(Department, JobRole, sep = \": \") %&gt;% as_factor()) %&gt;%\n  mutate(name      = fct_reorder(name, cost_of_attrition)) %&gt;%\n  mutate(cost_text = str_c(\"$\", format(cost_of_attrition / 1e6, digits = 2),\n                           \"M\", sep = \"\")) %&gt;%\n  ggplot(aes(cost_of_attrition, y = name)) +\n  geom_segment(aes(xend = 0, yend = name),    color = \"#2dc6d6\") +\n  geom_point(  aes(size = cost_of_attrition), color = \"#2dc6d6\") +\n  scale_x_continuous(labels = scales::dollar) +\n  geom_label(aes(label = cost_text, size = cost_of_attrition),\n             hjust = \"inward\", color = \"#2dc6d6\") +\n  scale_size(range = c(3, 5)) +\n  labs(title = \"Estimated cost of Attrition: By Dept and Job Role\",\n       y = \"\", x = \"Cost of attrition\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# Descriptive Features\nemployee_attrition_tbl %&gt;% select(Age, DistanceFromHome, Gender, MaritalStatus, NumCompaniesWorked, Over18)\n\n\n\n  \n\n\n\n\n# Employment Features\nemployee_attrition_tbl %&gt;% select(Department, EmployeeCount, EmployeeNumber, JobInvolvement, JobLevel, JobRole, JobSatisfaction)\n\n\n\n  \n\n\n\n\n# Compensation Features\nemployee_attrition_tbl %&gt;% select(DailyRate, HourlyRate, MonthlyIncome, MonthlyRate, PercentSalaryHike, StockOptionLevel)\n\n\n\n  \n\n\n\n\n# Survery Results\nemployee_attrition_tbl %&gt;% select(EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance)\n\n\n\n  \n\n\n\n\n# Performance Data\nemployee_attrition_tbl %&gt;% select(JobInvolvement, PerformanceRating)\n\n\n\n  \n\n\n\n\n# Work-Life Features\nemployee_attrition_tbl %&gt;% select(BusinessTravel, OverTime)\n\n\n\n  \n\n\n\n\n# Training & Education\nemployee_attrition_tbl %&gt;% select(Education, EducationField, TrainingTimesLastYear)\n\n\n\n  \n\n\n\n\n# Time-Based Features\nemployee_attrition_tbl %&gt;% select(TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)\n\n\n\n  \n\n\n\n\nlibrary(GGally)\nemployee_attrition_tbl %&gt;%\n  select(Attrition,  Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n9 Explore Features by Category\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n  color_expr &lt;- enquo(color)\n  if (rlang::quo_is_null(color_expr)) {\n    g &lt;- data %&gt;%\n      ggpairs(lower = \"blank\") \n    } else {\n    color_name &lt;- quo_name(color_expr)\n    g &lt;- data %&gt;%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")}\n  return(g)}\n\n\n10 Challanges\nDescriptive features: age, gender, marital status\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nEmployment features: department, job role, job level\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nCompensation features: HourlyRate, MonthlyIncome, StockOptionLevel\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 1: What can you deduce about the interaction between Monthly Income and Attrition?\nAnswer: Those that are leaving have a lower Monthly Income\nQuestion 2:What can you deduce about the interaction between Percent Salary Hike and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization\nQuestion 3:What can you deduce about the interaction between Stock Option Level and Attrition?\nAnswer: Those that are staying have a higher stock option level\nSurvey Results: Satisfaction level, Work Life Balance\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 4: What can you deduce about the interaction between Environment Satisfaction and Attrition?\nAnswer: A higher proportion of those leaving have a low environment satisfaction level\nQuestion 5:What can you deduce about the interaction between Work Life Balance and Attrition.\nAnswer: Those that are staying have a higher density of 2’s and 3’s\nPerformance Data: Job Involvement, Performance Rating\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 6: What Can you deduce about the interaction between Job Involvement and Attrition?\nAnswer: Those that are leaving have a lower density of 3’s and 4’s\nWork-Life Features\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 7: What can you deduce about the interaction between Over Time and Attrition?\nAnswer: The proportion of those staying that are working Over Time are high compared to those that are not staying\nTraining and Education\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 8: What can you deduce about the interaction between Training Times Last Year and Attrition.\nAnswer: People that leave tend to have less annual training\nTime-Based Features: Years at company, years in current role\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"years\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 9: What can you deduce about the interaction between Years At Company and Attrition.\nAnswer: People that leave tend to have less working years at the company\nQuestion 10: What can you deduce about the interaction between Years Since Last Promotion and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/02_Regression.html",
    "href": "content/01_journal/02_Regression.html",
    "title": "Supervised ML - Regression",
    "section": "",
    "text": "Challenge: Our goal is to figure out what gaps exist in the products and come up with a pricing algorithm that will help us to determine a price, if we were to come up with products in that product cateogry.\nCode:\n\n# load libraries\nlibrary(tidyverse)\nlibrary(parsnip)\nlibrary(tidymodels)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(recipes)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(rpart.plot)\n\n\n# Build the model\nknitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)\noptions(dplyr.summarise.inform = FALSE)\noptions(warn=-1)\nbike_orderlines_tbl &lt;- readRDS(\"bike_orderlines.rds\")\nmodel_sales_tbl &lt;- bike_orderlines_tbl %&gt;%\n  select(total_price, model, category_2, frame_material) %&gt;%\n  group_by(model, category_2, frame_material) %&gt;%\n  summarise(total_sales = sum(total_price)) %&gt;%\n  ungroup() %&gt;% arrange(desc(total_sales))\nmodel_sales_tbl %&gt;% mutate(category_2 = as_factor(category_2) %&gt;% \n                             fct_reorder(total_sales, .fun = max) %&gt;% \n                             fct_rev()) %&gt;%\n  ggplot(aes(frame_material, total_sales)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2c3e50\") +\n  facet_wrap(~ category_2) +\n  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\", accuracy = 0.1)) +\n  tidyquant::theme_tq() +\n  labs(title = \"Total Sales for Each Model\",x = \"Frame Material\", y = \"Revenue\")\n\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\n\n\n\n\n\n\n\nbike_features_tbl &lt;- readRDS(\"bike_features_tbl.rds\")\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  select(frame_material:gender, `Rear Derailleur`, `Shift Lever`) %&gt;% \n  mutate(`shimano dura-ace`        = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano dura-ace \") %&gt;% as.numeric(),\n         `shimano ultegra`         = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano ultegra \") %&gt;% as.numeric(),\n         `shimano 105`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano 105 \") %&gt;% as.numeric(),\n         `shimano tiagra`          = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano tiagra \") %&gt;% as.numeric(),\n         `Shimano sora`            = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano sora\") %&gt;% as.numeric(),\n         `shimano deore`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore(?! xt)\") %&gt;% as.numeric(),\n         `shimano slx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano slx\") %&gt;% as.numeric(),\n         `shimano grx`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano grx\") %&gt;% as.numeric(),\n         `Shimano xt`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano deore xt |shimano xt \") %&gt;% as.numeric(),\n         `Shimano xtr`             = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano xtr\") %&gt;% as.numeric(),\n         `Shimano saint`           = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"shimano saint\") %&gt;% as.numeric(),\n         `SRAM red`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram red\") %&gt;% as.numeric(),\n         `SRAM force`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram force\") %&gt;% as.numeric(),\n         `SRAM rival`              = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram rival\") %&gt;% as.numeric(),\n         `SRAM apex`               = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram apex\") %&gt;% as.numeric(),\n         `SRAM xx1`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram xx1\") %&gt;% as.numeric(),\n         `SRAM x01`                = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram x01|sram xo1\") %&gt;% as.numeric(),\n         `SRAM gx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram gx\") %&gt;% as.numeric(),\n         `SRAM nx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram nx\") %&gt;% as.numeric(),\n         `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n         `SRAM sx`                 = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"sram sx\") %&gt;% as.numeric(),\n         `Campagnolo potenza`      = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo potenza\") %&gt;% as.numeric(),\n         `Campagnolo super record` = `Rear Derailleur` %&gt;% str_to_lower() %&gt;% str_detect(\"campagnolo super record\") %&gt;% as.numeric(),\n         `shimano nexus`           = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano nexus\") %&gt;% as.numeric(),\n         `shimano alfine`          = `Shift Lever`     %&gt;% str_to_lower() %&gt;% str_detect(\"shimano alfine\") %&gt;% as.numeric()) %&gt;%  \n  select(-c(`Rear Derailleur`, `Shift Lever`)) %&gt;% \n  mutate_if(is.numeric, ~replace(., is.na(.), 0)) \n\nbike_features_tbl &lt;- bike_features_tbl %&gt;% \n  mutate(id = row_number()) %&gt;% \n  mutate(frame_material = factor(frame_material)) %&gt;%\n  select(id, everything()) \n\nbike_features_tbl %&gt;% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj &lt;- initial_split(bike_features_tbl, prop   = 0.80, strata = \"category_2\")\nsplit_obj %&gt;% training() %&gt;% distinct(category_2)\n\n\n\n  \n\n\nsplit_obj %&gt;% testing() %&gt;% distinct(category_2)\n\n\n\n  \n\n\ntrain_tbl &lt;- training(split_obj)\ntest_tbl  &lt;- testing(split_obj)\ntrain_tbl &lt;- train_tbl %&gt;% set_names(str_replace_all(names(train_tbl), \" |-\", \"_\"))\ntest_tbl  &lt;- test_tbl  %&gt;% set_names(str_replace_all(names(test_tbl), \" |-\", \"_\"))\n\n\n# Create features with the recipes package\noptions(warn=-1)\nbike_rec &lt;- recipe(frame_material ~ ., data = train_tbl) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  step_zv(all_predictors()) \n\n\n# Bundle the model and recipe with the workflow package\noptions(warn=-1)\nlr_mod &lt;- logistic_reg() %&gt;% set_engine(\"glm\")\nbike_wflow &lt;- workflow() %&gt;%add_model(lr_mod) %&gt;% add_recipe(bike_rec)\nbike_fit &lt;- bike_wflow %&gt;% fit(data = train_tbl)\nbike_fit %&gt;% pull_workflow_fit() %&gt;% tidy()\n\n\n\n  \n\n\nbike_predict &lt;- predict(bike_fit, test_tbl, type=\"prob\") %&gt;% \n  bind_cols(test_tbl %&gt;% select(frame_material, category_2)) \nbike_predict %&gt;% roc_curve(truth = frame_material, .pred_aluminium) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_predict %&gt;% \n  roc_curve(truth = frame_material, .pred_carbon) %&gt;% \n  autoplot()\n\n\n\n\n\n\nbike_predict\n\n\n\n  \n\n\nbike_predict %&gt;% roc_auc(truth = frame_material, .pred_aluminium)\n\n\n\n  \n\n\nroc_car &lt;- bike_predict %&gt;% roc_auc(truth = frame_material, .pred_carbon)\n\n\n# Evaluate your model with the yardstick package\noptions(warn=-1)\nmodel_01_linear_lm_simple &lt;- linear_reg(mode = \"regression\") %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ category_2 + frame_material, data = train_tbl)\nmodel_01_linear_lm_simple\n\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = price ~ category_2 + frame_material, data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;              (Intercept)        category_2All-Road            category_2City  \n#&gt;                  2064.64                    -70.69                  -1123.87  \n#&gt;  category_2Cross-Country      category_2Cyclocross       category_2Dirt Jump  \n#&gt;                   422.78                   -319.38                   -798.97  \n#&gt;       category_2Downhill          category_2E-City       category_2E-Fitness  \n#&gt;                  2282.18                   1004.79                    977.70  \n#&gt;       category_2E-Gravel      category_2E-Mountain          category_2E-Road  \n#&gt;                  1696.67                   1316.48                    854.36  \n#&gt;     category_2E-Trekking       category_2Endurance          category_2Enduro  \n#&gt;                  1260.08                   -469.88                    630.39  \n#&gt;      category_2Fat Bikes            category_2Race         category_2Touring  \n#&gt;                  -970.00                   1096.92                   -851.35  \n#&gt;          category_2Trail  category_2Triathlon Bike      frame_materialcarbon  \n#&gt;                  -177.79                    702.31                   1534.36\n\ntest_tbl &lt;- test_tbl %&gt;% filter(category_2 != \"Fat Bikes\")\n\nmodel_01_linear_lm_simple %&gt;%\n  predict(new_data = test_tbl) %&gt;%\n  bind_cols(test_tbl %&gt;% select(price)) %&gt;%\n  metrics(truth = price, estimate = .pred)\n\n\n\n  \n\n\ng1 &lt;- bike_features_tbl %&gt;% \n  mutate(category_2 = as.factor(category_2) %&gt;% \n           fct_reorder(price)) %&gt;% \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2dc6d6\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs( title = \"Unit Price for Each Model\", y = \"\", x = \"Category 2\")\ng1\n\n\n\n\n\n\nnew_trail &lt;- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Trail\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0)\nnew_trail\n\n\n\n  \n\n\npredict(model_01_linear_lm_simple, new_data = new_trail)\n\n\n\n  \n\n\nmodels_tbl &lt;- tibble(\n  model_id = str_c(\"Model 0\", 1:1),\n  model = list(\n    model_01_linear_lm_simple))\n\nmodels_tbl\n\n\n\n  \n\n\npredictions_new_trail_tbl &lt;- models_tbl %&gt;%\n  mutate(predictions = map(model, predict, new_data = new_trail)) %&gt;%\n  unnest(predictions) %&gt;%\n  mutate(category_2 = \"Trail\") %&gt;%\n  left_join(new_trail, by = \"category_2\")\npredictions_new_trail_tbl\n\n\n\n  \n\n\ng2 &lt;- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_trail_tbl) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 5,\n                           data = predictions_new_trail_tbl)\ng2"
  },
  {
    "objectID": "content/01_journal/03_H2O_I.html",
    "href": "content/01_journal/03_H2O_I.html",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "",
    "text": "Challenge: IBM has gathered information on employee satisfaction, income, seniority and some demographics. It includes the data of 1470 employees. In this regard, we are going to answer several questions based on the visualizations.\nCode:\n\n1 Business & Data Understanding: Department and Job Role\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(stringr)\nemployee_attrition_tbl &lt;- read.csv(\"Employee_Attrition.csv\")\ndept_job_role_tbl &lt;- employee_attrition_tbl %&gt;%\n  select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\ndept_job_role_tbl %&gt;%\n  group_by(Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n2 Attrition by department\n\ndept_job_role_tbl %&gt;%\n    group_by(Department, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department) %&gt;%\n  mutate(pct = n / sum(n))\n\n\n\n  \n\n\n\n\n3 Attrition by job role\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\")\n\n\n\n  \n\n\n\n\n4 Develop KPI\n\ndept_job_role_tbl %&gt;%\n  group_by(Department, JobRole, Attrition) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Department, JobRole) %&gt;%\n  mutate(pct = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  filter(Attrition %in% \"Yes\") %&gt;%\n  arrange(desc(pct)) %&gt;%\n  mutate(\n    above_industry_avg = case_when(\n      pct &gt; 0.088 ~ \"Yes\",\n      TRUE ~ \"No\"))\n\n\n\n  \n\n\n\n\n5 Function to calculate attrition cost\n\ncalculate_attrition_cost &lt;- function(\n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50) {\n  # Direct Costs\n  direct_cost &lt;- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  # Lost Productivity Costs\n  productivity_cost &lt;- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction &lt;- salary / workdays_per_year * workdays_position_open\n  # Estimated Turnover Per Employee\n  cost_per_employee &lt;- direct_cost + productivity_cost - salary_benefit_reduction\n  # Total Cost of Employee Turnover\n  total_cost &lt;- n * cost_per_employee\n  return(total_cost)}\n\n\n6 Function to convert counts to percentages.\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition)\n\n\n\n  \n\n\ncount_to_pct &lt;- function(data, ..., col = n) {\n  grouping_vars_expr &lt;- quos(...)\n  col_expr &lt;- enquo(col)\n  ret &lt;- data %&gt;%\n    group_by(!!! grouping_vars_expr) %&gt;%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %&gt;%\n    ungroup()\n  return(ret)}\ndept_job_role_tbl %&gt;%\n  count(JobRole, Attrition) %&gt;%\n  count_to_pct(JobRole)\n\n\n\n  \n\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) \n\n\n\n  \n\n\n\n\n7 Assess Attrition Function\n\nassess_attrition &lt;- function(data, attrition_col, attrition_value, baseline_pct) {\n  attrition_col_expr &lt;- enquo(attrition_col)\n  data %&gt;%\n    filter((!! attrition_col_expr) %in% attrition_value) %&gt;%\n    arrange(desc(pct)) %&gt;%\n    mutate(above_industry_avg = case_when(\n        pct &gt; baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"))}\n\n\n8 Visualization\n\ndept_job_role_tbl %&gt;%\n  count(Department, JobRole, Attrition) %&gt;%\n  count_to_pct(Department, JobRole) %&gt;%\n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %&gt;%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)) %&gt;%\n  mutate(name = str_c(Department, JobRole, sep = \": \") %&gt;% as_factor()) %&gt;%\n  mutate(name      = fct_reorder(name, cost_of_attrition)) %&gt;%\n  mutate(cost_text = str_c(\"$\", format(cost_of_attrition / 1e6, digits = 2),\n                           \"M\", sep = \"\")) %&gt;%\n  ggplot(aes(cost_of_attrition, y = name)) +\n  geom_segment(aes(xend = 0, yend = name),    color = \"#2dc6d6\") +\n  geom_point(  aes(size = cost_of_attrition), color = \"#2dc6d6\") +\n  scale_x_continuous(labels = scales::dollar) +\n  geom_label(aes(label = cost_text, size = cost_of_attrition),\n             hjust = \"inward\", color = \"#2dc6d6\") +\n  scale_size(range = c(3, 5)) +\n  labs(title = \"Estimated cost of Attrition: By Dept and Job Role\",\n       y = \"\", x = \"Cost of attrition\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n# Descriptive Features\nemployee_attrition_tbl %&gt;% select(Age, DistanceFromHome, Gender, MaritalStatus, NumCompaniesWorked, Over18)\n\n\n\n  \n\n\n\n\n# Employment Features\nemployee_attrition_tbl %&gt;% select(Department, EmployeeCount, EmployeeNumber, JobInvolvement, JobLevel, JobRole, JobSatisfaction)\n\n\n\n  \n\n\n\n\n# Compensation Features\nemployee_attrition_tbl %&gt;% select(DailyRate, HourlyRate, MonthlyIncome, MonthlyRate, PercentSalaryHike, StockOptionLevel)\n\n\n\n  \n\n\n\n\n# Survery Results\nemployee_attrition_tbl %&gt;% select(EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance)\n\n\n\n  \n\n\n\n\n# Performance Data\nemployee_attrition_tbl %&gt;% select(JobInvolvement, PerformanceRating)\n\n\n\n  \n\n\n\n\n# Work-Life Features\nemployee_attrition_tbl %&gt;% select(BusinessTravel, OverTime)\n\n\n\n  \n\n\n\n\n# Training & Education\nemployee_attrition_tbl %&gt;% select(Education, EducationField, TrainingTimesLastYear)\n\n\n\n  \n\n\n\n\n# Time-Based Features\nemployee_attrition_tbl %&gt;% select(TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)\n\n\n\n  \n\n\n\n\nlibrary(GGally)\nemployee_attrition_tbl %&gt;%\n  select(Attrition,  Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  ggpairs()\n\n\n\n\n\n\n\n\n9 Explore Features by Category\n\nplot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {\n  color_expr &lt;- enquo(color)\n  if (rlang::quo_is_null(color_expr)) {\n    g &lt;- data %&gt;%\n      ggpairs(lower = \"blank\") \n    } else {\n    color_name &lt;- quo_name(color_expr)\n    g &lt;- data %&gt;%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")}\n  return(g)}\n\n\n10 Challanges\nDescriptive features: age, gender, marital status\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nEmployment features: department, job role, job level\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nCompensation features: HourlyRate, MonthlyIncome, StockOptionLevel\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 1: What can you deduce about the interaction between Monthly Income and Attrition?\nAnswer: Those that are leaving have a lower Monthly Income\nQuestion 2:What can you deduce about the interaction between Percent Salary Hike and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization\nQuestion 3:What can you deduce about the interaction between Stock Option Level and Attrition?\nAnswer: Those that are staying have a higher stock option level\nSurvey Results: Satisfaction level, Work Life Balance\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 4: What can you deduce about the interaction between Environment Satisfaction and Attrition?\nAnswer: A higher proportion of those leaving have a low environment satisfaction level\nQuestion 5:What can you deduce about the interaction between Work Life Balance and Attrition.\nAnswer: Those that are staying have a higher density of 2’s and 3’s\nPerformance Data: Job Involvement, Performance Rating\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 6: What Can you deduce about the interaction between Job Involvement and Attrition?\nAnswer: Those that are leaving have a lower density of 3’s and 4’s\nWork-Life Features\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 7: What can you deduce about the interaction between Over Time and Attrition?\nAnswer: The proportion of those staying that are working Over Time are high compared to those that are not staying\nTraining and Education\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 8: What can you deduce about the interaction between Training Times Last Year and Attrition.\nAnswer: People that leave tend to have less annual training\nTime-Based Features: Years at company, years in current role\n\nemployee_attrition_tbl %&gt;%\n  select(Attrition, contains(\"years\")) %&gt;%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\nQuestion 9: What can you deduce about the interaction between Years At Company and Attrition.\nAnswer: People that leave tend to have less working years at the company\nQuestion 10: What can you deduce about the interaction between Years Since Last Promotion and Attrition?\nAnswer: It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/04_H2O_II.html",
    "href": "content/01_journal/04_H2O_II.html",
    "title": "Automated Machine Learning with H20 (II)",
    "section": "",
    "text": "Challenge: The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales. It’s a classic Binary Classification problem.\nCode:\nLoading libraries\n\nlibrary(tidymodels)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(sjmisc)\nlibrary(magrittr)\nlibrary(haven)\nlibrary(sjlabelled)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(h2o)\n\nLoading the training & test dataset\n\nproduct_backorders_tbl &lt;- read.csv(\"product_backorders.csv\")\nsplit_obj &lt;- initial_split(product_backorders_tbl, prop = 0.8)\ntrain_readable_tbl &lt;- training(split_obj)\ntest_readable_tbl  &lt;- testing(split_obj)\n\nSpecifying the response and predictor variables\n\nfactor_name &lt;- \"went_on_backorder\"\nrecipe_obj &lt;- \n  recipe(went_on_backorder ~ ., data = train_readable_tbl) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_mutate_at(factor_name, fn = as.factor) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;% \n  prep()\ntrain_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  &lt;- bake(recipe_obj, new_data = test_readable_tbl)\n\nh2o.init()\n\n#&gt;  Connection successful!\n#&gt; \n#&gt; R is connected to the H2O cluster: \n#&gt;     H2O cluster uptime:         5 minutes 46 seconds \n#&gt;     H2O cluster timezone:       Europe/Berlin \n#&gt;     H2O data parsing timezone:  UTC \n#&gt;     H2O cluster version:        3.40.0.4 \n#&gt;     H2O cluster version age:    1 month and 16 days \n#&gt;     H2O cluster name:           H2O_started_from_R_arash_sau427 \n#&gt;     H2O cluster total nodes:    1 \n#&gt;     H2O cluster total memory:   3.10 GB \n#&gt;     H2O cluster total cores:    16 \n#&gt;     H2O cluster allowed cores:  16 \n#&gt;     H2O cluster healthy:        TRUE \n#&gt;     H2O Connection ip:          localhost \n#&gt;     H2O Connection port:        54321 \n#&gt;     H2O Connection proxy:       NA \n#&gt;     H2O Internal Security:      FALSE \n#&gt;     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\nsplit_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.8), seed = 1234)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o &lt;- split_h2o[[1]]\nvalid_h2o &lt;- split_h2o[[2]]\ntest_h2o  &lt;- as.h2o(test_tbl)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ny &lt;- \"went_on_backorder\"\nx &lt;- setdiff(names(train_h2o), y)\n\nrunning AutoML specifying the stopping criterion\n\nautoml_models_h2o &lt;- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 60,\n  nfolds            = 5)\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n#&gt; 21:12:07.158: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#&gt; 21:12:07.168: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nView the leaderboard\n\ntypeof(automl_models_h2o)\n\n#&gt; [1] \"S4\"\n\nslotNames(automl_models_h2o)\n\n#&gt; [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#&gt; [5] \"modeling_steps\" \"training_info\"\n\nautoml_models_h2o@leaderboard \n\n#&gt;                                                  model_id       auc   logloss\n#&gt; 1 StackedEnsemble_BestOfFamily_3_AutoML_2_20230613_211207 0.9551766 0.1633847\n#&gt; 2 StackedEnsemble_BestOfFamily_2_AutoML_2_20230613_211207 0.9543583 0.1647352\n#&gt; 3    StackedEnsemble_AllModels_2_AutoML_2_20230613_211207 0.9542761 0.1639358\n#&gt; 4    StackedEnsemble_AllModels_1_AutoML_2_20230613_211207 0.9539661 0.1646292\n#&gt; 5    StackedEnsemble_AllModels_3_AutoML_2_20230613_211207 0.9535376 0.1662697\n#&gt; 6                          GBM_4_AutoML_2_20230613_211207 0.9522075 0.1675514\n#&gt;       aucpr mean_per_class_error      rmse        mse\n#&gt; 1 0.7444252            0.1512337 0.2197827 0.04830445\n#&gt; 2 0.7409579            0.1401826 0.2204597 0.04860249\n#&gt; 3 0.7428100            0.1380158 0.2199700 0.04838680\n#&gt; 4 0.7406329            0.1528350 0.2205233 0.04863053\n#&gt; 5 0.7382782            0.1520951 0.2206148 0.04867090\n#&gt; 6 0.7376284            0.1663224 0.2217855 0.04918881\n#&gt; \n#&gt; [20 rows x 7 columns]\n\nautoml_models_h2o@leader\n\n#&gt; Model Details:\n#&gt; ==============\n#&gt; \n#&gt; H2OBinomialModel: stackedensemble\n#&gt; Model ID:  StackedEnsemble_BestOfFamily_3_AutoML_2_20230613_211207 \n#&gt; Model Summary for Stacked Ensemble: \n#&gt;                                          key            value\n#&gt; 1                          Stacking strategy cross_validation\n#&gt; 2       Number of base models (used / total)              4/5\n#&gt; 3           # GBM base models (used / total)              1/1\n#&gt; 4           # DRF base models (used / total)              2/2\n#&gt; 5  # DeepLearning base models (used / total)              0/1\n#&gt; 6           # GLM base models (used / total)              1/1\n#&gt; 7                      Metalearner algorithm              GLM\n#&gt; 8         Metalearner fold assignment scheme           Random\n#&gt; 9                         Metalearner nfolds                5\n#&gt; 10                   Metalearner fold_column               NA\n#&gt; 11        Custom metalearner hyperparameters             None\n#&gt; \n#&gt; \n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on training data. **\n#&gt; \n#&gt; MSE:  0.02146092\n#&gt; RMSE:  0.1464955\n#&gt; LogLoss:  0.0852551\n#&gt; Mean Per-Class Error:  0.05829062\n#&gt; AUC:  0.9931495\n#&gt; AUCPR:  0.9640439\n#&gt; Gini:  0.9862991\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No  Yes    Error       Rate\n#&gt; No     8623  114 0.013048  =114/8737\n#&gt; Yes     126 1091 0.103533  =126/1217\n#&gt; Totals 8749 1205 0.024111  =240/9954\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.424664    0.900908 173\n#&gt; 2                       max f2  0.311009    0.915211 206\n#&gt; 3                 max f0point5  0.552482    0.923461 142\n#&gt; 4                 max accuracy  0.457723    0.976090 165\n#&gt; 5                max precision  0.994876    1.000000   0\n#&gt; 6                   max recall  0.024146    1.000000 343\n#&gt; 7              max specificity  0.994876    1.000000   0\n#&gt; 8             max absolute_mcc  0.424664    0.887198 173\n#&gt; 9   max min_per_class_accuracy  0.224656    0.955629 232\n#&gt; 10 max mean_per_class_accuracy  0.237963    0.956677 228\n#&gt; 11                     max tns  0.994876 8737.000000   0\n#&gt; 12                     max fns  0.994876 1214.000000   0\n#&gt; 13                     max fps  0.000312 8737.000000 399\n#&gt; 14                     max tps  0.024146 1217.000000 343\n#&gt; 15                     max tnr  0.994876    1.000000   0\n#&gt; 16                     max fnr  0.994876    0.997535   0\n#&gt; 17                     max fpr  0.000312    1.000000 399\n#&gt; 18                     max tpr  0.024146    1.000000 343\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on validation data. **\n#&gt; \n#&gt; MSE:  0.05034929\n#&gt; RMSE:  0.2243865\n#&gt; LogLoss:  0.169058\n#&gt; Mean Per-Class Error:  0.1516906\n#&gt; AUC:  0.9546535\n#&gt; AUCPR:  0.7373023\n#&gt; Gini:  0.9093071\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No Yes    Error       Rate\n#&gt; No     2541 102 0.038593  =102/2643\n#&gt; Yes      94 261 0.264789    =94/355\n#&gt; Totals 2635 363 0.065377  =196/2998\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.428057    0.727019 167\n#&gt; 2                       max f2  0.138137    0.784119 258\n#&gt; 3                 max f0point5  0.574555    0.739605 122\n#&gt; 4                 max accuracy  0.462915    0.935624 157\n#&gt; 5                max precision  0.987178    1.000000   0\n#&gt; 6                   max recall  0.011508    1.000000 367\n#&gt; 7              max specificity  0.987178    1.000000   0\n#&gt; 8             max absolute_mcc  0.428057    0.689945 167\n#&gt; 9   max min_per_class_accuracy  0.138137    0.890141 258\n#&gt; 10 max mean_per_class_accuracy  0.122785    0.892353 265\n#&gt; 11                     max tns  0.987178 2643.000000   0\n#&gt; 12                     max fns  0.987178  353.000000   0\n#&gt; 13                     max fps  0.000323 2643.000000 399\n#&gt; 14                     max tps  0.011508  355.000000 367\n#&gt; 15                     max tnr  0.987178    1.000000   0\n#&gt; 16                     max fnr  0.987178    0.994366   0\n#&gt; 17                     max fpr  0.000323    1.000000 399\n#&gt; 18                     max tpr  0.011508    1.000000 367\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: stackedensemble\n#&gt; ** Reported on cross-validation data. **\n#&gt; ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#&gt; \n#&gt; MSE:  0.05211933\n#&gt; RMSE:  0.2282966\n#&gt; LogLoss:  0.1756122\n#&gt; Mean Per-Class Error:  0.1521013\n#&gt; AUC:  0.9482074\n#&gt; AUCPR:  0.7425065\n#&gt; Gini:  0.8964148\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;           No  Yes    Error        Rate\n#&gt; No     10227  538 0.049977  =538/10765\n#&gt; Yes      376 1103 0.254226   =376/1479\n#&gt; Totals 10603 1641 0.074649  =914/12244\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold        value idx\n#&gt; 1                       max f1  0.320912     0.707051 208\n#&gt; 2                       max f2  0.119625     0.777502 287\n#&gt; 3                 max f0point5  0.631915     0.723573 116\n#&gt; 4                 max accuracy  0.438909     0.927066 172\n#&gt; 5                max precision  0.995063     1.000000   0\n#&gt; 6                   max recall  0.001866     1.000000 395\n#&gt; 7              max specificity  0.995063     1.000000   0\n#&gt; 8             max absolute_mcc  0.320912     0.665588 208\n#&gt; 9   max min_per_class_accuracy  0.122237     0.887041 286\n#&gt; 10 max mean_per_class_accuracy  0.104520     0.888662 295\n#&gt; 11                     max tns  0.995063 10765.000000   0\n#&gt; 12                     max fns  0.995063  1478.000000   0\n#&gt; 13                     max fps  0.000413 10765.000000 399\n#&gt; 14                     max tps  0.001866  1479.000000 395\n#&gt; 15                     max tnr  0.995063     1.000000   0\n#&gt; 16                     max fnr  0.995063     0.999324   0\n#&gt; 17                     max fpr  0.000413     1.000000 399\n#&gt; 18                     max tpr  0.001866     1.000000 395\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; Cross-Validation Metrics Summary: \n#&gt;                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#&gt; accuracy    0.924206  0.009231   0.913560   0.929656   0.929440   0.933415\n#&gt; auc         0.948475  0.003488   0.945368   0.953166   0.946871   0.951171\n#&gt; err         0.075794  0.009231   0.086440   0.070344   0.070560   0.066585\n#&gt; err_count 185.400000 20.525595 211.000000 176.000000 174.000000 163.000000\n#&gt; f0point5    0.680278  0.039312   0.629921   0.703883   0.700000   0.720365\n#&gt;           cv_5_valid\n#&gt; accuracy    0.914956\n#&gt; auc         0.945798\n#&gt; err         0.085044\n#&gt; err_count 203.000000\n#&gt; f0point5    0.647222\n#&gt; \n#&gt; ---\n#&gt;                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#&gt; precision           0.660305  0.047481   0.600536   0.690476   0.687117\n#&gt; r2                  0.509253  0.031669   0.467103   0.535838   0.509671\n#&gt; recall              0.777690  0.017205   0.783217   0.763158   0.756757\n#&gt; residual_deviance 858.669800 25.260990 893.400760 837.848700 864.102500\n#&gt; rmse                0.228157  0.006081   0.234780   0.222586   0.227576\n#&gt; specificity         0.944338  0.012129   0.930859   0.952684   0.952995\n#&gt;                   cv_4_valid cv_5_valid\n#&gt; precision           0.705357   0.618037\n#&gt; r2                  0.543409   0.490244\n#&gt; recall              0.787375   0.797945\n#&gt; residual_deviance 830.342040 867.655000\n#&gt; rmse                0.221897   0.233944\n#&gt; specificity         0.953889   0.931265\n\nextract_h2o_model_name_by_position &lt;- function(h2o_leaderboard,\n                                                n= 1, verbose = T){\n   model_name &lt;- h2o_leaderboard %&gt;%\n     as_tibble() %&gt;%\n     slice(n) %&gt;%\n     pull(model_id)\n   if (verbose) message(model_name)\n   return(model_name)}\nautoml_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(6) %&gt;% \n  h2o.getModel()\n\n#&gt; Model Details:\n#&gt; ==============\n#&gt; \n#&gt; H2OBinomialModel: gbm\n#&gt; Model ID:  GBM_4_AutoML_2_20230613_211207 \n#&gt; Model Summary: \n#&gt;   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n#&gt; 1              66                       66              112787        10\n#&gt;   max_depth mean_depth min_leaves max_leaves mean_leaves\n#&gt; 1        10   10.00000         55        200   131.00000\n#&gt; \n#&gt; \n#&gt; H2OBinomialMetrics: gbm\n#&gt; ** Reported on training data. **\n#&gt; \n#&gt; MSE:  0.02523486\n#&gt; RMSE:  0.1588548\n#&gt; LogLoss:  0.09791398\n#&gt; Mean Per-Class Error:  0.06442004\n#&gt; AUC:  0.9910454\n#&gt; AUCPR:  0.9529739\n#&gt; Gini:  0.9820907\n#&gt; R^2:  0.7623897\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;           No  Yes    Error        Rate\n#&gt; No     10579  186 0.017278  =186/10765\n#&gt; Yes      165 1314 0.111562   =165/1479\n#&gt; Totals 10744 1500 0.028667  =351/12244\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold        value idx\n#&gt; 1                       max f1  0.358083     0.882175 197\n#&gt; 2                       max f2  0.217215     0.899525 244\n#&gt; 3                 max f0point5  0.596419     0.908629 131\n#&gt; 4                 max accuracy  0.408854     0.971578 182\n#&gt; 5                max precision  0.991809     1.000000   0\n#&gt; 6                   max recall  0.021263     1.000000 361\n#&gt; 7              max specificity  0.991809     1.000000   0\n#&gt; 8             max absolute_mcc  0.358083     0.865885 197\n#&gt; 9   max min_per_class_accuracy  0.201559     0.951324 249\n#&gt; 10 max mean_per_class_accuracy  0.201559     0.951997 249\n#&gt; 11                     max tns  0.991809 10765.000000   0\n#&gt; 12                     max fns  0.991809  1476.000000   0\n#&gt; 13                     max fps  0.001226 10765.000000 399\n#&gt; 14                     max tps  0.021263  1479.000000 361\n#&gt; 15                     max tnr  0.991809     1.000000   0\n#&gt; 16                     max fnr  0.991809     0.997972   0\n#&gt; 17                     max fpr  0.001226     1.000000 399\n#&gt; 18                     max tpr  0.021263     1.000000 361\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: gbm\n#&gt; ** Reported on validation data. **\n#&gt; \n#&gt; MSE:  0.05065852\n#&gt; RMSE:  0.2250745\n#&gt; LogLoss:  0.1713142\n#&gt; Mean Per-Class Error:  0.1504713\n#&gt; AUC:  0.9528726\n#&gt; AUCPR:  0.7339576\n#&gt; Gini:  0.9057452\n#&gt; R^2:  0.5147224\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;          No Yes    Error       Rate\n#&gt; No     2540 103 0.038971  =103/2643\n#&gt; Yes      93 262 0.261972    =93/355\n#&gt; Totals 2633 365 0.065377  =196/2998\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold       value idx\n#&gt; 1                       max f1  0.368473    0.727778 176\n#&gt; 2                       max f2  0.112399    0.783691 270\n#&gt; 3                 max f0point5  0.490397    0.737809 146\n#&gt; 4                 max accuracy  0.490397    0.934957 146\n#&gt; 5                max precision  0.982694    1.000000   0\n#&gt; 6                   max recall  0.009827    1.000000 375\n#&gt; 7              max specificity  0.982694    1.000000   0\n#&gt; 8             max absolute_mcc  0.368473    0.690723 176\n#&gt; 9   max min_per_class_accuracy  0.124063    0.889898 264\n#&gt; 10 max mean_per_class_accuracy  0.108114    0.894581 273\n#&gt; 11                     max tns  0.982694 2643.000000   0\n#&gt; 12                     max fns  0.982694  353.000000   0\n#&gt; 13                     max fps  0.001029 2643.000000 399\n#&gt; 14                     max tps  0.009827  355.000000 375\n#&gt; 15                     max tnr  0.982694    1.000000   0\n#&gt; 16                     max fnr  0.982694    0.994366   0\n#&gt; 17                     max fpr  0.001029    1.000000 399\n#&gt; 18                     max tpr  0.009827    1.000000 375\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; H2OBinomialMetrics: gbm\n#&gt; ** Reported on cross-validation data. **\n#&gt; ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#&gt; \n#&gt; MSE:  0.05317814\n#&gt; RMSE:  0.2306039\n#&gt; LogLoss:  0.1787185\n#&gt; Mean Per-Class Error:  0.1364049\n#&gt; AUC:  0.9468332\n#&gt; AUCPR:  0.7367355\n#&gt; Gini:  0.8936663\n#&gt; R^2:  0.499277\n#&gt; \n#&gt; Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#&gt;           No  Yes    Error         Rate\n#&gt; No     10070  695 0.064561   =695/10765\n#&gt; Yes      308 1171 0.208249    =308/1479\n#&gt; Totals 10378 1866 0.081918  =1003/12244\n#&gt; \n#&gt; Maximum Metrics: Maximum metrics at their respective thresholds\n#&gt;                         metric threshold        value idx\n#&gt; 1                       max f1  0.217669     0.700149 242\n#&gt; 2                       max f2  0.093073     0.778294 298\n#&gt; 3                 max f0point5  0.624082     0.716583 112\n#&gt; 4                 max accuracy  0.501486     0.926331 151\n#&gt; 5                max precision  0.992126     1.000000   0\n#&gt; 6                   max recall  0.002930     1.000000 395\n#&gt; 7              max specificity  0.992126     1.000000   0\n#&gt; 8             max absolute_mcc  0.217669     0.659366 242\n#&gt; 9   max min_per_class_accuracy  0.110909     0.887413 289\n#&gt; 10 max mean_per_class_accuracy  0.083311     0.890301 304\n#&gt; 11                     max tns  0.992126 10765.000000   0\n#&gt; 12                     max fns  0.992126  1478.000000   0\n#&gt; 13                     max fps  0.001351 10765.000000 399\n#&gt; 14                     max tps  0.002930  1479.000000 395\n#&gt; 15                     max tnr  0.992126     1.000000   0\n#&gt; 16                     max fnr  0.992126     0.999324   0\n#&gt; 17                     max fpr  0.001351     1.000000 399\n#&gt; 18                     max tpr  0.002930     1.000000 395\n#&gt; \n#&gt; Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n#&gt; Cross-Validation Metrics Summary: \n#&gt;                               mean        sd cv_1_valid cv_2_valid cv_3_valid\n#&gt; accuracy                  0.923064  0.005084   0.929767   0.925684   0.922826\n#&gt; auc                       0.946101  0.003082   0.945715   0.942518   0.945379\n#&gt; err                       0.076936  0.005084   0.070233   0.074316   0.077174\n#&gt; err_count               188.400000 12.421755 172.000000 182.000000 189.000000\n#&gt; f0point5                  0.676272  0.018908   0.681976   0.706667   0.668244\n#&gt; f1                        0.706145  0.009657   0.695035   0.699670   0.705148\n#&gt; f2                        0.740806  0.040388   0.708604   0.692810   0.746367\n#&gt; lift_top_group            7.250674  0.885135   8.253040   6.361039   7.405911\n#&gt; logloss                   0.179685  0.007657   0.168251   0.188391   0.180103\n#&gt; max_per_class_error       0.232801  0.064685   0.282051   0.311688   0.223368\n#&gt; mcc                       0.666575  0.011061   0.655822   0.657395   0.664913\n#&gt; mean_per_class_accuracy   0.855761  0.025404   0.837145   0.824072   0.859586\n#&gt; mean_per_class_error      0.144239  0.025404   0.162855   0.175928   0.140414\n#&gt; mse                       0.053562  0.002555   0.049222   0.055802   0.053663\n#&gt; pr_auc                    0.734851  0.010646   0.737461   0.726949   0.726410\n#&gt; precision                 0.658540  0.034566   0.673539   0.711409   0.645714\n#&gt; r2                        0.495558  0.007268   0.503048   0.492476   0.487485\n#&gt; recall                    0.767199  0.064685   0.717949   0.688312   0.776632\n#&gt; rmse                      0.231381  0.005591   0.221860   0.236224   0.231652\n#&gt; specificity               0.944323  0.013936   0.956342   0.959832   0.942539\n#&gt;                         cv_4_valid cv_5_valid\n#&gt; accuracy                  0.920784   0.916258\n#&gt; auc                       0.951043   0.945850\n#&gt; err                       0.079216   0.083742\n#&gt; err_count               194.000000 205.000000\n#&gt; f0point5                  0.664997   0.659478\n#&gt; f1                        0.711310   0.719562\n#&gt; f2                        0.764555   0.791692\n#&gt; lift_top_group            7.915959   6.317419\n#&gt; logloss                   0.177294   0.184385\n#&gt; max_per_class_error       0.195286   0.151613\n#&gt; mcc                       0.672203   0.682544\n#&gt; mean_per_class_accuracy   0.870758   0.887243\n#&gt; mean_per_class_error      0.129242   0.112757\n#&gt; mse                       0.054209   0.054916\n#&gt; pr_auc                    0.752170   0.731264\n#&gt; precision                 0.637333   0.624703\n#&gt; r2                        0.491315   0.503466\n#&gt; recall                    0.804714   0.848387\n#&gt; rmse                      0.232828   0.234341\n#&gt; specificity               0.936803   0.926099\n\n\nPredicting using Leader Model\n\nstacked_ensemble_h2o &lt;- automl_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(6) %&gt;% \n  h2o.getModel()\n\npredictions &lt;- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#&gt; \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#&gt; [1] \"environment\"\n\npredictions_tbl &lt;- predictions %&gt;% as_tibble()\npredictions_tbl\n\n\n\n  \n\n\n\nSave the leader model\n\nautoml_models_h2o@leaderboard %&gt;% \n  extract_h2o_model_name_by_position(6) %&gt;% \n  h2o.getModel() %&gt;%\n  h2o.saveModel(path = \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\")\n\n#&gt; [1] \"C:\\\\Users\\\\arash\\\\Documents\\\\GitHub\\\\ss23-bdml-ArashAmiririgi\\\\content\\\\01_journal\\\\04_Modeling\\\\GBM_4_AutoML_2_20230613_211207\""
  }
]